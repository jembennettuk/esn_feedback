{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4fa75b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as pl\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "5c38ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "34fa27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/jb739/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "/its/home/jb739/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "X_te=mnist_testset.data               ## Test set images\n",
    "y_te=mnist_testset.test_labels        ## Test set labels\n",
    "\n",
    "N_o=10                                ## Number of output nodes/classes\n",
    "N_te=y_te.size()[0]                   ## Number of test samples\n",
    "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
    "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
    "\n",
    "X_tr=mnist_trainset.data              ## Train set images\n",
    "y_tr=mnist_trainset.train_labels      ## Train labels \n",
    "N_tr=y_tr.size()[0]                   \n",
    "\n",
    "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
    "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
    "\n",
    "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
    "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
    "\n",
    "X_val=X_tr[i_val,:,:]\n",
    "Y_val=Y_tr[i_val,:]\n",
    "\n",
    "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
    "N_tr=N_tr-N_val\n",
    "\n",
    "X_tr=X_tr[i_tr,:,:]\n",
    "Y_tr=Y_tr[i_tr,:]\n",
    "\n",
    "T=X_tr.size()[2]\n",
    "N_in=X_tr.size()[1]\n",
    "\n",
    "## Normalisation and conversion to float\n",
    "X_M=255\n",
    "\n",
    "X_tr=(X_tr.float()/X_M).to(device)\n",
    "X_val=(X_val.float()/X_M).to(device)\n",
    "X_te=(X_te.float()/X_M).to(device)\n",
    "\n",
    "Y_tr=Y_tr.float().to(device)\n",
    "Y_val=Y_val.float().to(device)\n",
    "Y_te=Y_te.float().to(device)\n",
    "# Y_tr=torch.tile(Y_tr.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n",
    "# Y_val=torch.tile(Y_val.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n",
    "# Y_te=torch.tile(Y_te.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "902544bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data2Classes(X,Y):\n",
    "    \n",
    "    ind=torch.where(Y==1)[1]\n",
    "\n",
    "    nClass=torch.max(ind)+1\n",
    "    \n",
    "    X1=[]\n",
    "    Y1=[]\n",
    "    \n",
    "    for n in range(nClass):\n",
    "    \n",
    "        ind1=torch.where(ind==n)[0].type(torch.long)\n",
    "\n",
    "        X1.append(torch.clone(X[ind1,:]).to(device))\n",
    "        Y1.append(torch.clone(Y[ind1,:]).to(device))\n",
    "        \n",
    "    return X1, Y1\n",
    "        \n",
    "# X_tr/X_val/X_te are lists of length 10 (1 entry per class)\n",
    "X_tr, Y_tr=Data2Classes(X_tr,Y_tr)\n",
    "\n",
    "X_val, Y_val=Data2Classes(X_val,Y_val)\n",
    "\n",
    "X_te, Y_te=Data2Classes(X_te,Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c1ad7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_James(nn.Module):\n",
    "    \n",
    "    def __init__(self,N,N_in,N_av,alpha,rho,gamma,Ns,fb_ind,batch_size,nClass):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.N=N\n",
    "        self.alpha=torch.tensor(alpha,device=device)\n",
    "        self.rho=rho\n",
    "        self.N_av=N_av\n",
    "        self.N_in=N_in\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        diluition=1-N_av/N\n",
    "        W=np.random.uniform(-1,1,[N,N])\n",
    "        W=W*(np.random.uniform(0,1,[N,N])>diluition)\n",
    "        eig=np.linalg.eigvals(W)\n",
    "        self.W=torch.from_numpy(self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
    "        \n",
    "        self.x=[]\n",
    "        \n",
    "        if self.N_in==1:\n",
    "            \n",
    "            self.W_in=2*np.random.randint(0,2,[self.N_in,self.N])-1\n",
    "            self.W_in=torch.from_numpy(self.W_in*self.gamma,device=device).float()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.W_in=np.random.randn(self.N_in,self.N)\n",
    "            self.W_in=torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
    "            \n",
    "        \n",
    "        self.Ws=[]\n",
    "        self.bs=[]\n",
    "        self.Ns=Ns\n",
    "        \n",
    "        for n in range(1,np.shape(Ns)[0]):\n",
    "            \n",
    "            self.Ws.append(nn.Parameter(torch.randn([Ns[n-1],Ns[n]],device=device)/torch.sqrt(Ns[n-1]+Ns[n])))\n",
    "            self.bs.append(nn.Parameter(torch.zeros([Ns[n]],device=device)))\n",
    "        \n",
    "        self.fb_ind=fb_ind\n",
    "        \n",
    "        self.W_fb=[]\n",
    "        self.W_fb.append(nn.Parameter(torch.randn([Ns[fb_ind],Ns[0]],device=device)/torch.sqrt(Ns[n-1]+Ns[n])))\n",
    "    \n",
    "        # Useful hyperparameters\n",
    "        self.nClass=nClass\n",
    "        self.batch_size = batch_size\n",
    "        self.nSampPerClassPerBatch = int(batch_size/nClass) # No. input samples per class, per batch\n",
    "        # # Setup target vector to compute Loss and Accuracy\n",
    "        # self.target = torch.zeros(self.batch_size).long().to(device)\n",
    "        # for j in range(1,nClass):\n",
    "        #     self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
    "        # Setup one-hot encoded target vector to compute Loss and Accuracy\n",
    "        self.target = torch.zeros(self.batch_size, nClass, dtype=torch.float32).to(device)\n",
    "        for j in range(1,nClass):\n",
    "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,j] = 1.\n",
    "    \n",
    "    def Initialize_Hyper(self,eta):\n",
    "        \n",
    "        self.eta=eta\n",
    "        \n",
    "        self.opt=optim.Adam([{ 'params': self.Ws+self.bs+self.W_fb, 'lr':eta }])\n",
    "    \n",
    "    \n",
    "    def Reset(self,s):\n",
    "        \n",
    "        batch_size=s.size()[0]\n",
    "        self.x=torch.zeros([batch_size,self.N],device=device)\n",
    "        self.xs=[]\n",
    "        self.xs.append(self.x)\n",
    "        \n",
    "        for n in range(1,np.shape(self.Ns)[0]-1):\n",
    "            \n",
    "            self.xs.append(torch.zeros([batch_size,self.Ns[n]],device=device))\n",
    "            if n==self.fb_ind:\n",
    "                \n",
    "                self.fb=torch.clone(self.xs[n])\n",
    "        \n",
    "    def Forward(self,s):\n",
    "        \n",
    "        \n",
    "        self.x=(1-self.alpha)*self.x+self.alpha*torch.tanh(torch.matmul(s,self.W_in)+torch.matmul(self.x,self.W)+\\\n",
    "                                                          torch.matmul(self.fb,self.W_fb[0]))\n",
    "        \n",
    "        self.xs[0]=self.x\n",
    "        for n in range(1,np.shape(self.Ns)[0]-1):\n",
    "            \n",
    "\n",
    "            self.xs[n]=torch.relu(torch.matmul(self.xs[n-1],self.Ws[n-1])+self.bs[n-1])\n",
    "            \n",
    "            if n==self.fb_ind:\n",
    "                \n",
    "                self.fb=torch.clone(self.xs[n])\n",
    "    \n",
    "        \n",
    "    def Response(self,Input,Targets):\n",
    "        \n",
    "        T=Input.shape[2]\n",
    "        Out=torch.zeros([Input.shape[0],self.Ns[-1],T],device=device)\n",
    "        \n",
    "        self.Reset(Input[:,0,0])\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for t in range(T):\n",
    "\n",
    "            self.Forward(Input[:,:,t])\n",
    "            \n",
    "            Out[:,:,t]=torch.matmul(self.xs[-1],self.Ws[-1])+self.bs[-1]\n",
    "            \n",
    "        Err=loss(Out[:,:,-1],Targets)\n",
    "\n",
    "        Err.backward(retain_graph=True)\n",
    "\n",
    "        Acc=torch.mean( torch.eq(torch.argmax(Targets,1),torch.argmax(Out[:,:,-1],1)).float() )                           \n",
    "\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "            \n",
    "        return Out, Err, Acc\n",
    "    \n",
    "    def Set_Transfer(self,Ns_transfer,eta_transfer):\n",
    "        \n",
    "        self.Ws_t=[]\n",
    "        self.bs_t=[]\n",
    "        self.Ns_transfer=Ns_transfer\n",
    "        \n",
    "        for n in range(1,np.shape(Ns_transfer)[0]):\n",
    "            \n",
    "            self.Ws_t.append(nn.Parameter(torch.randn([Ns_transfer[n-1],Ns_transfer[n]],device=device)/torch.sqrt(Ns_transfer[n-1]+Ns_transfer[n])))\n",
    "            self.bs_t.append(nn.Parameter(torch.zeros([Ns_transfer[n]],device=device)))\n",
    "        \n",
    "        self.opt_tr=optim.Adam([{ 'params': self.Ws_t+self.bs_t, 'lr':eta_transfer}])\n",
    "\n",
    "    def Transfer(self,Input,Targets):\n",
    "        \n",
    "        T=Input.shape[2]\n",
    "        X=torch.zeros([Input.shape[0],self.Ns[-2]],device=device)\n",
    "        self.Reset(Input[:,0,0])\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for t in range(T):\n",
    "\n",
    "                self.Forward(Input[:,:,t])\n",
    "                \n",
    "            X=torch.clone(self.xs[-1])\n",
    "        \n",
    "        Xs_transfer=[]\n",
    "        Xs_transfer.append(X)\n",
    "        \n",
    "        for n in range(1,np.shape(self.Ns_transfer)[0]-1):\n",
    "            \n",
    "            Xs_transfer.append(torch.relu(torch.matmul(self.Xs_transfer[n-1],self.Ws_t[n-1])+self.bs_t[n-1]))\n",
    "            \n",
    "        Out=torch.matmul(Xs_transfer[-1],self.Ws_t[-1])+self.bs_t[-1]\n",
    "        \n",
    "        Err=loss(Out,Targets)\n",
    "        \n",
    "        Err.backward(retain_graph=True)\n",
    "\n",
    "        Acc=torch.mean( torch.eq(torch.argmax(Targets,1),torch.argmax(Out,1)).float() )                           \n",
    "\n",
    "        self.opt_tr.step()\n",
    "        self.opt_tr.zero_grad()\n",
    "            \n",
    "        return Out, Err, Acc\n",
    "    \n",
    "    def generateBatch(self, method, X):\n",
    "       \n",
    "        ### For normal batches, e.g. for transfer learning\n",
    "        if method=='simple':\n",
    "            batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
    "            self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
    "            # for j in range(self.batch_size):\n",
    "            #     ind1 = torch.randint(0,self.nClass,[1])\n",
    "            #     ind2 = torch.randint(0,X[ind1].shape[0],[1])\n",
    "            #     batch[j,:,:] = torch.clone(X[ind1][ind2,:,:])\n",
    "            #     self.target[j,ind1] = 1.\n",
    "            for k in range(self.nClass):\n",
    "                rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
    "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
    "                self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
    "        ### For triplet loss\n",
    "        if method=='tripletLoss':\n",
    "            \n",
    "            # Populate batch\n",
    "            batch = torch.zeros([3*self.batch_size, self.nInputs,self.tMax]).to(device)\n",
    "            for k in range(self.nClass):\n",
    "                # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
    "                ind_ap = np.random.choice(X[k].shape[0],(self.nSampPerClassPerBatch,2), replace=False)\n",
    "                # Populate Anchor and Positive samples\n",
    "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,0],:])\n",
    "                batch[self.batch_size+k*self.nSampPerClassPerBatch:self.batch_size+(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,1],:])\n",
    "                # Populate negative samples\n",
    "                randClass = np.random.choice((np.arange(self.nClass)!=k).nonzero()[0], self.nSampPerClassPerBatch)\n",
    "                for m, cl in enumerate(randClass):\n",
    "                    batch[2*self.batch_size + k*self.nSampPerClassPerBatch+m,:] = torch.clone(X[cl][np.random.randint(X[cl].shape[0]),:])\n",
    "            \n",
    "        ### For Protoypical Loss\n",
    "        if method=='prototypicalLoss':\n",
    "            batch = torch.zeros((self.nP+self.nQ)*self.nClass, X[0].shape[1], X[0].shape[2]).to(device)\n",
    "            for j, x in enumerate(X): # For each class (X is a list)\n",
    "                ind = np.random.choice(x.shape[0], self.nP+self.nQ, replace=False)\n",
    "                batch[j*self.nP:(j+1)*self.nP,:] = torch.clone(x[ind[:self.nP],:])\n",
    "                batch[self.nP*self.nClass+j*self.nQ:self.nP*self.nClass+(j+1)*self.nQ] = torch.clone(x[ind[self.nP:],:])\n",
    "\n",
    "        return batch\n",
    "                           \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "33f35e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8f97e188e0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 196.85x196.85 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = NN.generateBatch('simple',X_tr)\n",
    "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "j=-8\n",
    "pl.imshow(batch[j,:,:].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7e90c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan) tensor(nan)\n",
      "tensor(0.1887) tensor(0.5880)\n",
      "tensor(0.1241) tensor(0.7734)\n",
      "tensor(0.0968) tensor(0.8352)\n",
      "tensor(0.0794) tensor(0.8682)\n",
      "tensor(0.0683) tensor(0.8878)\n",
      "tensor(0.0633) tensor(0.8946)\n",
      "tensor(0.0595) tensor(0.9016)\n",
      "tensor(0.0617) tensor(0.8954)\n",
      "tensor(0.0515) tensor(0.9190)\n",
      "tensor(0.0480) tensor(0.9216)\n",
      "tensor(0.0419) tensor(0.9320)\n",
      "tensor(0.0445) tensor(0.9258)\n",
      "tensor(0.0436) tensor(0.9310)\n",
      "tensor(0.0419) tensor(0.9302)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/its/home/jb739/esn_feedback/Model_James.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_train):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     batch \u001b[39m=\u001b[39m NN\u001b[39m.\u001b[39mgenerateBatch(\u001b[39m'\u001b[39m\u001b[39msimple\u001b[39m\u001b[39m'\u001b[39m,X_tr)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     out, err, acc\u001b[39m=\u001b[39mNN\u001b[39m.\u001b[39;49mResponse(batch,NN\u001b[39m.\u001b[39;49mtarget)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     Err[n]\u001b[39m=\u001b[39merr\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     Acc[n]\u001b[39m=\u001b[39macc\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/its/home/jb739/esn_feedback/Model_James.ipynb Cell 7\u001b[0m in \u001b[0;36mModel_James.Response\u001b[0;34m(self, Input, Targets)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m loss\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mForward(Input[:,:,t])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m     Out[:,:,t]\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m Err\u001b[39m=\u001b[39mloss(Out[:,:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],Targets)\n",
      "\u001b[1;32m/its/home/jb739/esn_feedback/Model_James.ipynb Cell 7\u001b[0m in \u001b[0;36mModel_James.Forward\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxs[n]\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrelu(torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxs[n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWs[n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs[n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m n\u001b[39m==\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfb_ind:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/Model_James.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfb\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mclone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxs[n])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_in=X_tr[0].size()[1]\n",
    "N=500\n",
    "N_av=10\n",
    "alpha=0.8\n",
    "rho=0.99\n",
    "gamma=0.1\n",
    "Ns=torch.tensor([500,100,10])\n",
    "fb_ind=1\n",
    "\n",
    "eta=0.001\n",
    "N_train=5000\n",
    "batch_size=50\n",
    "nClass = 10\n",
    "NN=Model_James(N, N_in, N_av, alpha, rho, gamma, Ns, fb_ind, batch_size, nClass)\n",
    "NN.Initialize_Hyper(eta)\n",
    "\n",
    "Err=torch.zeros([N_train])\n",
    "Acc=torch.zeros([N_train])\n",
    "\n",
    "for n in range(N_train):\n",
    "    \n",
    "    batch = NN.generateBatch('simple',X_tr)\n",
    "    \n",
    "    out, err, acc=NN.Response(batch,NN.target)\n",
    "    \n",
    "    Err[n]=err.detach().to('cpu')\n",
    "    Acc[n]=acc.detach().to('cpu')\n",
    "    \n",
    "    if n%250==0:\n",
    "        \n",
    "        print(torch.mean(Err[n-100:n]), torch.mean(Acc[n-100:n]))\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    NN.Reset(X_tr)\n",
    "    for t in range(28):\n",
    "        NN.Forward(X_tr[:,:,t])\n",
    "    X=torch.clone(NN.xs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize=tuple(np.array((20.,10.))/2.54)); ax = pl.axes()\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "ind=(torch.argmax(Y_tr[:,:,0], 1)==3).nonzero().squeeze()\n",
    "pl.plot(X[ind[0:10],:].cpu().transpose(0,1), c=[1,0,0])\n",
    "ind=(torch.argmax(Y_tr[:,:,0], 1)==4).nonzero().squeeze()\n",
    "pl.plot(X[ind[0:10],:].cpu().transpose(0,1), c=[0,0,1])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff94ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns_transfer=torch.clone(Ns[-2:])\n",
    "\n",
    "N_transfer=1000\n",
    "Err_t=torch.zeros([N_transfer])\n",
    "Acc_t=torch.zeros([N_transfer])\n",
    "\n",
    "eta_transfer=0.001\n",
    "NN.Set_Transfer(Ns_transfer,eta_transfer)\n",
    "\n",
    "\n",
    "for n in range(N_transfer):\n",
    "    \n",
    "    rand_ind=np.random.randint(0,X_tr.size()[0],[batch_size])\n",
    "    X_b=X_tr[rand_ind,:,:]\n",
    "    Y_b=Y_tr[rand_ind,:]\n",
    "    \n",
    "    out, err, acc=NN.Transfer(X_b,Y_b)\n",
    "    \n",
    "    Err_t[n]=err.detach().to('cpu')\n",
    "    Acc_t[n]=acc.detach().to('cpu')\n",
    "    \n",
    "    if n%250==0:\n",
    "        \n",
    "        print(torch.mean(Err_t[n-100:n]), torch.mean(Acc_t[n-100:n]))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv] *",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
