{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa75b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as pl\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te=mnist_testset.data               ## Test set images\n",
    "y_te=mnist_testset.test_labels        ## Test set labels\n",
    "\n",
    "N_o=10                                ## Number of output nodes/classes\n",
    "N_te=y_te.size()[0]                   ## Number of test samples\n",
    "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
    "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
    "\n",
    "X_tr=mnist_trainset.data              ## Train set images\n",
    "y_tr=mnist_trainset.train_labels      ## Train labels \n",
    "N_tr=y_tr.size()[0]                   \n",
    "\n",
    "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
    "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
    "\n",
    "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
    "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
    "\n",
    "X_val=X_tr[i_val,:,:]\n",
    "Y_val=Y_tr[i_val,:]\n",
    "\n",
    "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
    "N_tr=N_tr-N_val\n",
    "\n",
    "X_tr=X_tr[i_tr,:,:]\n",
    "Y_tr=Y_tr[i_tr,:]\n",
    "\n",
    "T=X_tr.size()[2]\n",
    "N_in=X_tr.size()[1]\n",
    "\n",
    "## Normalisation and conversion to float\n",
    "X_M=255\n",
    "\n",
    "X_tr=(X_tr.float()/X_M).to(device)\n",
    "X_val=(X_val.float()/X_M).to(device)\n",
    "X_te=(X_te.float()/X_M).to(device)\n",
    "\n",
    "Y_tr=Y_tr.float().to(device)\n",
    "Y_val=Y_val.float().to(device)\n",
    "Y_te=Y_te.float().to(device)\n",
    "# Y_tr=torch.tile(Y_tr.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n",
    "# Y_val=torch.tile(Y_val.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n",
    "# Y_te=torch.tile(Y_te.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902544bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data2Classes(X,Y):\n",
    "    \n",
    "    ind=torch.where(Y==1)[1]\n",
    "\n",
    "    nClass=torch.max(ind)+1\n",
    "    \n",
    "    X1=[]\n",
    "    Y1=[]\n",
    "    \n",
    "    for n in range(nClass):\n",
    "    \n",
    "        ind1=torch.where(ind==n)[0].type(torch.long)\n",
    "\n",
    "        X1.append(torch.clone(X[ind1,:]).to(device))\n",
    "        Y1.append(torch.clone(Y[ind1,:]).to(device))\n",
    "        \n",
    "    return X1, Y1\n",
    "        \n",
    "# X_tr/X_val/X_te are lists of length 10 (1 entry per class)\n",
    "X_tr, Y_tr=Data2Classes(X_tr,Y_tr)\n",
    "\n",
    "X_val, Y_val=Data2Classes(X_val,Y_val)\n",
    "\n",
    "X_te, Y_te=Data2Classes(X_te,Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_James(nn.Module):\n",
    "    \n",
    "    def __init__(self,N,N_in,N_av,alpha,rho,gamma,Ns,fb_ind,batch_size,nClass):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.N=N\n",
    "        self.alpha=torch.tensor(alpha,device=device)\n",
    "        self.rho=rho\n",
    "        self.N_av=N_av\n",
    "        self.N_in=N_in\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        diluition=1-N_av/N\n",
    "        W=np.random.uniform(-1,1,[N,N])\n",
    "        W=W*(np.random.uniform(0,1,[N,N])>diluition)\n",
    "        eig=np.linalg.eigvals(W)\n",
    "        self.W=torch.from_numpy(self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
    "        \n",
    "        self.x=[]\n",
    "        \n",
    "        if self.N_in==1:\n",
    "            \n",
    "            self.W_in=2*np.random.randint(0,2,[self.N_in,self.N])-1\n",
    "            self.W_in=torch.from_numpy(self.W_in*self.gamma,device=device).float()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.W_in=np.random.randn(self.N_in,self.N)\n",
    "            self.W_in=torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
    "            \n",
    "        \n",
    "        self.Ws=[]\n",
    "        self.bs=[]\n",
    "        self.Ns=Ns\n",
    "        \n",
    "        for n in range(1,np.shape(Ns)[0]):\n",
    "            \n",
    "            self.Ws.append(nn.Parameter(torch.randn([Ns[n-1],Ns[n]],device=device)/torch.sqrt(Ns[n-1]+Ns[n])))\n",
    "            self.bs.append(nn.Parameter(torch.zeros([Ns[n]],device=device)))\n",
    "        \n",
    "        self.fb_ind=fb_ind\n",
    "        \n",
    "        self.W_fb=[]\n",
    "        self.W_fb.append(nn.Parameter(torch.randn([Ns[fb_ind],Ns[0]],device=device)/torch.sqrt(Ns[n-1]+Ns[n])))\n",
    "    \n",
    "        # Useful hyperparameters\n",
    "        self.nClass=nClass\n",
    "        self.batch_size = batch_size\n",
    "        self.nSampPerClassPerBatch = int(batch_size/nClass) # No. input samples per class, per batch\n",
    "        # # Setup target vector to compute Loss and Accuracy\n",
    "        # self.target = torch.zeros(self.batch_size).long().to(device)\n",
    "        # for j in range(1,nClass):\n",
    "        #     self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
    "        # Setup one-hot encoded target vector to compute Loss and Accuracy\n",
    "        self.target = torch.zeros(self.batch_size, nClass, dtype=torch.float32).to(device)\n",
    "        for j in range(1,nClass):\n",
    "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,j] = 1.\n",
    "    \n",
    "    def Initialize_Hyper(self,eta):\n",
    "        \n",
    "        self.eta=eta\n",
    "        \n",
    "        self.opt=optim.Adam([{ 'params': self.Ws+self.bs+self.W_fb, 'lr':eta }])\n",
    "    \n",
    "    \n",
    "    def Reset(self,s):\n",
    "        \n",
    "        batch_size=s.size()[0]\n",
    "        self.x=torch.zeros([batch_size,self.N],device=device)\n",
    "        self.xs=[]\n",
    "        self.xs.append(self.x)\n",
    "        \n",
    "        for n in range(1,np.shape(self.Ns)[0]-1):\n",
    "            \n",
    "            self.xs.append(torch.zeros([batch_size,self.Ns[n]],device=device))\n",
    "            if n==self.fb_ind:\n",
    "                \n",
    "                self.fb=torch.clone(self.xs[n])\n",
    "        \n",
    "    def Forward(self,s):\n",
    "        \n",
    "        \n",
    "        self.x=(1-self.alpha)*self.x+self.alpha*torch.tanh(torch.matmul(s,self.W_in)+torch.matmul(self.x,self.W)+\\\n",
    "                                                          torch.matmul(self.fb,self.W_fb[0]))\n",
    "        \n",
    "        self.xs[0]=self.x\n",
    "        for n in range(1,np.shape(self.Ns)[0]-1):\n",
    "            \n",
    "\n",
    "            self.xs[n]=torch.relu(torch.matmul(self.xs[n-1],self.Ws[n-1])+self.bs[n-1])\n",
    "            \n",
    "            if n==self.fb_ind:\n",
    "                \n",
    "                self.fb=torch.clone(self.xs[n])\n",
    "    \n",
    "        \n",
    "    def Response(self,Input,Targets):\n",
    "        \n",
    "        T=Input.shape[2]\n",
    "        Out=torch.zeros([Input.shape[0],self.Ns[-1],T],device=device)\n",
    "        \n",
    "        self.Reset(Input[:,0,0])\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for t in range(T):\n",
    "\n",
    "            self.Forward(Input[:,:,t])\n",
    "            \n",
    "            Out[:,:,t]=torch.matmul(self.xs[-1],self.Ws[-1])+self.bs[-1]\n",
    "            \n",
    "        Err=loss(Out[:,:,-1],Targets)\n",
    "\n",
    "        Err.backward(retain_graph=True)\n",
    "\n",
    "        Acc=torch.mean( torch.eq(torch.argmax(Targets,1),torch.argmax(Out[:,:,-1],1)).float() )                           \n",
    "\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "            \n",
    "        return Out, Err, Acc\n",
    "    \n",
    "    def Set_Transfer(self,Ns_transfer,eta_transfer):\n",
    "        \n",
    "        self.Ws_t=[]\n",
    "        self.bs_t=[]\n",
    "        self.Ns_transfer=Ns_transfer\n",
    "        \n",
    "        for n in range(1,np.shape(Ns_transfer)[0]):\n",
    "            \n",
    "            self.Ws_t.append(nn.Parameter(torch.randn([Ns_transfer[n-1],Ns_transfer[n]],device=device)/torch.sqrt(Ns_transfer[n-1]+Ns_transfer[n])))\n",
    "            self.bs_t.append(nn.Parameter(torch.zeros([Ns_transfer[n]],device=device)))\n",
    "        \n",
    "        self.opt_tr=optim.Adam([{ 'params': self.Ws_t+self.bs_t, 'lr':eta_transfer}])\n",
    "\n",
    "    def Transfer(self,Input,Targets):\n",
    "        \n",
    "        T=Input.shape[2]\n",
    "        X=torch.zeros([Input.shape[0],self.Ns[-2]],device=device)\n",
    "        self.Reset(Input[:,0,0])\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for t in range(T):\n",
    "\n",
    "                self.Forward(Input[:,:,t])\n",
    "                \n",
    "            X=torch.clone(self.xs[-1])\n",
    "        \n",
    "        Xs_transfer=[]\n",
    "        Xs_transfer.append(X)\n",
    "        \n",
    "        for n in range(1,np.shape(self.Ns_transfer)[0]-1):\n",
    "            \n",
    "            Xs_transfer.append(torch.relu(torch.matmul(self.Xs_transfer[n-1],self.Ws_t[n-1])+self.bs_t[n-1]))\n",
    "            \n",
    "        Out=torch.matmul(Xs_transfer[-1],self.Ws_t[-1])+self.bs_t[-1]\n",
    "        \n",
    "        Err=loss(Out,Targets)\n",
    "        \n",
    "        Err.backward(retain_graph=True)\n",
    "\n",
    "        Acc=torch.mean( torch.eq(torch.argmax(Targets,1),torch.argmax(Out,1)).float() )                           \n",
    "\n",
    "        self.opt_tr.step()\n",
    "        self.opt_tr.zero_grad()\n",
    "            \n",
    "        return Out, Err, Acc\n",
    "    \n",
    "    def generateBatch(self, method, X):\n",
    "       \n",
    "        ### For normal batches, e.g. for transfer learning\n",
    "        if method=='simple':\n",
    "            batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
    "            self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
    "            # for j in range(self.batch_size):\n",
    "            #     ind1 = torch.randint(0,self.nClass,[1])\n",
    "            #     ind2 = torch.randint(0,X[ind1].shape[0],[1])\n",
    "            #     batch[j,:,:] = torch.clone(X[ind1][ind2,:,:])\n",
    "            #     self.target[j,ind1] = 1.\n",
    "            for k in range(self.nClass):\n",
    "                rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
    "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
    "                self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
    "        ### For triplet loss\n",
    "        if method=='tripletLoss':\n",
    "            \n",
    "            # Populate batch\n",
    "            batch = torch.zeros([3*self.batch_size, self.nInputs,self.tMax]).to(device)\n",
    "            for k in range(self.nClass):\n",
    "                # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
    "                ind_ap = np.random.choice(X[k].shape[0],(self.nSampPerClassPerBatch,2), replace=False)\n",
    "                # Populate Anchor and Positive samples\n",
    "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,0],:])\n",
    "                batch[self.batch_size+k*self.nSampPerClassPerBatch:self.batch_size+(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,1],:])\n",
    "                # Populate negative samples\n",
    "                randClass = np.random.choice((np.arange(self.nClass)!=k).nonzero()[0], self.nSampPerClassPerBatch)\n",
    "                for m, cl in enumerate(randClass):\n",
    "                    batch[2*self.batch_size + k*self.nSampPerClassPerBatch+m,:] = torch.clone(X[cl][np.random.randint(X[cl].shape[0]),:])\n",
    "            \n",
    "        ### For Protoypical Loss\n",
    "        if method=='prototypicalLoss':\n",
    "            batch = torch.zeros((self.nP+self.nQ)*self.nClass, X[0].shape[1], X[0].shape[2]).to(device)\n",
    "            for j, x in enumerate(X): # For each class (X is a list)\n",
    "                ind = np.random.choice(x.shape[0], self.nP+self.nQ, replace=False)\n",
    "                batch[j*self.nP:(j+1)*self.nP,:] = torch.clone(x[ind[:self.nP],:])\n",
    "                batch[self.nP*self.nClass+j*self.nQ:self.nP*self.nClass+(j+1)*self.nQ] = torch.clone(x[ind[self.nP:],:])\n",
    "\n",
    "        return batch\n",
    "                           \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f35e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = NN.generateBatch('simple',X_tr)\n",
    "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "j=-8\n",
    "pl.imshow(batch[j,:,:].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in=X_tr[0].size()[1]\n",
    "N=500\n",
    "N_av=10\n",
    "alpha=0.8\n",
    "rho=0.99\n",
    "gamma=0.1\n",
    "Ns=torch.tensor([500,100,10])\n",
    "fb_ind=1\n",
    "\n",
    "eta=0.001\n",
    "N_train=5000\n",
    "batch_size=50\n",
    "nClass = 10\n",
    "NN=Model_James(N, N_in, N_av, alpha, rho, gamma, Ns, fb_ind, batch_size, nClass)\n",
    "NN.Initialize_Hyper(eta)\n",
    "\n",
    "Err=torch.zeros([N_train])\n",
    "Acc=torch.zeros([N_train])\n",
    "\n",
    "for n in range(N_train):\n",
    "    \n",
    "    batch = NN.generateBatch('simple',X_tr)\n",
    "    \n",
    "    out, err, acc=NN.Response(batch,NN.target)\n",
    "    \n",
    "    Err[n]=err.detach().to('cpu')\n",
    "    Acc[n]=acc.detach().to('cpu')\n",
    "    \n",
    "    if n%250==0:\n",
    "        \n",
    "        print(torch.mean(Err[n-100:n]), torch.mean(Acc[n-100:n]))\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    NN.Reset(X_tr)\n",
    "    for t in range(28):\n",
    "        NN.Forward(X_tr[:,:,t])\n",
    "    X=torch.clone(NN.xs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize=tuple(np.array((20.,10.))/2.54)); ax = pl.axes()\n",
    "ax.spines[['top','right']].set_visible(False)\n",
    "ind=(torch.argmax(Y_tr[:,:,0], 1)==3).nonzero().squeeze()\n",
    "pl.plot(X[ind[0:10],:].cpu().transpose(0,1), c=[1,0,0])\n",
    "ind=(torch.argmax(Y_tr[:,:,0], 1)==4).nonzero().squeeze()\n",
    "pl.plot(X[ind[0:10],:].cpu().transpose(0,1), c=[0,0,1])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff94ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns_transfer=torch.clone(Ns[-2:])\n",
    "\n",
    "N_transfer=1000\n",
    "Err_t=torch.zeros([N_transfer])\n",
    "Acc_t=torch.zeros([N_transfer])\n",
    "\n",
    "eta_transfer=0.001\n",
    "NN.Set_Transfer(Ns_transfer,eta_transfer)\n",
    "\n",
    "\n",
    "for n in range(N_transfer):\n",
    "    \n",
    "    rand_ind=np.random.randint(0,X_tr.size()[0],[batch_size])\n",
    "    X_b=X_tr[rand_ind,:,:]\n",
    "    Y_b=Y_tr[rand_ind,:]\n",
    "    \n",
    "    out, err, acc=NN.Transfer(X_b,Y_b)\n",
    "    \n",
    "    Err_t[n]=err.detach().to('cpu')\n",
    "    Acc_t[n]=acc.detach().to('cpu')\n",
    "    \n",
    "    if n%250==0:\n",
    "        \n",
    "        print(torch.mean(Err_t[n-100:n]), torch.mean(Acc_t[n-100:n]))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
