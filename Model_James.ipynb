{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa75b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te=mnist_testset.data               ## Test set images\n",
    "y_te=mnist_testset.test_labels        ## Test set labels\n",
    "\n",
    "N_o=10                                ## Number of output nodes/classes\n",
    "N_te=y_te.size()[0]                   ## Number of test samples\n",
    "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
    "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
    "\n",
    "X_tr=mnist_trainset.data              ## Train set images\n",
    "y_tr=mnist_trainset.train_labels      ## Train labels \n",
    "N_tr=y_tr.size()[0]                   \n",
    "\n",
    "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
    "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
    "\n",
    "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
    "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
    "\n",
    "X_val=X_tr[i_val,:,:]\n",
    "Y_val=Y_tr[i_val,:]\n",
    "\n",
    "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
    "N_tr=N_tr-N_val\n",
    "\n",
    "X_tr=X_tr[i_tr,:,:]\n",
    "Y_tr=Y_tr[i_tr,:]\n",
    "\n",
    "T=X_tr.size()[2]\n",
    "N_in=X_tr.size()[1]\n",
    "\n",
    "## Normalisation and conversion to float\n",
    "X_M=255\n",
    "\n",
    "X_tr=(X_tr.float()/X_M).to(device)\n",
    "X_val=(X_val.float()/X_M).to(device)\n",
    "X_te=(X_te.float()/X_M).to(device)\n",
    "\n",
    "Y_tr=torch.tile(Y_tr.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n",
    "Y_val=torch.tile(Y_val.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n",
    "Y_te=torch.tile(Y_te.float().unsqueeze(2),[1,1,X_tr.size()[2]]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_James(nn.Module):\n",
    "    \n",
    "    def __init__(self,N,N_in,N_av,alpha,rho,gamma,Ns,fb_ind):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.N=N\n",
    "        self.alpha=torch.tensor(alpha,device=device)\n",
    "        self.rho=rho\n",
    "        self.N_av=N_av\n",
    "        self.N_in=N_in\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        diluition=1-N_av/N\n",
    "        W=np.random.uniform(-1,1,[N,N])\n",
    "        W=W*(np.random.uniform(0,1,[N,N])>diluition)\n",
    "        eig=np.linalg.eigvals(W)\n",
    "        self.W=torch.from_numpy(self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
    "        \n",
    "        self.x=[]\n",
    "        \n",
    "        if self.N_in==1:\n",
    "            \n",
    "            self.W_in=2*np.random.randint(0,2,[self.N_in,self.N])-1\n",
    "            self.W_in=torch.from_numpy(self.W_in*self.gamma,device=device).float()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.W_in=np.random.randn(self.N_in,self.N)\n",
    "            self.W_in=torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
    "            \n",
    "        \n",
    "        self.Ws=[]\n",
    "        self.bs=[]\n",
    "        self.Ns=Ns\n",
    "        \n",
    "        for n in range(1,np.shape(Ns)[0]):\n",
    "            \n",
    "            self.Ws.append(nn.Parameter(torch.randn([Ns[n-1],Ns[n]],device=device)/torch.sqrt(Ns[n-1]+Ns[n])))\n",
    "            self.bs.append(nn.Parameter(torch.zeros([Ns[n]],device=device)))\n",
    "        \n",
    "        self.fb_ind=fb_ind\n",
    "        \n",
    "        self.W_fb=[]\n",
    "        self.W_fb.append(nn.Parameter(torch.randn([Ns[fb_ind],Ns[0]],device=device)/torch.sqrt(Ns[n-1]+Ns[n])))\n",
    "    \n",
    "    def Initialize_Hyper(self,eta):\n",
    "        \n",
    "        self.eta=eta\n",
    "        \n",
    "        self.opt=optim.Adam([{ 'params': self.Ws+self.bs+self.W_fb, 'lr':eta }])\n",
    "    \n",
    "    \n",
    "    def Reset(self,s):\n",
    "        \n",
    "        batch_size=s.size()[0]\n",
    "        self.x=torch.zeros([batch_size,self.N],device=device)\n",
    "        self.xs=[]\n",
    "        self.xs.append(self.x)\n",
    "        \n",
    "        for n in range(1,np.shape(self.Ns)[0]-1):\n",
    "            \n",
    "            self.xs.append(torch.zeros([batch_size,self.Ns[n]],device=device))\n",
    "            if n==self.fb_ind:\n",
    "                \n",
    "                self.fb=torch.clone(self.xs[n])\n",
    "        \n",
    "    def Forward(self,s):\n",
    "        \n",
    "        \n",
    "        self.x=(1-self.alpha)*self.x+self.alpha*torch.tanh(torch.matmul(s,self.W_in)+torch.matmul(self.x,self.W)+\\\n",
    "                                                          torch.matmul(self.fb,self.W_fb[0]))\n",
    "        \n",
    "        self.xs[0]=self.x\n",
    "        for n in range(1,np.shape(self.Ns)[0]-1):\n",
    "            \n",
    "\n",
    "            self.xs[n]=torch.relu(torch.matmul(self.xs[n-1],self.Ws[n-1])+self.bs[n-1])\n",
    "            \n",
    "            if n==self.fb_ind:\n",
    "                \n",
    "                self.fb=torch.clone(self.xs[n])\n",
    "    \n",
    "        \n",
    "    def Response(self,Input,Targets):\n",
    "        \n",
    "        T=Input.shape[2]\n",
    "        Out=torch.zeros([Input.shape[0],self.Ns[-1],T],device=device)\n",
    "        \n",
    "        self.Reset(Input[:,0,0])\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for t in range(T):\n",
    "\n",
    "            self.Forward(Input[:,:,t])\n",
    "            \n",
    "            Out[:,:,t]=torch.matmul(self.xs[-1],self.Ws[-1])+self.bs[-1]\n",
    "            \n",
    "        Err=loss(Out[:,:,-1],Targets[:,:,-1])\n",
    "\n",
    "        Err.backward(retain_graph=True)\n",
    "\n",
    "        Acc=torch.mean( torch.eq(torch.argmax(Targets[:,:,-1],1),torch.argmax(Out[:,:,-1],1)).float() )                           \n",
    "\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "            \n",
    "        return Out, Err, Acc\n",
    "    \n",
    "    def Set_Transfer(self,Ns_transfer,eta_transfer):\n",
    "        \n",
    "        self.Ws_t=[]\n",
    "        self.bs_t=[]\n",
    "        self.Ns_transfer=Ns_transfer\n",
    "        \n",
    "        for n in range(1,np.shape(Ns_transfer)[0]):\n",
    "            \n",
    "            self.Ws_t.append(nn.Parameter(torch.randn([Ns_transfer[n-1],Ns_transfer[n]],device=device)/torch.sqrt(Ns_transfer[n-1]+Ns_transfer[n])))\n",
    "            self.bs_t.append(nn.Parameter(torch.zeros([Ns_transfer[n]],device=device)))\n",
    "        \n",
    "        self.opt_tr=optim.Adam([{ 'params': self.Ws_t+self.bs_t, 'lr':eta_transfer}])\n",
    "\n",
    "    def Transfer(self,Input,Targets):\n",
    "        \n",
    "        T=Input.shape[2]\n",
    "        X=torch.zeros([Input.shape[0],self.Ns[-2]],device=device)\n",
    "        self.Reset(Input[:,0,0])\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for t in range(T):\n",
    "\n",
    "                self.Forward(Input[:,:,t])\n",
    "                \n",
    "            X=torch.clone(self.xs[-1])\n",
    "        \n",
    "        Xs_transfer=[]\n",
    "        Xs_transfer.append(X)\n",
    "        \n",
    "        for n in range(1,np.shape(self.Ns_transfer)[0]-1):\n",
    "            \n",
    "            Xs_transfer.append(torch.relu(torch.matmul(self.Xs_transfer[n-1],self.Ws_t[n-1])+self.bs_t[n-1]))\n",
    "            \n",
    "        Out=torch.matmul(Xs_transfer[-1],self.Ws_t[-1])+self.bs_t[-1]\n",
    "        \n",
    "        Err=loss(Out,Targets[:,:,-1])\n",
    "        \n",
    "        Err.backward(retain_graph=True)\n",
    "\n",
    "        Acc=torch.mean( torch.eq(torch.argmax(Targets[:,:,-1],1),torch.argmax(Out,1)).float() )                           \n",
    "\n",
    "        self.opt_tr.step()\n",
    "        self.opt_tr.zero_grad()\n",
    "            \n",
    "        return Out, Err, Acc\n",
    "                           \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in=X_tr.size()[1]\n",
    "N=500\n",
    "N_av=10\n",
    "alpha=0.8\n",
    "rho=0.99\n",
    "gamma=0.1\n",
    "Ns=torch.tensor([500,100,10])\n",
    "fb_ind=1\n",
    "\n",
    "NN=Model_James(N, N_in, N_av, alpha, rho, gamma, Ns, fb_ind)\n",
    "eta=0.001\n",
    "NN.Initialize_Hyper(eta)\n",
    "N_train=5000\n",
    "batch_size=50\n",
    "\n",
    "\n",
    "Err=torch.zeros([N_train])\n",
    "Acc=torch.zeros([N_train])\n",
    "\n",
    "for n in range(N_train):\n",
    "    \n",
    "    rand_ind=np.random.randint(0,X_tr.size()[0],[batch_size])\n",
    "    X_b=X_tr[rand_ind,:,:]\n",
    "    Y_b=Y_tr[rand_ind,:,:]\n",
    "    \n",
    "    out, err, acc=NN.Response(X_b,Y_b)\n",
    "    \n",
    "    Err[n]=err.detach().to('cpu')\n",
    "    Acc[n]=acc.detach().to('cpu')\n",
    "    \n",
    "    if n%250==0:\n",
    "        \n",
    "        print(torch.mean(Err[n-100:n]), torch.mean(Acc[n-100:n]))\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff94ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns_transfer=torch.clone(Ns[-2:])\n",
    "\n",
    "N_transfer=1000\n",
    "Err_t=torch.zeros([N_transfer])\n",
    "Acc_t=torch.zeros([N_transfer])\n",
    "\n",
    "eta_transfer=0.001\n",
    "NN.Set_Transfer(Ns_transfer,eta_transfer)\n",
    "\n",
    "\n",
    "for n in range(N_transfer):\n",
    "    \n",
    "    rand_ind=np.random.randint(0,X_tr.size()[0],[batch_size])\n",
    "    X_b=X_tr[rand_ind,:,:]\n",
    "    Y_b=Y_tr[rand_ind,:,:]\n",
    "    \n",
    "    out, err, acc=NN.Transfer(X_b,Y_b)\n",
    "    \n",
    "    Err_t[n]=err.detach().to('cpu')\n",
    "    Acc_t[n]=acc.detach().to('cpu')\n",
    "    \n",
    "    if n%250==0:\n",
    "        \n",
    "        print(torch.mean(Err_t[n-100:n]), torch.mean(Acc_t[n-100:n]))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
