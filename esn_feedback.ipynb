{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aW_6ADldkbuB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW_6ADldkbuB",
        "outputId": "fe8306c7-3a8e-4165-e62e-41a424a840b9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_QJywZpKBcrP",
      "metadata": {
        "id": "_QJywZpKBcrP"
      },
      "outputs": [],
      "source": [
        "directory='.'\n",
        "# directory='/content/drive/MyDrive/esn2sparse'; \n",
        "# !cp \"/content/drive/MyDrive/esn2sparse/params.py\" \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0caa84e4",
      "metadata": {
        "id": "0caa84e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from scipy import stats\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as pl\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import time\n",
        "import torch.jit as jit\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from os.path import exists\n",
        "import gc\n",
        "import importlib\n",
        "import params_feedback as par\n",
        "import time\n",
        "import os\n",
        "# device = 'cpu'\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75f2936",
      "metadata": {
        "id": "e75f2936"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root=directory+'/data', train=True, download=True, transform=None)\n",
        "mnist_testset = datasets.MNIST(root=directory+'/data', train=False, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c42c75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9c42c75",
        "outputId": "0928ebb6-40ac-4b31-b171-06853d2d74fa"
      },
      "outputs": [],
      "source": [
        "X_te=mnist_testset.data               ## Test set images\n",
        "y_te=mnist_testset.test_labels        ## Test set labels\n",
        "\n",
        "N_o=10                                ## Number of output nodes/classes\n",
        "N_te=y_te.size()[0]                   ## Number of test samples\n",
        "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
        "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
        "\n",
        "X_tr=mnist_trainset.data              ## Train set images\n",
        "y_tr=mnist_trainset.train_labels      ## Train labels \n",
        "N_tr=y_tr.size()[0]                   ## Number of training samples\n",
        "N_i = X_tr.size()[1]                  ## Number of inputs to ESN\n",
        "\n",
        "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
        "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
        "\n",
        "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
        "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
        "\n",
        "X_val=X_tr[i_val,:,:]\n",
        "Y_val=Y_tr[i_val,:]\n",
        "\n",
        "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
        "N_tr=N_tr-N_val\n",
        "\n",
        "X_tr=X_tr[i_tr,:,:]\n",
        "Y_tr=Y_tr[i_tr,:]\n",
        "\n",
        "T=X_tr.size()[2]\n",
        "N_in=X_tr.size()[1]\n",
        "\n",
        "## Normalisation and conversion to float\n",
        "X_M=255\n",
        "# X_tr=torch.reshape( (X_tr.float()/X_M),[-1,784]) \n",
        "# X_val=torch.reshape((X_val.float()/X_M),[-1,784])\n",
        "# X_te=torch.reshape((X_te.float()/X_M),[-1,784])\n",
        "X_tr=torch.reshape( (X_tr.float()),[-1,784]).to(device)\n",
        "X_val=torch.reshape((X_val.float()),[-1,784]).to(device)\n",
        "X_te=torch.reshape((X_te.float()),[-1,784]).to(device)\n",
        "\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])\n",
        "\n",
        "X_tr=torch.reshape( (X_tr),[-1,28,28]) \n",
        "X_val=torch.reshape((X_val),[-1,28,28])\n",
        "X_te=torch.reshape((X_te),[-1,28,28])\n",
        "\n",
        "Y_tr=Y_tr.float().to(device)\n",
        "Y_val=Y_val.float().to(device)\n",
        "Y_te=Y_te.float().to(device)\n",
        "\n",
        "print(f'X_tr shape = {X_tr.shape},    Y_tr shape = {Y_tr.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2148197e",
      "metadata": {
        "id": "2148197e"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data (OLD VERSION - results in outlines)\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,X_tr[j,:]==X_tr[j,:].min()] = X_tr[j,X_tr[j,:]==X_tr[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_tr[j,:].max() - X_tr[j,:].min())\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,X_val[j,:]==X_val[j,:].min()] = X_val[j,X_val[j,:]==X_val[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_val[j,:].max() - X_val[j,:].min())\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,X_te[j,:]==X_te[j,:].min()] = X_te[j,X_te[j,:]==X_te[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_te[j,:].max() - X_te[j,:].min())\n",
        "\n",
        "# Z-score inputs\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a64e52",
      "metadata": {
        "id": "d8a64e52"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data\n",
        "contrast_mean = torch.tensor(0.5).to(device)\n",
        "contrast_range = torch.tensor(0.99).to(device)\n",
        "# Training data\n",
        "for j in range(X_tr.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_tr[j,:] -= torch.min(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.max(X_tr[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_tr[j,:] = torch.tensor(1.0).to(device) - X_tr[j,:]\n",
        "    # Z-score\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= (torch.std(X_tr[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Validation data\n",
        "for j in range(X_val.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_val[j,:] -= torch.min(X_val[j,:])\n",
        "    X_val[j,:] /= torch.max(X_val[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_val[j,:] = torch.tensor(1.0).to(device) - X_val[j,:]\n",
        "    # Z-score\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= (torch.std(X_val[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Test data\n",
        "for j in range(X_te.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_te[j,:] -= torch.min(X_te[j,:])\n",
        "    X_te[j,:] /= torch.max(X_te[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_te[j,:] = torch.tensor(1.0).to(device) - X_te[j,:]\n",
        "    # Z-score\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= (torch.std(X_te[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a05756",
      "metadata": {
        "id": "04a05756"
      },
      "outputs": [],
      "source": [
        "# IF NOT USING ESN\n",
        "# Make inputs 2800-dim and z-score\n",
        "from scipy import stats\n",
        "with torch.no_grad():\n",
        "    w = torch.randn([784, par.N_esn*28]).to(device)/torch.sqrt(torch.tensor(par.N_esn*28+784)).to(device)\n",
        "    batchsize = torch.tensor(100).to(device)\n",
        "    # Operate on batches of data to save GPU-memory\n",
        "    # Training data\n",
        "    nbatch = torch.ceil(X_tr.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_tr.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_tr[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]),:,:].reshape(batchsize, X_tr.shape[1]*X_tr.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_tr = torch.clone(perm).cpu()\n",
        "    # Validation data\n",
        "    nbatch = torch.ceil(X_val.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_val.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_val[j*batchsize:min((j+1)*batchsize,X_val.shape[0]),:,:].reshape(batchsize, X_val.shape[1]*X_val.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_val.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_val = torch.clone(perm).cpu()\n",
        "    # Test data\n",
        "    nbatch = torch.ceil(X_te.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_te.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_te[j*batchsize:min((j+1)*batchsize,X_te.shape[0]),:,:].reshape(batchsize, X_te.shape[1]*X_te.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_te.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_te = torch.clone(perm).cpu()\n",
        "    \n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "\n",
        "    # X_tr = torch.tensor(stats.zscore(torch.matmul(X_tr.reshape(X_tr.shape[0], X_tr.shape[1]*X_tr.shape[2]), w).numpy(), axis=1))\n",
        "    # X_val = torch.tensor(stats.zscore(torch.matmul(X_val.reshape(X_val.shape[0], X_val.shape[1]*X_val.shape[2]), w).numpy(), axis=1))\n",
        "    # X_te = torch.tensor(stats.zscore(torch.matmul(X_te.reshape(X_te.shape[0], X_te.shape[1]*X_te.shape[2]), w).numpy(), axis=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f55bc9b",
      "metadata": {
        "id": "3f55bc9b"
      },
      "outputs": [],
      "source": [
        "def Data2Classes(X,Y):\n",
        "    \n",
        "    ind=torch.where(Y==1)[1]\n",
        "\n",
        "    N_class=torch.max(ind)+1\n",
        "    \n",
        "    X1=[]\n",
        "    Y1=[]\n",
        "    \n",
        "    for n in range(N_class):\n",
        "    \n",
        "        ind1=torch.where(ind==n)[0].type(torch.long)\n",
        "\n",
        "        X1.append(X[ind1,:].to(device))\n",
        "        Y1.append(Y[ind1,:].to(device))\n",
        "        \n",
        "    return X1, Y1\n",
        "        \n",
        "# X_tr/X_val/X_te are lists of length 10 (1 entry per class)\n",
        "X_tr, Y_tr=Data2Classes(X_tr,Y_tr)\n",
        "\n",
        "X_val, Y_val=Data2Classes(X_val,Y_val)\n",
        "\n",
        "X_te, Y_te=Data2Classes(X_te,Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhLHzeGYPsE1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "zhLHzeGYPsE1",
        "outputId": "c15a1dbb-6850-4f41-febc-a380d4d35bae"
      },
      "outputs": [],
      "source": [
        "# Plot activity and correlation between classes\n",
        "N_esn = par.N_esn\n",
        "a = np.zeros((1000,N_esn*28))\n",
        "for j in range(len(X_tr)):\n",
        "    a[j*100:(j+1)*100,:] = X_tr[j][0:100,:].numpy()\n",
        "print(np.max(a), np.min(a))\n",
        "c = np.matmul(stats.zscore(a,axis=1), np.transpose(stats.zscore(a,axis=1))) / a.shape[1]\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[0][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[2][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(c,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[0, 1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85bd1a87",
      "metadata": {},
      "source": [
        "xent: \n",
        " - train from layer 0 to layer -1\n",
        " - train forward pass to layer -1\n",
        " - train loss from layer -1\n",
        " - transfer from layer tranFrom to layer -1\n",
        " - transfer forward pass to layer -1\n",
        " - transfer loss from layer -1\n",
        "\n",
        "metric:\n",
        " - train from layer 0 to layer tranFrom-1\n",
        " - train forward pass to layer tranFrom-1\n",
        " - train loss from layer tranForm-1\n",
        " - transfer frmo layer tranFrom to layer -1\n",
        " - transfer forward pass to layer -1\n",
        " - transfer loss from layer -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6708d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPclassic(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        self.lossfn = nn.BCEWithLogitsLoss()\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)\n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.fbLayer = par.fbLayer\n",
        "        self.lossLayer = len(par.Ns)-1\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        \n",
        "        self.Ns = par.Ns\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/torch.sqrt(torch.tensor(self.Ns[self.fbLayer] + self.N))).to(device))\n",
        "\n",
        "    def initialOptimiser(self):\n",
        "        if self.fbLayer:\n",
        "            self.iniOpt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':self.etaInitial }])\n",
        "        else:\n",
        "            self.iniOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaInitial }])\n",
        "        \n",
        "    def Forward(self, input):\n",
        "        ### ESN\n",
        "        if self.fbLayer:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.xFB, self.W_fb))\n",
        "        else:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W))\n",
        "        \n",
        "        ### Hidden layers\n",
        "        for n in range(1,len(self.Ns)-1): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n] = torch.relu( torch.add(torch.matmul(self.x[n-1],self.Ws[n-1]),self.bs[n-1]) )\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "        \n",
        "        ### Output\n",
        "        self.x[-1] = torch.add(torch.matmul(self.x[-2],self.Ws[-1]),self.bs[-1])\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True).to(device))\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "    \n",
        "    def lossCrossEntropy(self, nSamples):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(self.x[-1]), torch.sum(torch.exp(self.x[-1]), 1, keepdim=True).tile((1,self.x[-1].shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "\n",
        "    def accuracyTarget(self):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(self.x[-1],1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self):\n",
        "        nSamples = self.x[-1].shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device)\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = self.x[-1][j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(self.x[-1][j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "    \n",
        "    def getLossAccuracy(self, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        L = self.lossCrossEntropy(self.x[-1].shape[0])\n",
        "        accTa = self.accuracyTarget()\n",
        "        accClCe = self.accuracyClassCentroid()\n",
        "\n",
        "        if backwardFlag:\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            \n",
        "        return L,accTa,accClCe\n",
        "    \n",
        "    def response(self, Input):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        \n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t])\n",
        "\n",
        "    # def generateBatch(self, X):\n",
        "       \n",
        "    #     batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
        "    #     self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
        "    #     # for j in range(self.batch_size):\n",
        "    #     #     ind1 = torch.randint(0,self.nClass,[1])\n",
        "    #     #     ind2 = torch.randint(0,X[ind1].shape[0],[1])\n",
        "    #     #     batch[j,:,:] = torch.clone(X[ind1][ind2,:,:])\n",
        "    #     #     self.target[j,ind1] = 1.\n",
        "\n",
        "    #     for k in range(self.nClass):\n",
        "    #         rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "    #         batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
        "    #         self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
        "\n",
        "    #     return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t])\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GWXIPFSI_Ynd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWXIPFSI_Ynd",
        "outputId": "00ee5616-9405-48e4-b18f-5efaa192ed57"
      },
      "outputs": [],
      "source": [
        "###\n",
        "### WITHOUT metric learning\n",
        "###\n",
        "def xent_esn_fb(expName,rngSeed):\n",
        "\n",
        "    ### Initiliase RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save weights every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2]\n",
        "\n",
        "    ###############################\n",
        "    #### First phase of training\n",
        "    ###############################\n",
        "\n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nWeightSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    savDW = [] # List of arrays to store DW histograms over time\n",
        "    for k in range(len(par.Ns)-1):\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPclassic(par)\n",
        "\n",
        "    MOD.initialOptimiser()\n",
        "    L_tr = []; L_tr.append(np.zeros([par.nEpisodes]))\n",
        "    A_trTa = []; A_trTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_trClCe = []; A_trClCe.append(np.zeros([par.nEpisodes]))\n",
        "    L_val = []; L_val.append(np.zeros([par.nEpisodes]))\n",
        "    A_valTa = []; A_valTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_valClCe = []; A_valClCe.append(np.zeros([par.nEpisodes]))\n",
        "\n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "    for n in range(par.nEpisodes):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodes-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpisodes}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li, l in enumerate(par.saveLayers):\n",
        "                    RESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights and (n%save_every)<1:\n",
        "            savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "        \n",
        "        # Get response to input\n",
        "        MOD.response(Im)\n",
        "        # Compute loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy(backwardFlag=True, opt=MOD.iniOpt)\n",
        "\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpisodes}')\n",
        "            return -1\n",
        "        #####\n",
        "        #####\n",
        "        \n",
        "        # Store loss and accuracy\n",
        "        L_tr[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_trTa[0][n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        A_trClCe[0][n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "\n",
        "        # Store weight changes for effective learning rate\n",
        "        if par.saveFlag_DW and (n%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "        if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    savDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "        \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Get response to input\n",
        "            MOD.response(Im)\n",
        "            # Compute loss and accuracy\n",
        "            loss, accValTa, accValClCe = MOD.getLossAccuracy()\n",
        "            \n",
        "            # Store loss and accuracy\n",
        "            L_val[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_valTa[0][n]=np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            A_valClCe[0][n]=np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "        \n",
        "        # Update learning rate\n",
        "        MOD.iniOpt.param_groups[0]['lr'] = par.etaInitial * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if ((n+1)%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrTa_mean=np.mean(A_trTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrClCe_mean=np.mean(A_trClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            mseVal_mean=np.mean(L_val[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValTa_mean=np.mean(A_valTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValClCe_mean=np.mean(A_valClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n+1)/np.float32(par.nEpisodes)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                                           Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ### Save hidden layer representations for transfer learning\n",
        "    H_tr = []\n",
        "    H_val = []\n",
        "    with torch.no_grad():\n",
        "        for j in range(par.nClass):\n",
        "            MOD.response(X_tr[j])\n",
        "            H_tr.append(MOD.x[par.saveRepLayer])\n",
        "            MOD.response(X_val[j])\n",
        "            H_val.append(MOD.x[par.saveRepLayer])\n",
        "    \n",
        "\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD, outputDir + '/' + 'model'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    torch.save([H_tr, H_val], outputDir + '/' + 'hResp'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa4cf3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPmetric(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)    \n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.nInputs = par.nInputs\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.tMax = par.tMax\n",
        "        self.fbLayer = par.fbLayer\n",
        "        self.metricLossType = par.metricLossType\n",
        "        self.margin = par.margin\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        self.batch_size = par.batch_size\n",
        "        if par.metricLossType=='prototypicalLoss':\n",
        "            self.nP = par.nSampProto\n",
        "            self.nQ = int(par.batch_size / par.nClass)\n",
        "        self.tri = 1 if par.metricLossType=='tripletLoss' else 0\n",
        "        self.wPerf = torch.exp(-torch.arange(self.tMax).flip(0)/par.tauPerf)\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ns=par.Ns\n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/10**4).to(device))\n",
        "\n",
        "    def metricOptimiser(self):\n",
        "\n",
        "        if self.fbLayer:\n",
        "            self.metOpt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':self.etaInitial }])\n",
        "        else:\n",
        "            self.metOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaInitial }])\n",
        "\n",
        "    def Forward(self, input):\n",
        "\n",
        "        ### ESN\n",
        "        if self.fbLayer:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.xFB, self.W_fb))\n",
        "        else:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W))\n",
        "        \n",
        "        ### Hidden layers\n",
        "        for n in range(1,len(self.Ns)): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n] = torch.relu( torch.add(torch.matmul(self.x[n-1],self.Ws[n-1]),self.bs[n-1]) )\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True).to(device))\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "\n",
        "    def lossCrossEntropy(self, nSamples, r):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(r), torch.sum(torch.exp(r), 1, keepdim=True).tile((1,r.shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "    \n",
        "    def tripletLoss(self, resp):\n",
        "        # Implement hard negative mining\n",
        "\n",
        "        dAP = (resp[0] - resp[1]).pow(2).sum(1).sqrt() # anchor-positive\n",
        "        dAN = (resp[0] - resp[2]).pow(2).sum(1).sqrt() # anchor-negative\n",
        "        \n",
        "        dPN = (resp[1] - resp[2]).pow(2).sum(1).sqrt() # positive-negative\n",
        "        ind = torch.le(dPN, dAN)\n",
        "        ind1 = torch.nonzero(ind)\n",
        "        ind2 = torch.nonzero(torch.logical_not(ind))\n",
        "        \n",
        "        L = torch.concat((torch.maximum(torch.zeros(ind2.shape).to(device), dAP[ind2] - dAN[ind2] + self.margin), \n",
        "                         torch.maximum(torch.zeros(ind1.shape).to(device), dAP[ind1] - dPN[ind1] + self.margin)), dim=0).mean()\n",
        "        # if t%50==0:\n",
        "        #     print(f'DP = {dAP.mean()}                            DN = {dAN.mean()}                           maxRESP = {resp[0].max()}')\n",
        "\n",
        "        return L.mean()\n",
        "    \n",
        "    def prototypicalLoss(self, r):\n",
        "        proto = torch.zeros(self.N_class, self.Ns[-1]).to(device) # init mem for prototypes\n",
        "        dist = torch.zeros(self.N_class*self.nQ, self.N_class).to(device) # init mem for distances\n",
        "        prob = torch.zeros(self.N_class*self.nQ).to(device) # Init mem for probabilities\n",
        "        # Compute prototypes\n",
        "        for j in range(self.N_class):\n",
        "            proto[j,:] = torch.clone(r[j*self.nP:(j+1)*self.nP,:]).mean(0)\n",
        "        for j in range(self.N_class): # for each class\n",
        "            for k in range(self.nQ): # for each query in class j\n",
        "                # Compute distances between queries and prototypes\n",
        "                fx = torch.clone(r[self.nP*self.N_class + j*self.nQ + k,:]).unsqueeze(0).tile(self.N_class,1)\n",
        "                dist[j*self.nQ+k,:] = (fx - proto).pow(2).sum(1).sqrt()\n",
        "            # Compute probabilities using softmax\n",
        "            prob[j*self.nQ:(j+1)*self.nQ] = torch.divide(torch.exp(-dist[j*self.nQ:(j+1)*self.nQ,j]), \n",
        "                                               torch.sum(torch.exp(-dist[j*self.nQ:(j+1)*self.nQ,:]), 1))\n",
        "        \n",
        "        L = - torch.log(prob)\n",
        "\n",
        "        return L.mean(), proto\n",
        "    \n",
        "    def accuracyTarget(self, r):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(r,1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self, r, prototypes=[]):\n",
        "        nSamples = r.shape[0]\n",
        "        # Compute class centroids\n",
        "        if len(prototypes)>0:\n",
        "            centroids = prototypes\n",
        "        else:\n",
        "            centroids = torch.zeros(self.N_class, r.shape[1]).to(device)\n",
        "            for j in range(self.N_class):\n",
        "                centroids[j,:] = r[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(r[j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "\n",
        "    def getLossAccuracy(self, learning, r, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        if learning=='metric':\n",
        "            if self.metricLossType=='tripletLoss':\n",
        "                L = self.tripletLoss(r)\n",
        "                accTa = self.accuracyTarget(r[0])\n",
        "                accClCe = self.accuracyClassCentroid(r[0])\n",
        "            elif self.metricLossType=='prototypicalLoss':\n",
        "                L, proto = self.prototypicalLoss(r[0])\n",
        "                accTa = self.accuracyTarget(r[0][self.N_class*self.nP:,:])\n",
        "                accClCe = self.accuracyClassCentroid(r[0][self.N_class*self.nP:,:], proto)\n",
        "                \n",
        "        elif learning=='transfer':\n",
        "            L = self.lossCrossEntropy(r.shape[0], r)\n",
        "            accTa = self.accuracyTarget(r)\n",
        "            accClCe = self.accuracyClassCentroid(r)\n",
        "\n",
        "        if backwardFlag:          \n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        return L,accTa,accClCe\n",
        "      \n",
        "    def response(self, Input, tripletFlag=False):\n",
        "\n",
        "        if tripletFlag:\n",
        "            N_samples = int(Input.shape[0] / 3)\n",
        "        else:\n",
        "            N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "\n",
        "        # Forward pass for anchor, positive, and negative\n",
        "        if tripletFlag:\n",
        "            self.Reset(N_samples * 3)\n",
        "        else:\n",
        "            self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:,:,t])\n",
        "\n",
        "        # Compile list of responses from lossLayer. When using Triplet Loss, \n",
        "        # list is 3x1 for [anchor, positive, negative]\n",
        "        r = [] \n",
        "        for j in range(1+2*int(tripletFlag)):\n",
        "            r.append(torch.clone(self.x[-1][j*N_samples:(j+1)*N_samples,:]))\n",
        "        \n",
        "        return r\n",
        "    \n",
        "    def generateBatch(self, method, X):\n",
        "       \n",
        "        ### For normal batches, e.g. for transfer learning\n",
        "        if method=='simple':\n",
        "            batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
        "            self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
        "            for k in range(self.nClass):\n",
        "                rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
        "                self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
        "        \n",
        "        ### For triplet loss\n",
        "        if method=='tripletLoss':\n",
        "            \n",
        "            # Populate batch\n",
        "            batch = torch.zeros([3*self.batch_size, self.nInputs,self.tMax]).to(device)\n",
        "            for k in range(self.N_class):\n",
        "                # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
        "                ind_ap = np.random.choice(X[k].shape[0],(self.nSampPerClassPerBatch,2), replace=False)\n",
        "                # Populate Anchor and Positive samples\n",
        "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,0],:])\n",
        "                batch[self.batch_size+k*self.nSampPerClassPerBatch:self.batch_size+(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,1],:])\n",
        "                # Populate negative samples\n",
        "                randClass = np.random.choice((np.arange(self.N_class)!=k).nonzero()[0], self.nSampPerClassPerBatch)\n",
        "                for m, cl in enumerate(randClass):\n",
        "                    batch[2*self.batch_size + k*self.nSampPerClassPerBatch+m,:] = torch.clone(X[cl][np.random.randint(X[cl].shape[0]),:])\n",
        "            \n",
        "        ### For Protoypical Loss\n",
        "        if method=='prototypicalLoss':\n",
        "            batch = torch.zeros((self.nP+self.nQ)*self.N_class, X[0].shape[1], X[0].shape[2]).to(device)\n",
        "            for j, x in enumerate(X): # For each class (X is a list)\n",
        "                ind = np.random.choice(x.shape[0], self.nP+self.nQ, replace=False)\n",
        "                batch[j*self.nP:(j+1)*self.nP,:] = torch.clone(x[ind[:self.nP],:])\n",
        "                batch[self.nP*self.N_class+j*self.nQ:self.nP*self.N_class+(j+1)*self.nQ] = torch.clone(x[ind[self.nP:],:])\n",
        "\n",
        "        return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:,:,t])\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f140603c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# swLR = 0.00001#0.00046 # Sweep over these learning rates\n",
        "# rngSeed = 11117 # No. runs per hyperparameter\n",
        "\n",
        "# # Update hyperparameter\n",
        "# importlib.reload(par)\n",
        "# par.eta = swLR\n",
        "# par.fbLayer = 2\n",
        "# par.saveFlag_FBWeights = True\n",
        "# par.nEpisodes = 50\n",
        "# par.nSaveMaxT = par.nEpisodes\n",
        "# expName = 'test'\n",
        "###\n",
        "### WITH metric learning\n",
        "###\n",
        "def metric_esn_fb(expName, rngSeed):\n",
        "\n",
        "    ### Initialise RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save data every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2] # No. time steps per input sequence\n",
        "    \n",
        "    ###############################\n",
        "    #### First phase of training\n",
        "    ###############################\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        print('Saving weights')\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nWeightSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    savDW = [] # List of arrays to store DW histograms over time\n",
        "    for k in range(len(par.Ns)-1):\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPmetric(par)\n",
        "    MOD.metricOptimiser()\n",
        "\n",
        "    ### Init memory for saving data\n",
        "    L_tr = []; L_tr.append(np.zeros([par.nEpisodes]))\n",
        "    A_trTa = []; A_trTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_trClCe = []; A_trClCe.append(np.zeros([par.nEpisodes]))\n",
        "    L_val = []; L_val.append(np.zeros([par.nEpisodes]))\n",
        "    A_valTa = []; A_valTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_valClCe = []; A_valClCe.append(np.zeros([par.nEpisodes]))\n",
        "    \n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpisodes):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodes-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpisodes}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li, l in enumerate(par.saveLayers):\n",
        "                    RESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights and (n%save_every)<1:\n",
        "            savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        Im = MOD.generateBatch(par.metricLossType, X_tr)\n",
        "\n",
        "        # Get response to input\n",
        "        if MOD.metricLossType=='tripletLoss':\n",
        "            r = MOD.response(Im, tripletFlag=True)\n",
        "        else:\n",
        "            r = MOD.response(Im)\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy('metric',r,backwardFlag=True, opt=MOD.metOpt)\n",
        "\n",
        "        ####\n",
        "        #### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpisodes}')\n",
        "            return -1\n",
        "        ####\n",
        "        ####\n",
        "\n",
        "        # Store training loss and accuracy\n",
        "        L_tr[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_trTa[0][n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        A_trClCe[0][n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "\n",
        "        # Store weight changes for effective learning rate\n",
        "        with torch.no_grad():\n",
        "            if par.saveFlag_DW and (n%save_every)<1:\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(len(MOD.Ns)-1):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "            if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(len(MOD.Ns)-1):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    savDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "\n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            Im = MOD.generateBatch(par.metricLossType, X_val)\n",
        "\n",
        "            # Get response to input\n",
        "            if MOD.metricLossType=='tripletLoss':\n",
        "                r = MOD.response(Im, tripletFlag=True)\n",
        "            else:\n",
        "                r = MOD.response(Im)\n",
        "                \n",
        "            # Compute loss and accuracy\n",
        "            loss, accValTa, accValClCe = MOD.getLossAccuracy('metric',r)\n",
        "\n",
        "            # Store validation loss and accuracy\n",
        "            L_val[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_valTa[0][n]=np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            A_valClCe[0][n]=np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.metOpt.param_groups[0]['lr'] = par.etaInitial * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if ((n+1)%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrTa_mean=np.mean(A_trTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrClCe_mean=np.mean(A_trClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            mseVal_mean=np.mean(L_val[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValTa_mean=np.mean(A_valTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValClCe_mean=np.mean(A_valClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n+1)/np.float32(par.nEpisodes)*100.:.3}%   Tr-Loss: {mseTr_mean}, Val-Loss: {mseVal_mean};  Tr-AccTa: {accTrTa_mean}, Val-AccTa: {accValTa_mean}')\n",
        "            print(f'          Tr-AccClCe: {accTrClCe_mean}, Val-AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ### Save hidden layer representations for transfer learning\n",
        "    H_tr = []\n",
        "    H_val = []\n",
        "    with torch.no_grad():\n",
        "        for j in range(par.nClass):\n",
        "            MOD.response(X_tr[j])\n",
        "            H_tr.append(MOD.x[-1])\n",
        "            MOD.response(X_val[j])\n",
        "            H_val.append(MOD.x[-1])\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD, outputDir + '/' + 'model'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    torch.save([H_tr, H_val], outputDir + '/' + 'hResp'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4c3949",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPtransfer(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        self.lossfn = nn.BCEWithLogitsLoss()\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)\n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.N_i = par.nInputs\n",
        "        self.lossLayer = len(par.Ns)-1\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        \n",
        "        self.Ns = par.Ns\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "    def transferOptimiser(self):\n",
        "        \n",
        "        self.tranOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaTransfer }])\n",
        "        \n",
        "    def Forward(self, input):\n",
        "        \n",
        "        self.x[0] = torch.clone(input)\n",
        "        ### Hidden layers\n",
        "        for n in range(1,len(self.Ns)-1): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n] = torch.relu( torch.add(torch.matmul(self.x[n-1],self.Ws[n-1]),self.bs[n-1]) )\n",
        "        \n",
        "        ### Output\n",
        "        self.x[-1] = torch.add(torch.matmul(self.x[-2],self.Ws[-1]),self.bs[-1])\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True).to(device))\n",
        "    \n",
        "    def lossCrossEntropy(self, nSamples):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(self.x[-1]), torch.sum(torch.exp(self.x[-1]), 1, keepdim=True).tile((1,self.x[-1].shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "\n",
        "    def accuracyTarget(self):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(self.x[-1],1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self):\n",
        "        nSamples = self.x[-1].shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device)\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = self.x[-1][j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(self.x[-1][j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "    \n",
        "    def getLossAccuracy(self, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        L = self.lossCrossEntropy(self.x[-1].shape[0])\n",
        "        accTa = self.accuracyTarget()\n",
        "        accClCe = self.accuracyClassCentroid()\n",
        "\n",
        "        if backwardFlag:\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            \n",
        "        return L,accTa,accClCe\n",
        "    \n",
        "    def response(self, Input):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        \n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t])\n",
        "\n",
        "    # def generateBatch(self, X):\n",
        "       \n",
        "    #     batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
        "    #     self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
        "    #     # for j in range(self.batch_size):\n",
        "    #     #     ind1 = torch.randint(0,self.nClass,[1])\n",
        "    #     #     ind2 = torch.randint(0,X[ind1].shape[0],[1])\n",
        "    #     #     batch[j,:,:] = torch.clone(X[ind1][ind2,:,:])\n",
        "    #     #     self.target[j,ind1] = 1.\n",
        "\n",
        "    #     for k in range(self.nClass):\n",
        "    #         rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "    #         batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
        "    #         self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
        "\n",
        "    #     return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t], t)\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd0c2ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################\n",
        "#### Transfer Learning\n",
        "###############################\n",
        "def tran_esn_fb(expName, rngSeed):\n",
        "    \n",
        "    ### Initiliase RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment # Storage directory for inputs\n",
        "    outputDir = directory+'/data/'+'tran_'+experiment # Storage directory for outputs\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir) \n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Load inputs\n",
        "    print(f'Loading inputs from: {expDir}/hResp{str(rngSeed)}.pt')\n",
        "    inputs = torch.load(expDir + '/' + 'hResp'+str(rngSeed)+'.pt')\n",
        "    X_tr = inputs[0]\n",
        "    X_val = inputs[1]\n",
        "    print(f'Shape of X_tr is {X_tr[0].shape}')\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save weights every <> epochs, up to epoch nSaveMaxT\n",
        "    if len(X_tr[0].shape) > 2:\n",
        "        tMax = X_tr[0].shape[2]\n",
        "    else:\n",
        "        tMax = 0\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPtransfer(par)\n",
        "\n",
        "    ### Init lists for saving data\n",
        "    outL_tr = np.zeros([par.nEpisodesTran])\n",
        "    outA_trTa = np.zeros([par.nEpisodesTran])\n",
        "    outA_trClCe = np.zeros([par.nEpisodesTran])\n",
        "    outL_val = np.zeros([par.nEpisodesTran])\n",
        "    outA_valTa = np.zeros([par.nEpisodesTran])\n",
        "    outA_valClCe = np.zeros([par.nEpisodesTran])\n",
        "    outsavDW = []\n",
        "\n",
        "    print(f'**********************START TRANSFER TRAINING')\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        outRESP = []\n",
        "        for layer in range(len(par.Ns)):\n",
        "            if len(X_tr[0].shape) > 2:\n",
        "                outRESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "            else:\n",
        "                outRESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer]))) \n",
        "\n",
        "    # Init memory to save dw histograms\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    for k in range(len(par.Ns)-1):\n",
        "        outsavDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        outsavDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise optimiser\n",
        "    MOD.transferOptimiser()\n",
        "    \n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpisodesTran):\n",
        "        # ### Save output responses before updates\n",
        "        # if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodesTran-1)):\n",
        "        #     print(f'Saving response on iteration {n} of {par.nEpisodesTran}')\n",
        "        #     with torch.no_grad():\n",
        "        #         s = X_tr[0][0:par.nSaveSamples,:]\n",
        "        #         for k in range(1,par.nClass):\n",
        "        #             s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "        #         resp = MOD.responseSave(s, range(len(par.Ns)))\n",
        "        #         for li, l in enumerate(range(len(par.Ns))):\n",
        "        #             outRESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "        #     resp_saveind += 1\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "\n",
        "        # Get response to input\n",
        "        MOD.Reset(Im.shape[0])\n",
        "        with torch.no_grad():\n",
        "            MOD.x[0] = torch.clone(Im)\n",
        "        try:\n",
        "            MOD.Forward(Im)\n",
        "        except:\n",
        "            print(f'Size of layer 0 activity: {MOD.x[0].shape}')\n",
        "            for k in range(len(par.Ns)-1):\n",
        "                print(f'Size of layer {k} activity: {MOD.x[k+1].shape}')\n",
        "                print(f'Size of layer {k} weights: {MOD.Ws[k].shape}')\n",
        "        \n",
        "        # Compute training loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy(backwardFlag=True, opt=MOD.tranOpt)\n",
        "        \n",
        "        ####\n",
        "        #### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpisodesTran}')\n",
        "            return -1\n",
        "        ####\n",
        "        ####\n",
        "\n",
        "        # Store loss and accuracy\n",
        "        outL_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        outA_trTa[n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        outA_trClCe[n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "        \n",
        "        # Store weight changes for effective learning rate\n",
        "        with torch.no_grad():\n",
        "            if par.saveFlag_DW and (n%save_every)<1:\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "            if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    outsavDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "            \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Get response to input\n",
        "            MOD.Reset(Im.shape[0])\n",
        "            MOD.x[0] = Im\n",
        "            MOD.Forward(Im)\n",
        "\n",
        "            # Compute validation loss and accuracy\n",
        "            loss,accValTa,accValClCe = MOD.getLossAccuracy()\n",
        "\n",
        "            # Store loss and accuracy\n",
        "            outL_val[n] = np.copy(np.array(loss.to('cpu').detach()))\n",
        "            outA_valTa[n] = np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            outA_valClCe[n] = np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.tranOpt.param_groups[0]['lr'] = par.etaTransfer * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if ((n+1)%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(outL_tr[(n+1)-par.reportTime:(n+1)])\n",
        "            accTrTa_mean=np.mean(outA_trTa[(n+1)-par.reportTime:(n+1)])\n",
        "            accTrClCe_mean=np.mean(outA_trClCe[(n+1)-par.reportTime:(n+1)])\n",
        "            mseVal_mean=np.mean(outL_val[(n+1)-par.reportTime:(n+1)])\n",
        "            accValTa_mean=np.mean(outA_valTa[(n+1)-par.reportTime:(n+1)])\n",
        "            accValClCe_mean=np.mean(outA_valClCe[(n+1)-par.reportTime:(n+1)])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n+1)/np.float32(par.nEpisodesTran)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                               Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "    \n",
        "    wTran = [torch.clone(MOD.Ws[-1]).detach().cpu().numpy(), torch.clone(MOD.bs[-1]).detach().cpu().numpy()]\n",
        "\n",
        "    torch.save(outL_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outL_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outRESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(outsavDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    torch.save(wTran, outputDir + '/' + 'wTran'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c1af1725",
      "metadata": {},
      "source": [
        "# Test run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e95e7dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "swLR = 0.001#0.00046 # Sweep over these learning rates\n",
        "seed = 11117 # No. runs per hyperparameter\n",
        "### Initial training\n",
        "# Update hyperparameter\n",
        "importlib.reload(par)\n",
        "par.etaInitial = swLR\n",
        "par.etaTransfer = 0.001\n",
        "par.fbLayer = 2\n",
        "par.saveRepLayer = 2\n",
        "par.saveFlag_FBWeights = False if not par.fbLayer else True # Save feedback weights\n",
        "par.nEpisodes = 5000\n",
        "par.reportTime = 250\n",
        "# par.Ns = [par.N_esn, 100, par.nClass] # No. neurons in each layer - FOR XENT\n",
        "par.Ns = [par.N_esn, 100, 100] # No. neurons in each layer - FOR METRIC\n",
        "par.nSaveMaxT = par.nEpisodes \n",
        "par.metricLossType = 'prototypicalLoss'\n",
        "expName = 'test'\n",
        "# xent_esn_fb(expName, seed)\n",
        "metric_esn_fb(expName, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e8063e",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Test Transfer learning\n",
        "par.nEpisodesTran = 1000\n",
        "par.etaTransfer = 0.001\n",
        "par.Ns = [100,par.nClass]\n",
        "par.nInputs = 100\n",
        "par.fbLayer = []\n",
        "tran_esn_fb(expName, seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa8aa35e",
      "metadata": {},
      "source": [
        "# Run parameter sweep (CrossEntropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c625fc92",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-15/3,-7/3,5) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "modName = 'xenFB1'\n",
        "experiment = 'swLR'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.saveFlag_FBWeights = True\n",
        "        par.nSaveMaxT = par.nEpisodes \n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = 1\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************FB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB1********************************')\n",
        "\n",
        "modName = 'xenFB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.saveFlag_FBWeights = True\n",
        "        par.nSaveMaxT = par.nEpisodes \n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = []\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************FB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB0********************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "80762c57",
      "metadata": {},
      "source": [
        "# Run parameter sweep (MetricLearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30567a8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-15/3,-7/3,5) # Sweep over these learning rates\n",
        "nSeeds = 2 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "experiment = 'swLR'\n",
        "losstype = 'prototypicalLoss'\n",
        "if losstype=='tripletLoss':\n",
        "    lossSuffix = 'tri'\n",
        "elif losstype=='prototypicalLoss':\n",
        "    lossSuffix = 'pro'\n",
        "\n",
        "modName = lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [par.N_esn, 100]\n",
        "        par.metricLossType = 'prototypicalLoss'\n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = 1\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************metFB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB1********************************')\n",
        "\n",
        "modName = lossSuffix+'FB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [par.N_esn, 100]\n",
        "        par.metricLossType = 'prototypicalLoss'\n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = []\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************metFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB0********************************')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0550739c",
      "metadata": {},
      "source": [
        "# Run transfer learning, using learned representations as inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7da07e",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Transfer learning\n",
        "swLR = np.logspace(-15/3,-7/3,5) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "experiment = 'swLR'\n",
        "losstype = 'xentropy'\n",
        "if losstype=='tripletLoss':\n",
        "    lossSuffix = 'tri'\n",
        "elif losstype=='prototypicalLoss':\n",
        "    lossSuffix = 'pro'\n",
        "elif losstype=='xentropy':\n",
        "    lossSuffix = 'xen'\n",
        "\n",
        "###\n",
        "### Learned representations using CROSS-ENTROPY Loss with FB1\n",
        "###\n",
        "modName = lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "c = torch.load('/its/home/jb739/esn_feedback/data/xenFB1_swLR_0/complete.pt')\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        if c[j,k]>0:\n",
        "            t = time.time()\n",
        "            # Update hyperparameters\n",
        "            importlib.reload(par)\n",
        "            par.Ns = [100, par.nClass]\n",
        "            par.nInputs = 100\n",
        "            par.fbLayer = []\n",
        "            expName = expNameSuffix+str(j)\n",
        "            complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        else:\n",
        "            complete[j, k] = -1\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_xenFB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/tran_'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "###\n",
        "### Learned representations using CROSS-ENTROPY Loss with FB0\n",
        "###\n",
        "modName = lossSuffix+'FB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "c = torch.load('/its/home/jb739/esn_feedback/data/xenFB0_swLR_0/complete.pt')\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        if c[j,k]>0:\n",
        "            t = time.time()\n",
        "            # Update hyperparameters\n",
        "            importlib.reload(par)\n",
        "            par.Ns = [100, par.nClass]\n",
        "            par.nInputs = 100\n",
        "            par.fbLayer = []\n",
        "            expName = expNameSuffix+str(j)\n",
        "            complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        else:\n",
        "            complete[j, k] = -1\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_xenFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/tran_'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "losstype = 'prototypicalLoss'\n",
        "if losstype=='tripletLoss':\n",
        "    lossSuffix = 'tri'\n",
        "elif losstype=='prototypicalLoss':\n",
        "    lossSuffix = 'pro'\n",
        "elif losstype=='xentropy':\n",
        "    lossSuffix = 'xen'\n",
        "\n",
        "nSeeds = 2 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "###\n",
        "### Learned representations using PROTOTYPICAL Loss with FB1\n",
        "###\n",
        "modName = lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "c = torch.load('/its/home/jb739/esn_feedback/data/proFB1_swLR_0/complete.pt')\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        if c[j,k]>0:\n",
        "            t = time.time()\n",
        "            # Update hyperparameters\n",
        "            importlib.reload(par)\n",
        "            par.Ns = [100, par.nClass]\n",
        "            par.nInputs = 100\n",
        "            par.fbLayer = []\n",
        "            expName = expNameSuffix+str(j)\n",
        "            complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        else:\n",
        "            complete[j, k] = -1\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_proFB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/tran_'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "###\n",
        "### Learned representations using PROTOTYPICAL Loss with FB0\n",
        "###\n",
        "modName = lossSuffix+'FB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "c = torch.load('/its/home/jb739/esn_feedback/data/proFB0_swLR_0/complete.pt')\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        if c[j,k]>0:\n",
        "            t = time.time()\n",
        "            # Update hyperparameters\n",
        "            importlib.reload(par)\n",
        "            par.Ns = [100, par.nClass]\n",
        "            par.nInputs = 100\n",
        "            par.fbLayer = []\n",
        "            expName = expNameSuffix+str(j)\n",
        "            complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        else:\n",
        "            complete[j, k] = -1\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_proFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/tran_'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "04ddc697",
      "metadata": {},
      "source": [
        "### Run multiple cases of transfer learning per trained network, to investigate whether performance is hindered by getting stuck in local minima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87f370f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-5,-7/3,7) # LRs used for sweep above\n",
        "swLR = [swLR[4]]\n",
        "nSeeds = 1 # No. runs per hyperparameter\n",
        "\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for k, sd in enumerate(seeds):\n",
        "    t = time.time()\n",
        "    # Update hyperparameter\n",
        "    importlib.reload(par)\n",
        "    par.saveFlag_FBWeights = True\n",
        "    par.maxLayer = 3\n",
        "    par.lossLayer = 3\n",
        "    par.nSaveMaxT = par.nEpisodes \n",
        "    par.etaInitial = swLR[0]\n",
        "    par.fbLayer = 2\n",
        "    par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "    par.nTransferRuns = 10\n",
        "    expName = 'FB2_LocMin'\n",
        "    complete = xent_esn_fb(expName, sd)\n",
        "    print(f'k={k}/{len(seeds)}')\n",
        "    print(f'*************************FB2_LocMin Run time: {time.time()-t}')\n",
        "\n",
        "print('*******************************FINISHED FB2********************************')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19b65aae",
      "metadata": {
        "id": "19b65aae"
      },
      "source": [
        "# Plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600e634a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600e634a",
        "outputId": "9cd64612-b412-4422-f3f5-f179e36a7907"
      },
      "outputs": [],
      "source": [
        "# ### Load Triplet data\n",
        "# inputDir = directory+'/data/'+experiment    # Storage directory for input/label data\n",
        "# met = torch.load(inputDir + 'met_save.pt')\n",
        "# l_tri = torch.load(inputDir + 'loss_triplet.pt')\n",
        "# l_out = torch.load(inputDir + 'loss_out.pt')\n",
        "# a_tri = torch.load(inputDir + 'acc_triplet.pt')\n",
        "# a_out = torch.load(inputDir + 'acc_out.pt')\n",
        "# dist = torch.load(inputDir + 'dist_triplet.pt')\n",
        "# nt_dist = dist.shape[0] \n",
        "# nt_out = l_out.shape[0]\n",
        "# nt_met = met.shape[0]\n",
        "# N_triplet=45000\n",
        "# N_out=5000\n",
        "# batch_size=64\n",
        "# eta_t=0.0002\n",
        "# eta_o = 0.001\n",
        "# eta_t_tau = 40000.0\n",
        "# eta_o_tau = 4000.0\n",
        "# N_class=10\n",
        "# margin=2\n",
        "# save_N = 100 #100 # # of saved epochs\n",
        "# save_every = np.floor(N_triplet / save_N) # Save data every <> epohsChoice 3\n",
        "# save_Nsamples = 100 # # of inputs from each class for which to save resonses\n",
        "\n",
        "### Load Classic data\n",
        "experiment = 'met_FB0_swLR_0'\n",
        "expDir = directory+'/data/'+experiment # To read in saved data\n",
        "sd_name='11113'\n",
        "inputDir = expDir#+'/'+sd_name  \n",
        "figDir = directory+'/figs/met/'+experiment # To export figures\n",
        "print(figDir)\n",
        "if not os.path.exists(figDir):\n",
        "    os.mkdir(figDir)\n",
        "\n",
        "RESP = torch.load(inputDir + '/respSave'+sd_name+'.pt')\n",
        "print(f'Num layers = {len(RESP)}')\n",
        "lossTr = torch.load(inputDir + '/lossTr'+sd_name+'.pt')\n",
        "accTr = torch.load(inputDir + '/accTr'+sd_name+'.pt')\n",
        "accVal = torch.load(inputDir + '/accVal'+sd_name+'.pt')\n",
        "weights = torch.load(inputDir + '/weightSave'+sd_name+'.pt')\n",
        "nt = RESP[0].shape[-1]\n",
        "kernel = np.ones(50)/50\n",
        "\n",
        "### Set figure properties\n",
        "import matplotlib\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams.update({'font.size': 6})\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91085166",
      "metadata": {},
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e873e09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# saveflag = True\n",
        "saveflag = False\n",
        "\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# layer = 2\n",
        "# c=0\n",
        "# times=[0, 4]\n",
        "\n",
        "# ### Plot responses\n",
        "# # t=0\n",
        "# # for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "# #             pl.plot(RESP[layer][t,i,::10,:].transpose(), linewidth=0.5)\n",
        "# for c in [0, 1, 9]:\n",
        "#     for j, t in enumerate(times):\n",
        "#         for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "#             pl.plot(RESP[layer][t,i,:,:].transpose(), linewidth=0.5)\n",
        "#         if j==0:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,0.2))\n",
        "#         elif j==1:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,10,20))\n",
        "#         if saveflag:\n",
        "#             pl.savefig(figDir+'/resp_c'+str(c)+'t'+str(j)+'.svg', format=\"svg\")\n",
        "#         ax.cla() \n",
        "\n",
        "# ### Plot accuracy\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.linspace(1,par.nEpisodes,par.nEpisodes),np.convolve(np.pad(accTr, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# pl.plot(np.linspace(1,par.nEpisodes,par.nEpisodes),np.convolve(np.pad(accVal, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# # pl.plot(accTr, linewidth=0.5); pl.plot(accVal, linewidth=0.5)\n",
        "# ax.xaxis.set_ticks((0,par.nEpisodes)); ax.yaxis.set_ticks((0,1))\n",
        "# if saveflag:\n",
        "#     pl.savefig(figDir+'/accTrVal.svg', format=\"svg\")\n",
        "\n",
        "### Plot Feedback Weight Evolution\n",
        "if par.saveFlag_FBWeights:\n",
        "    fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "    ax.spines[['top','right']].set_visible(False)\n",
        "    for j in range(0,1000,100):\n",
        "        pl.plot(np.linspace(1,par.nEpisodes,par.nSave+1),weights[::2,j,:].transpose(), linewidth=0.5)\n",
        "ax.xaxis.set_ticks((0,par.nEpisodes)); ax.yaxis.set_ticks((-.05,0,.05))\n",
        "if saveflag:\n",
        "    pl.savefig(figDir+'/fbWeights.svg', format=\"svg\")\n",
        "\n",
        "# ### Correlation between met responses\n",
        "# layer = 1\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# # imdata = ax.imshow(stats.zscore(np.reshape(RESP[1][0,:,:,:],[RESP[1].shape[1], RESP[1].shape[2]*RESP[1].shape[3]]), 1).transpose(),vmin=-1.0, vmax=1.0)\n",
        "# c0 = np.matmul(stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# c1 = np.matmul(stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "82efca1a",
      "metadata": {
        "id": "82efca1a"
      },
      "source": [
        "### Plot triplet distances and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90dcb30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "e90dcb30",
        "outputId": "2637691c-fdaa-4170-c851-b3d7de14cff7"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as pl\n",
        "# saveflag = True\n",
        "saveflag = False\n",
        "tripletflag = True\n",
        "# tripletflag = False\n",
        "\n",
        "kernel = np.ones(50)/50\n",
        "prefix = experiment#'classic_original_'\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams['axes.labelsize'] = 6\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "###Plot distances\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,0]); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,1], linestyle='dashed')\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,0], linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,1], linewidth=0.5)\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=20)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,15,30))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,20))\n",
        "  # ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'distances.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplett loss\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_distX_tr.shape)),np.convolve(l_tri,kernel,mode='same')); \n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),l_tri, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(l_tri, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  # ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=0.5*1.05*np.max(l_tri))\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=1.0)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,1))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'loss_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplet accuracy\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),a_tri*100, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(a_tri*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,100))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'acc_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot output loss\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),np.convolve(l_tri,kernel,mode='same')); \n",
        "pl.plot(np.linspace(1,nt_out,nt_out),l_out, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(l_out, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=0.8)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0.0,0.8))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "ax.set_xticklabels((0, nt_out), fontdict={'family': 'sans-serif', 'size':5})\n",
        "ax.set_yticklabels((0.5,0.8), fontdict={'family': 'sans-serif', 'size':5})\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'loss_out.svg', format=\"svg\")\n",
        "\n",
        "###Plot output accuracy\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),a_out*100, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(a_out*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0,100))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'acc_out.svg', format=\"svg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a8b3d98",
      "metadata": {
        "id": "3a8b3d98"
      },
      "source": [
        "### Plot met responses over training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c442906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "1c442906",
        "outputId": "97f4589d-d70e-4cb2-e668-7042ecb759a8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as pl\n",
        "from scipy import stats\n",
        "pl.rcParams['savefig.dpi'] = 400\n",
        "\n",
        "saveflag = True\n",
        "# saveflag = False\n",
        "\n",
        "### Raw met reponses\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,5.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "plmet = np.squeeze(met[0,:,199])\n",
        "pl.plot(np.linspace(1,nt_met,nt_met)*save_every,plmet)\n",
        "ax.set_xlim(xmin=0, xmax=nt_met*save_every); ax.set_ylim(ymin=0.0, ymax=1.05*np.max(plmet))\n",
        "ax.xaxis.set_ticks((0,nt_met*save_every)); ax.yaxis.set_ticks((0,))\n",
        "pl.show()\n",
        "\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[0,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[-1,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1.svg', format=\"svg\")\n",
        "\n",
        "### Correlation between met responses\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = np.matmul(stats.zscore(np.squeeze(met[0,:,:]),0).transpose(), stats.zscore(np.squeeze(met[0,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c1 = np.matmul(stats.zscore(np.squeeze(met[-1,:,:]),0).transpose(), stats.zscore(np.squeeze(met[-1,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9f4c2a58",
      "metadata": {
        "id": "9f4c2a58"
      },
      "source": [
        "# Plot weights"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
