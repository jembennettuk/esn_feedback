{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aW_6ADldkbuB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW_6ADldkbuB",
        "outputId": "fe8306c7-3a8e-4165-e62e-41a424a840b9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "_QJywZpKBcrP",
      "metadata": {
        "id": "_QJywZpKBcrP"
      },
      "outputs": [],
      "source": [
        "directory='.'\n",
        "# directory='/content/drive/MyDrive/esn2sparse'; \n",
        "# !cp \"/content/drive/MyDrive/esn2sparse/params.py\" \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0caa84e4",
      "metadata": {
        "id": "0caa84e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jb739/apps/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jb739/apps/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from scipy import stats\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as pl\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import time\n",
        "import torch.jit as jit\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from os.path import exists\n",
        "import gc\n",
        "import importlib\n",
        "import params_feedback as par\n",
        "import time\n",
        "import os\n",
        "# device = 'cpu'\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e75f2936",
      "metadata": {
        "id": "e75f2936"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root=directory+'/data', train=True, download=True, transform=None)\n",
        "mnist_testset = datasets.MNIST(root=directory+'/data', train=False, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d9c42c75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9c42c75",
        "outputId": "0928ebb6-40ac-4b31-b171-06853d2d74fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jb739/apps/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "/home/jb739/apps/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_tr shape = torch.Size([50000, 28, 28]),    Y_tr shape = torch.Size([50000, 10])\n"
          ]
        }
      ],
      "source": [
        "X_te=mnist_testset.data               ## Test set images\n",
        "y_te=mnist_testset.test_labels        ## Test set labels\n",
        "\n",
        "N_o=10                                ## Number of output nodes/classes\n",
        "N_te=y_te.size()[0]                   ## Number of test samples\n",
        "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
        "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
        "\n",
        "X_tr=mnist_trainset.data              ## Train set images\n",
        "y_tr=mnist_trainset.train_labels      ## Train labels \n",
        "N_tr=y_tr.size()[0]                   ## Number of training samples\n",
        "N_i = X_tr.size()[1]                  ## Number of inputs to ESN\n",
        "\n",
        "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
        "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
        "\n",
        "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
        "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
        "\n",
        "X_val=X_tr[i_val,:,:]\n",
        "Y_val=Y_tr[i_val,:]\n",
        "\n",
        "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
        "N_tr=N_tr-N_val\n",
        "\n",
        "X_tr=X_tr[i_tr,:,:]\n",
        "Y_tr=Y_tr[i_tr,:]\n",
        "\n",
        "T=X_tr.size()[2]\n",
        "N_in=X_tr.size()[1]\n",
        "\n",
        "## Normalisation and conversion to float\n",
        "X_M=255\n",
        "# X_tr=torch.reshape( (X_tr.float()/X_M),[-1,784]) \n",
        "# X_val=torch.reshape((X_val.float()/X_M),[-1,784])\n",
        "# X_te=torch.reshape((X_te.float()/X_M),[-1,784])\n",
        "X_tr=torch.reshape( (X_tr.float()),[-1,784]).to(device)\n",
        "X_val=torch.reshape((X_val.float()),[-1,784]).to(device)\n",
        "X_te=torch.reshape((X_te.float()),[-1,784]).to(device)\n",
        "\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])\n",
        "\n",
        "X_tr=torch.reshape( (X_tr),[-1,28,28]) \n",
        "X_val=torch.reshape((X_val),[-1,28,28])\n",
        "X_te=torch.reshape((X_te),[-1,28,28])\n",
        "\n",
        "Y_tr=Y_tr.float().to(device)\n",
        "Y_val=Y_val.float().to(device)\n",
        "Y_te=Y_te.float().to(device)\n",
        "\n",
        "print(f'X_tr shape = {X_tr.shape},    Y_tr shape = {Y_tr.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2148197e",
      "metadata": {
        "id": "2148197e"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data (OLD VERSION - results in outlines)\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,X_tr[j,:]==X_tr[j,:].min()] = X_tr[j,X_tr[j,:]==X_tr[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_tr[j,:].max() - X_tr[j,:].min())\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,X_val[j,:]==X_val[j,:].min()] = X_val[j,X_val[j,:]==X_val[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_val[j,:].max() - X_val[j,:].min())\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,X_te[j,:]==X_te[j,:].min()] = X_te[j,X_te[j,:]==X_te[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_te[j,:].max() - X_te[j,:].min())\n",
        "\n",
        "# Z-score inputs\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a64e52",
      "metadata": {
        "id": "d8a64e52"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data\n",
        "contrast_mean = torch.tensor(0.5).to(device)\n",
        "contrast_range = torch.tensor(0.99).to(device)\n",
        "# Training data\n",
        "for j in range(X_tr.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_tr[j,:] -= torch.min(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.max(X_tr[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_tr[j,:] = torch.tensor(1.0).to(device) - X_tr[j,:]\n",
        "    # Z-score\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= (torch.std(X_tr[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Validation data\n",
        "for j in range(X_val.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_val[j,:] -= torch.min(X_val[j,:])\n",
        "    X_val[j,:] /= torch.max(X_val[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_val[j,:] = torch.tensor(1.0).to(device) - X_val[j,:]\n",
        "    # Z-score\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= (torch.std(X_val[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Test data\n",
        "for j in range(X_te.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_te[j,:] -= torch.min(X_te[j,:])\n",
        "    X_te[j,:] /= torch.max(X_te[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_te[j,:] = torch.tensor(1.0).to(device) - X_te[j,:]\n",
        "    # Z-score\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= (torch.std(X_te[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a05756",
      "metadata": {
        "id": "04a05756"
      },
      "outputs": [],
      "source": [
        "# IF NOT USING ESN\n",
        "# Make inputs 2800-dim and z-score\n",
        "from scipy import stats\n",
        "with torch.no_grad():\n",
        "    w = torch.randn([784, par.N_esn*28]).to(device)/torch.sqrt(torch.tensor(par.N_esn*28+784)).to(device)\n",
        "    batchsize = torch.tensor(100).to(device)\n",
        "    # Operate on batches of data to save GPU-memory\n",
        "    # Training data\n",
        "    nbatch = torch.ceil(X_tr.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_tr.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_tr[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]),:,:].reshape(batchsize, X_tr.shape[1]*X_tr.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_tr = torch.clone(perm).cpu()\n",
        "    # Validation data\n",
        "    nbatch = torch.ceil(X_val.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_val.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_val[j*batchsize:min((j+1)*batchsize,X_val.shape[0]),:,:].reshape(batchsize, X_val.shape[1]*X_val.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_val.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_val = torch.clone(perm).cpu()\n",
        "    # Test data\n",
        "    nbatch = torch.ceil(X_te.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_te.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_te[j*batchsize:min((j+1)*batchsize,X_te.shape[0]),:,:].reshape(batchsize, X_te.shape[1]*X_te.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_te.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_te = torch.clone(perm).cpu()\n",
        "    \n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "\n",
        "    # X_tr = torch.tensor(stats.zscore(torch.matmul(X_tr.reshape(X_tr.shape[0], X_tr.shape[1]*X_tr.shape[2]), w).numpy(), axis=1))\n",
        "    # X_val = torch.tensor(stats.zscore(torch.matmul(X_val.reshape(X_val.shape[0], X_val.shape[1]*X_val.shape[2]), w).numpy(), axis=1))\n",
        "    # X_te = torch.tensor(stats.zscore(torch.matmul(X_te.reshape(X_te.shape[0], X_te.shape[1]*X_te.shape[2]), w).numpy(), axis=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3f55bc9b",
      "metadata": {
        "id": "3f55bc9b"
      },
      "outputs": [],
      "source": [
        "def Data2Classes(X,Y):\n",
        "    \n",
        "    ind=torch.where(Y==1)[1]\n",
        "\n",
        "    N_class=torch.max(ind)+1\n",
        "    \n",
        "    X1=[]\n",
        "    Y1=[]\n",
        "    \n",
        "    for n in range(N_class):\n",
        "    \n",
        "        ind1=torch.where(ind==n)[0].type(torch.long)\n",
        "\n",
        "        X1.append(X[ind1,:].to(device))\n",
        "        Y1.append(Y[ind1,:].to(device))\n",
        "        \n",
        "    return X1, Y1\n",
        "        \n",
        "# X_tr/X_val/X_te are lists of length 10 (1 entry per class)\n",
        "X_tr, Y_tr=Data2Classes(X_tr,Y_tr)\n",
        "\n",
        "X_val, Y_val=Data2Classes(X_val,Y_val)\n",
        "\n",
        "X_te, Y_te=Data2Classes(X_te,Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhLHzeGYPsE1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "zhLHzeGYPsE1",
        "outputId": "c15a1dbb-6850-4f41-febc-a380d4d35bae"
      },
      "outputs": [],
      "source": [
        "# Plot activity and correlation between classes\n",
        "N_esn = par.N_esn\n",
        "a = np.zeros((1000,N_esn*28))\n",
        "for j in range(len(X_tr)):\n",
        "    a[j*100:(j+1)*100,:] = X_tr[j][0:100,:].numpy()\n",
        "print(np.max(a), np.min(a))\n",
        "c = np.matmul(stats.zscore(a,axis=1), np.transpose(stats.zscore(a,axis=1))) / a.shape[1]\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[0][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[2][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(c,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[0, 1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7cbfc3",
      "metadata": {
        "id": "8b7cbfc3"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    \n",
        "    def __init__(self,Ns,N_class):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.N_class=N_class\n",
        "        N_layers=np.shape(Ns)[0]\n",
        "        \n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ths=[]\n",
        "        self.Th_bs=[]\n",
        "        \n",
        "        self.Ns=Ns\n",
        "        for n in range(1,np.shape(Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter(torch.randn([Ns[n-1],Ns[n]]).to(device)/torch.sqrt(torch.tensor(Ns[n-1]+Ns[n]))))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([Ns[n]]).to(device)))\n",
        "\n",
        "        \n",
        "        self.Ths.append( nn.Parameter( torch.randn([Ns[-1],N_class])/torch.sqrt(torch.tensor(Ns[n-1]+Ns[n])) ) ) \n",
        "        self.Th_bs.append( nn.Parameter(torch.zeros([N_class])) )\n",
        "        \n",
        "    def Initialise_Hyperparameters(self,eta_t, eta_o,batch_size,margin):\n",
        "\n",
        "\n",
        "        self.eta_t=eta_t\n",
        "        self.eta_o=eta_o\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "        self.opt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':eta_t }])\n",
        "        \n",
        "        self.opt_out=optim.Adam([{ 'params': self.Ths+self.Th_bs, 'lr':eta_o }])\n",
        "                              \n",
        "        self.margin=margin\n",
        "        \n",
        "        \n",
        "    def Forward_Triplet(self, S):\n",
        "        \n",
        "        ys=[]\n",
        "        ns = S.shape[1] # Number of samples\n",
        "        ACC = 0.0\n",
        "        o = torch.ones(9,1)\n",
        "        for k in range(3):\n",
        "        \n",
        "            xs=[]\n",
        "            xs.append(S[k,:,:])\n",
        "\n",
        "            for n in range(0,np.shape(self.Ns)[0]-2):\n",
        "                # Compute activities in each layer\n",
        "                xs.append(torch.relu( torch.add(torch.matmul(xs[n],self.Ws[n]),self.bs[n]) ) )\n",
        "\n",
        "            ys.append( torch.add(torch.matmul(xs[-1],self.Ws[-1]),self.bs[-1]) )\n",
        "        \n",
        "        # # Use Euclidean distance\n",
        "        # dneg = torch.sum( torch.pow( ys[0].detach()-ys[2].detach(), 2 ), 1)\n",
        "        # dposneg = torch.sum( torch.pow( ys[1].detach()-ys[2].detach(), 2 ), 1)\n",
        "        # # Find the hard negative pairs and swap anchor and positive if dposneg>dneg\n",
        "        # ind = dneg > dposneg\n",
        "        # acopy = torch.clone(ys[0])\n",
        "        # ys[0][ind,:] = torch.clone(ys[1][ind,:])\n",
        "        # ys[1][ind,:] = torch.clone(acopy[ind,:])\n",
        "        # Recompute distances\n",
        "        dpos = torch.sum( torch.pow( ys[0]-ys[1], 2 ), 1)\n",
        "        dneg = torch.sum( torch.pow( ys[0]-ys[2], 2 ), 1)\n",
        "        \n",
        "        E = dpos - dneg + self.margin\n",
        "        E = torch.mean( E*(E>0) )\n",
        "        if E>0:\n",
        "            \n",
        "            E.backward()\n",
        "        \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        \n",
        "        # # Use correlation\n",
        "        # a = (ys[0] - torch.matmul(torch.mean(ys[0],1,keepdim=True), torch.ones(1,ys[0].shape[1]))) / torch.matmul(torch.std(ys[0],1,keepdim=True), torch.ones(1,ys[0].shape[1]))\n",
        "        # p = (ys[1] - torch.matmul(torch.mean(ys[1],1,keepdim=True), torch.ones(1,ys[0].shape[1]))) / torch.matmul(torch.std(ys[1],1,keepdim=True), torch.ones(1,ys[0].shape[1]))\n",
        "        # n = (ys[2] - torch.matmul(torch.mean(ys[2],1,keepdim=True), torch.ones(1,ys[0].shape[1]))) / torch.matmul(torch.std(ys[2],1,keepdim=True), torch.ones(1,ys[0].shape[1]))\n",
        "        # dpos = torch.sum( torch.mul(a,p) ,1) / a.shape[1]\n",
        "        # dneg = torch.sum( torch.mul(a,n) ,1) / a.shape[1]\n",
        "        # E = torch.mean(dneg - dpos)\n",
        "        \n",
        "        # E.backward()\n",
        "    \n",
        "        # self.opt.step()\n",
        "        # self.opt.zero_grad()\n",
        "\n",
        "        # print(f'-----  Triplet loss = {torch.mean(E)},-----  +ve = {torch.mean(dpos)},------  -ve = {torch.mean(dneg)}')\n",
        "        \n",
        "        dn = torch.zeros(ns)\n",
        "        for j in range(ns):            \n",
        "            ind1 = torch.randperm(ns)\n",
        "            dn = torch.sum(torch.pow(torch.matmul(o, ys[0][j,None,:]) - ys[2][ind1[0:9],:], 2.), 1)\n",
        "            ACC += torch.any(dpos[j]*o > dn).logical_not().long()\n",
        "        ACC /= ns\n",
        "\n",
        "        return E, ACC, torch.mean(dpos), torch.mean(dneg)\n",
        "        \n",
        "        \n",
        "    def Forward_out(self, S, Y):\n",
        "       \n",
        "        \n",
        "        xs=[]\n",
        "        xs.append(S)\n",
        "\n",
        "        for n in range(0,np.shape(self.Ns)[0]-2): # For each layer except the final layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            xs.append(torch.relu( torch.add(torch.matmul(xs[n],self.Ws[n]),self.bs[n]) ) )\n",
        "\n",
        "        y= torch.add(torch.matmul(xs[-1],self.Ws[-1]),self.bs[-1])\n",
        "        out=torch.add(torch.matmul(y.detach(),self.Ths[0]),self.Th_bs[0]) \n",
        "        \n",
        "        E_out=torch.mean(torch.sum( torch.pow( out-Y, 2 ), 1))\n",
        "        \n",
        "        Acc=torch.mean(torch.eq( torch.argmax(out,1), torch.argmax(Y,1) ).type(torch.float))\n",
        "        \n",
        "        E_out.backward()\n",
        "        \n",
        "        self.opt_out.step()\n",
        "        self.opt_out.zero_grad()\n",
        "        \n",
        "        \n",
        "        return E_out, Acc\n",
        "        \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-p_rD-i5OWR",
      "metadata": {
        "id": "H-p_rD-i5OWR"
      },
      "outputs": [],
      "source": [
        "class MLPclassic(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)\n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.lossLayer = par.lossLayer\n",
        "        self.maxLayer = par.maxLayer\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ns=par.Ns\n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.fbLayer = par.fbLayer\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/10**4).to(device))\n",
        "\n",
        "    def initialOptimiser(self):\n",
        "        if hasattr(self,'fbLayer'):\n",
        "            self.iniOpt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':self.etaInitial }])\n",
        "        else:\n",
        "            self.iniOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaInitial }])\n",
        "\n",
        "    def transferOptimiser(self):\n",
        "        \n",
        "        self.tranOpt=optim.Adam([{ 'params': self.Ws[(self.maxLayer-1):]+self.bs[(self.maxLayer-1):], 'lr':self.etaTransfer }])\n",
        "        \n",
        "    def Forward(self):\n",
        "\n",
        "        for n in range(0,len(self.Ns)-1): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n+1] = torch.clone(torch.relu( torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n]) ))\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True))\n",
        "\n",
        "    def ESN_1step(self, s, t):\n",
        "        if t>0:\n",
        "            if hasattr(self,'fbLayer'):\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.x[self.fbLayer], self.W_fb)))\n",
        "            else:\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)))\n",
        "        else:\n",
        "            self.x[0] = torch.clone(self.alpha * torch.tanh(torch.matmul(s, self.W_in)))\n",
        "    \n",
        "    def lossCrossEntropy(self, nSamples):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(self.x[-1]), torch.sum(torch.exp(self.x[-1]), 1, keepdim=True).tile((1,self.x[-1].shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "\n",
        "    def accuracyTarget(self):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(self.x[-1],1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self):\n",
        "        nSamples = self.x[-1].shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device)\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = self.x[-1][j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(self.x[-1][j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "    \n",
        "    def getLossAccuracy(self, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        L = self.lossCrossEntropy(self.x[-1].shape[0])\n",
        "        accTa = self.accuracyTarget()\n",
        "        accClCe = self.accuracyClassCentroid()\n",
        "\n",
        "        if backwardFlag:\n",
        "            opt.zero_grad()\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            \n",
        "        return L,accTa,accClCe\n",
        "    \n",
        "    def response(self, Input):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        \n",
        "        for t in range(T):\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GWXIPFSI_Ynd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWXIPFSI_Ynd",
        "outputId": "00ee5616-9405-48e4-b18f-5efaa192ed57"
      },
      "outputs": [],
      "source": [
        "###\n",
        "### WITHOUT metric learning\n",
        "###\n",
        "def xent_esn_fb(expName,rngSeed):\n",
        "\n",
        "    ### Initiliase RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save weights every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2]\n",
        "\n",
        "    ###############################\n",
        "    #### First phase of training\n",
        "    ###############################\n",
        "\n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nWeightSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    savDW = [] # List of arrays to store DW histograms over time\n",
        "    for k in range(par.maxLayer):\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPclassic(par)\n",
        "\n",
        "    MOD.initialOptimiser()\n",
        "    L_tr=np.zeros([par.nEpochs])\n",
        "    A_trTa=np.zeros([par.nEpochs])\n",
        "    A_trClCe=np.zeros([par.nEpochs])\n",
        "    L_val=np.zeros([par.nEpochs])\n",
        "    A_valTa=np.zeros([par.nEpochs])\n",
        "    A_valClCe=np.zeros([par.nEpochs])\n",
        "\n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "    for n in range(par.nEpochs):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpochs-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpochs}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li, l in enumerate(par.saveLayers):\n",
        "                    RESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights and (n%save_every)<1:\n",
        "            savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "        \n",
        "        # Get response to input\n",
        "        MOD.response(Im)\n",
        "        # Compute loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy(backwardFlag=True, opt=MOD.iniOpt)\n",
        "\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpochs}')\n",
        "            return -1\n",
        "        #####\n",
        "        #####\n",
        "        \n",
        "        # Store loss and accuracy\n",
        "        L_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_trTa[n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        A_trClCe[n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "\n",
        "        # Store weight changes for effective learning rate\n",
        "        if par.saveFlag_DW and (n%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(par.maxLayer):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "        if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(par.maxLayer):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    savDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "\n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Get response to input\n",
        "            MOD.response(Im)\n",
        "            # Compute loss and accuracy\n",
        "            loss, accValTa, accValClCe = MOD.getLossAccuracy()\n",
        "            \n",
        "            # Store loss and accuracy\n",
        "            L_val[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_valTa[n]=np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            A_valClCe[n]=np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.iniOpt.param_groups[0]['lr'] = par.etaInitial * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if (n%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[n-par.reportTime:n])\n",
        "            accTrTa_mean=np.mean(A_trTa[n-par.reportTime:n])\n",
        "            accTrClCe_mean=np.mean(A_trClCe[n-par.reportTime:n])\n",
        "            mseVal_mean=np.mean(L_val[n-par.reportTime:n])\n",
        "            accValTa_mean=np.mean(A_valTa[n-par.reportTime:n])\n",
        "            accValClCe_mean=np.mean(A_valClCe[n-par.reportTime:n])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n)/np.float32(par.nEpochs)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                                           Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ###############################\n",
        "    #### Second phase of training\n",
        "    ###############################\n",
        "    print('**********************START TRANSFER TRAINING')\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        outRESP = []\n",
        "        for layer in range(par.maxLayer+1,len(MOD.Ns)):\n",
        "            outRESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "\n",
        "    ### Reset output layer weights\n",
        "    MOD.Ws[-1] = nn.Parameter((torch.randn([MOD.Ns[-2],MOD.Ns[-1]])/torch.sqrt(torch.tensor(MOD.Ns[-2]+MOD.Ns[-1]))).to(device))\n",
        "    MOD.bs[-1] = nn.Parameter(torch.zeros([MOD.Ns[-1]]).to(device))\n",
        "\n",
        "    ### Init memory for saving data\n",
        "    outL_tr=np.zeros([par.nEpisodesOut])\n",
        "    outA_trTa=np.zeros([par.nEpisodesOut])\n",
        "    outA_trClCe=np.zeros([par.nEpisodesOut])\n",
        "    outL_val=np.zeros([par.nEpisodesOut])\n",
        "    outA_valTa=np.zeros([par.nEpisodesOut])\n",
        "    outA_valClCe=np.zeros([par.nEpisodesOut])\n",
        "\n",
        "    MOD.maxLayer = len(MOD.Ns)-1\n",
        "    MOD.lossLayer = MOD.maxLayer\n",
        "\n",
        "    ### Initialise optimiser\n",
        "    MOD.transferOptimiser()\n",
        "    \n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpisodesOut):\n",
        "        ### Save output responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodesOut-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpisodesOut}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, range(par.maxLayer+1,len(par.Ns)))\n",
        "                for li, l in enumerate(range(par.maxLayer+1,len(par.Ns))):\n",
        "                    outRESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "\n",
        "        # Get response to input\n",
        "        MOD.response(Im)\n",
        "        # Compute training loss and accuracy\n",
        "        loss, accTrTa, accClCe = MOD.getLossAccuracy(backwardFlag=True, opt=MOD.tranOpt)\n",
        "        \n",
        "        # print(f'mean gradient is {torch.mean(MOD.Ws[-1].grad)}')\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        # if torch.any(torch.isnan(MOD.x[-1])):\n",
        "        #     print(f'Quitting at batch {n} of {par.nEpisodesOut}')\n",
        "        #     return -1\n",
        "        #####\n",
        "        #####\n",
        "\n",
        "        # Store loss and accuracy\n",
        "        outL_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        outA_trTa[n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        outA_trClCe[n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "        \n",
        "        # # Print delta_W for multiple layers\n",
        "        # if (n%save_every)<1:\n",
        "        #     with torch.no_grad():\n",
        "        #         # Save before weights\n",
        "        #         w1 = []\n",
        "        #         for k in range(par.maxLayer):\n",
        "        #             w1.append(torch.clone(MOD.Ws[k].data))\n",
        "        #         if par.fbLayer:\n",
        "        #             w1.append(torch.clone(MOD.W_fb.data))\n",
        "        # if ((n-1)%save_every)<1:\n",
        "        #     with torch.no_grad():\n",
        "        #         # Save after weights\n",
        "        #         w2 = []\n",
        "        #         for k in range(par.maxLayer):\n",
        "        #             w2.append(torch.clone(MOD.Ws[k].data))\n",
        "        #         if par.fbLayer:\n",
        "        #             w2.append(torch.clone(MOD.W_fb.data))\n",
        "        #         # Compute weight change\n",
        "        #         for k in range(len(w1)):\n",
        "        #             dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "        #             print(f'Max dW[{k}] = {dw[k].max()}')\n",
        "            \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Get response to input\n",
        "            MOD.response(Im)    \n",
        "            # Compute validation loss and accuracy\n",
        "            loss,accValTa,accValClCe = MOD.getLossAccuracy()\n",
        "\n",
        "            # Store loss and accuracy\n",
        "            outL_val[n] = np.copy(np.array(loss.to('cpu').detach()))\n",
        "            outA_valTa[n] = np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            outA_valClCe[n] = np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.tranOpt.param_groups[0]['lr'] = par.etaTransfer * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if (n%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(outL_tr[n-par.reportTime:n])\n",
        "            accTrTa_mean=np.mean(outA_trTa[n-par.reportTime:n])\n",
        "            accTrClCe_mean=np.mean(outA_trClCe[n-par.reportTime:n])\n",
        "            mseVal_mean=np.mean(outL_val[n-par.reportTime:n])\n",
        "            accValTa_mean=np.mean(outA_valTa[n-par.reportTime:n])\n",
        "            accValClCe_mean=np.mean(outA_valClCe[n-par.reportTime:n])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n)/np.float32(par.nEpisodesOut)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                                           Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outL_tr, outputDir + '/' + 'out_lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trTa, outputDir + '/' + 'out_accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trClCe, outputDir + '/' + 'out_accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outL_val, outputDir + '/' + 'out_lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valTa, outputDir + '/' + 'out_accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valClCe, outputDir + '/' + 'out_accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(outRESP, outputDir + '/' + 'out_respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD, outputDir + '/' + 'model'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "dfa4cf3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPmetric(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)    \n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.nInputs = par.nInputs\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.tMax = par.tMax\n",
        "        self.maxLayer = par.maxLayer\n",
        "        self.lossLayer = par.lossLayer\n",
        "        self.metricLossType = par.metricLossType\n",
        "        self.margin = par.margin\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        self.batch_size = par.batch_size\n",
        "        if par.metricLossType=='prototypicalLoss':\n",
        "            self.nP = par.nSampProto\n",
        "            self.nQ = int(par.batch_size / par.nClass)\n",
        "        self.tri = 1 if par.metricLossType=='tripletLoss' else 0\n",
        "        self.wPerf = torch.exp(-torch.arange(self.tMax).flip(0)/par.tauPerf)\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ns=par.Ns\n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.fbLayer = par.fbLayer\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/10**4).to(device))\n",
        "\n",
        "    def metricOptimiser(self):\n",
        "\n",
        "        if hasattr(self,'fbLayer'):\n",
        "            self.metOpt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':self.etaInitial }])\n",
        "        else:\n",
        "            self.metOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaInitial }])\n",
        "\n",
        "    def transferOptimiser(self):\n",
        "        \n",
        "        print(f'Length of weights: {len(self.Ws)}')\n",
        "        print(f'Length of ouput layer weights: {len(self.Ws[(self.maxLayer-1):])}')\n",
        "        print(f'maxLayer: {self.maxLayer}')\n",
        "        self.tranOpt=optim.Adam([{ 'params': self.Ws[(self.maxLayer-1):]+self.bs[(self.maxLayer-1):], 'lr':self.etaTransfer }])\n",
        "\n",
        "    def Forward(self):\n",
        "\n",
        "        for n in range(0,np.minimum(self.maxLayer+1,len(self.Ns))-1): # For each layer, up to maxLayer\n",
        "            self.x[n+1] = torch.clone(torch.relu( torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n]) ))\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True))\n",
        "\n",
        "    def ESN_1step(self, s, t):\n",
        "        if t>0:\n",
        "            if hasattr(self,'fbLayer'):\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.x[self.fbLayer], self.W_fb)))\n",
        "            else:\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)))\n",
        "        else:\n",
        "            self.x[0] = torch.clone(self.alpha * torch.tanh(torch.matmul(s, self.W_in)))\n",
        "\n",
        "    def lossCrossEntropy(self, nSamples, r):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(r), torch.sum(torch.exp(r), 1, keepdim=True).tile((1,r.shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "    \n",
        "    def tripletLoss(self, resp):\n",
        "        # Implement hard negative mining\n",
        "\n",
        "        dAP = (resp[0] - resp[1]).pow(2).sum(1).sqrt() # anchor-positive\n",
        "        dAN = (resp[0] - resp[2]).pow(2).sum(1).sqrt() # anchor-negative\n",
        "        \n",
        "        dPN = (resp[1] - resp[2]).pow(2).sum(1).sqrt() # positive-negative\n",
        "        ind = torch.le(dPN, dAN)\n",
        "        ind1 = torch.nonzero(ind)\n",
        "        ind2 = torch.nonzero(torch.logical_not(ind))\n",
        "        \n",
        "        L = torch.concat((torch.maximum(torch.zeros(ind2.shape).to(device), dAP[ind2] - dAN[ind2] + self.margin), \n",
        "                         torch.maximum(torch.zeros(ind1.shape).to(device), dAP[ind1] - dPN[ind1] + self.margin)), dim=0).mean()\n",
        "        # if t%50==0:\n",
        "        #     print(f'DP = {dAP.mean()}                            DN = {dAN.mean()}                           maxRESP = {resp[0].max()}')\n",
        "\n",
        "        return L.mean()\n",
        "    \n",
        "    def prototypicalLoss(self, r):\n",
        "        proto = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device) # init mem for prototypes\n",
        "        dist = torch.zeros(self.N_class*self.nQ, self.N_class).to(device) # init mem for distances\n",
        "        prob = torch.zeros(self.N_class*self.nQ).to(device) # Init mem for probabilities\n",
        "        # Compute prototypes\n",
        "        for j in range(self.N_class):\n",
        "            proto[j,:] = torch.clone(r[j*self.nP:(j+1)*self.nP,:]).mean(0)\n",
        "        for j in range(self.N_class): # for each class\n",
        "            for k in range(self.nQ): # for each query in class j\n",
        "                # Compute distances between queries and prototypes\n",
        "                fx = torch.clone(r[self.nP*self.N_class + j*self.nQ + k,:]).unsqueeze(0).tile(self.N_class,1)\n",
        "                dist[j*self.nQ+k,:] = (fx - proto).pow(2).sum(1).sqrt()\n",
        "            # Compute probabilities using softmax\n",
        "            prob[j*self.nQ:(j+1)*self.nQ] = torch.divide(torch.exp(-dist[j*self.nQ:(j+1)*self.nQ,j]), \n",
        "                                               torch.sum(torch.exp(-dist[j*self.nQ:(j+1)*self.nQ,:]), 1))\n",
        "        \n",
        "        L = - torch.log(prob)\n",
        "\n",
        "        return L.mean(), proto\n",
        "    \n",
        "    def accuracyTarget(self, r):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(r,1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self, r, prototypes=[]):\n",
        "        nSamples = r.shape[0]\n",
        "        # Compute class centroids\n",
        "        if len(prototypes)>0:\n",
        "            centroids = prototypes\n",
        "        else:\n",
        "            centroids = torch.zeros(self.N_class, r.shape[1]).to(device)\n",
        "            for j in range(self.N_class):\n",
        "                centroids[j,:] = r[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(r[j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "\n",
        "    def getLossAccuracy(self, learning, r, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        if learning=='metric':\n",
        "            if self.metricLossType=='tripletLoss':\n",
        "                L = self.tripletLoss(r)\n",
        "                accTa = self.accuracyTarget(r[0])\n",
        "                accClCe = self.accuracyClassCentroid(r[0])\n",
        "            elif self.metricLossType=='prototypicalLoss':\n",
        "                L, proto = self.prototypicalLoss(r[0])\n",
        "                accTa = self.accuracyTarget(r[0][self.N_class*self.nP:,:])\n",
        "                accClCe = self.accuracyClassCentroid(r[0][self.N_class*self.nP:,:], proto)\n",
        "                \n",
        "        elif learning=='transfer':\n",
        "            L = self.lossCrossEntropy(r.shape[0], r)\n",
        "            accTa = self.accuracyTarget(r)\n",
        "            accClCe = self.accuracyClassCentroid(r)\n",
        "\n",
        "        if backwardFlag:\n",
        "            opt.zero_grad()\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            \n",
        "        return L,accTa,accClCe\n",
        "      \n",
        "    def response(self, Input, lossLayer, tripletFlag=False):\n",
        "\n",
        "        if tripletFlag:\n",
        "            N_samples = int(Input.shape[0] / 3)\n",
        "        else:\n",
        "            N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "\n",
        "        # Forward pass for anchor, positive, and negative\n",
        "        if tripletFlag:\n",
        "            self.Reset(N_samples * 3)\n",
        "        else:\n",
        "            self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "\n",
        "        # Compile list of responses from lossLayer. When using Triplet Loss, \n",
        "        # list is 3x1 for [anchor, positive, negative]\n",
        "        r = [] \n",
        "        for j in range(1+2*int(tripletFlag)):\n",
        "            r.append(torch.clone(self.x[lossLayer][j*N_samples:(j+1)*N_samples,:]))\n",
        "        \n",
        "        return r\n",
        "    \n",
        "    def generateBatch(self, method, X):\n",
        "       \n",
        "        ### For normal batches, e.g. for transfer learning\n",
        "        if method=='simple':\n",
        "            rand_ind = np.random.randint(0,X[0].shape[0],(self.nSampPerClassPerBatch,))\n",
        "            batch = X[0][rand_ind,:]\n",
        "            for k in range(1,self.N_class):\n",
        "                rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "                batch = torch.concat([batch, X[k][rand_ind,:]],0)\n",
        "        \n",
        "        ### For triplet loss\n",
        "        if method=='tripletLoss':\n",
        "            \n",
        "            # Populate batch\n",
        "            batch = torch.zeros([3*self.batch_size, self.nInputs,self.tMax]).to(device)\n",
        "            for k in range(self.N_class):\n",
        "                # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
        "                ind_ap = np.random.choice(X[k].shape[0],(self.nSampPerClassPerBatch,2), replace=False)\n",
        "                # Populate Anchor and Positive samples\n",
        "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,0],:])\n",
        "                batch[self.batch_size+k*self.nSampPerClassPerBatch:self.batch_size+(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,1],:])\n",
        "                # Populate negative samples\n",
        "                randClass = np.random.choice((np.arange(self.N_class)!=k).nonzero()[0], self.nSampPerClassPerBatch)\n",
        "                for m, cl in enumerate(randClass):\n",
        "                    batch[2*self.batch_size + k*self.nSampPerClassPerBatch+m,:] = torch.clone(X[cl][np.random.randint(X[cl].shape[0]),:])\n",
        "            \n",
        "        ### For Protoypical Loss\n",
        "        if method=='prototypicalLoss':\n",
        "            batch = torch.zeros((self.nP+self.nQ)*self.N_class, X[0].shape[1], X[0].shape[2]).to(device)\n",
        "            for j, x in enumerate(X): # For each class (X is a list)\n",
        "                ind = np.random.choice(x.shape[0], self.nP+self.nQ, replace=False)\n",
        "                batch[j*self.nP:(j+1)*self.nP,:] = torch.clone(x[ind[:self.nP],:])\n",
        "                batch[self.nP*self.N_class+j*self.nQ:self.nP*self.N_class+(j+1)*self.nQ] = torch.clone(x[ind[self.nP:],:])\n",
        "\n",
        "        return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "f140603c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# swLR = 0.00001#0.00046 # Sweep over these learning rates\n",
        "# rngSeed = 11117 # No. runs per hyperparameter\n",
        "\n",
        "# # Update hyperparameter\n",
        "# importlib.reload(par)\n",
        "# par.eta = swLR\n",
        "# par.fbLayer = 2\n",
        "# par.saveFlag_FBWeights = True\n",
        "# par.nEpochs = 50\n",
        "# par.nSaveMaxT = par.nEpochs\n",
        "# expName = 'test'\n",
        "###\n",
        "### WITH metric learning\n",
        "###\n",
        "def metric_esn_fb(expName, rngSeed):\n",
        "\n",
        "    ### Initialise RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save data every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2] # No. time steps per input sequence\n",
        "    \n",
        "    ###############################\n",
        "    #### First phase of training\n",
        "    ###############################\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        print('Saving weights')\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nWeightSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    savDW = [] # List of arrays to store DW histograms over time\n",
        "    for k in range(par.maxLayer):\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Init RNG\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.seed(rngSeed)\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPmetric(par)\n",
        "    MOD.metricOptimiser()\n",
        "\n",
        "    ### Init memory for saving data\n",
        "    L_tr=np.zeros([par.nEpochs])\n",
        "    A_trTa=np.zeros([par.nEpochs])\n",
        "    A_trClCe=np.zeros([par.nEpochs])\n",
        "    L_val=np.zeros([par.nEpochs])\n",
        "    A_valTa=np.zeros([par.nEpochs])\n",
        "    A_valClCe=np.zeros([par.nEpochs])\n",
        "\n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpochs):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpochs-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpochs}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li, l in enumerate(par.saveLayers):\n",
        "                    RESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights and (n%save_every)<1:\n",
        "            savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        Im = MOD.generateBatch(par.metricLossType, X_tr)\n",
        "\n",
        "        # Get response to input\n",
        "        if MOD.metricLossType=='tripletLoss':\n",
        "            r = MOD.response(Im, MOD.lossLayer, tripletFlag=True)\n",
        "        else:\n",
        "            r = MOD.response(Im, MOD.lossLayer)\n",
        "        # Compute loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy('metric',r,backwardFlag=True, opt=MOD.metOpt)\n",
        "\n",
        "        ####\n",
        "        #### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpochs}')\n",
        "            return -1\n",
        "        ####\n",
        "        ####\n",
        "\n",
        "        # Store training loss and accuracy\n",
        "        L_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_trTa[n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        A_trClCe[n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "\n",
        "        # Store weight changes for effective learning rate\n",
        "        if par.saveFlag_DW and (n%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(MOD.maxLayer):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "        if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(MOD.maxLayer):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    savDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "\n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            Im = MOD.generateBatch(par.metricLossType, X_val)\n",
        "\n",
        "            # Get response to input\n",
        "            if MOD.metricLossType=='tripletLoss':\n",
        "                r = MOD.response(Im, MOD.lossLayer, tripletFlag=True)\n",
        "            else:\n",
        "                r = MOD.response(Im, MOD.lossLayer)\n",
        "                \n",
        "            # Compute loss and accuracy\n",
        "            loss, accValTa, accValClCe = MOD.getLossAccuracy('metric',r)\n",
        "\n",
        "            # Store training loss and accuracy\n",
        "            L_val[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_valTa[n]=np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            A_valClCe[n]=np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.metOpt.param_groups[0]['lr'] = par.etaInitial * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if (n%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[n-par.reportTime:n])\n",
        "            accTrTa_mean=np.mean(A_trTa[n-par.reportTime:n])\n",
        "            accTrClCe_mean=np.mean(A_trClCe[n-par.reportTime:n])\n",
        "            mseVal_mean=np.mean(L_val[n-par.reportTime:n])\n",
        "            accValTa_mean=np.mean(A_valTa[n-par.reportTime:n])\n",
        "            accValClCe_mean=np.mean(A_valClCe[n-par.reportTime:n])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n)/np.float32(par.nEpochs)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                                           Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ###############################\n",
        "    #### Second phase of training\n",
        "    ###############################\n",
        "    print('**********************START TRANSFER TRAINING')\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        outRESP = []\n",
        "        for layer in range(par.maxLayer+1,len(MOD.Ns)):\n",
        "            outRESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "\n",
        "    ### Init memory for saving data\n",
        "    outL_tr=np.zeros([par.nEpisodesOut])\n",
        "    outA_trTa=np.zeros([par.nEpisodesOut])\n",
        "    outA_trClCe=np.zeros([par.nEpisodesOut])\n",
        "    outL_val=np.zeros([par.nEpisodesOut])\n",
        "    outA_valTa=np.zeros([par.nEpisodesOut])\n",
        "    outA_valClCe=np.zeros([par.nEpisodesOut])\n",
        "\n",
        "    MOD.maxLayer = len(MOD.Ns)-1\n",
        "    MOD.lossLayer = MOD.maxLayer\n",
        "\n",
        "    ### Initialise optimiser\n",
        "    MOD.transferOptimiser()\n",
        "\n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpisodesOut):\n",
        "        ### Save output responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodesOut-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpisodesOut}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, range(par.maxLayer+1,len(par.Ns)))\n",
        "                for li, l in enumerate(range(par.maxLayer+1,len(par.Ns))):\n",
        "                    outRESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        Im = MOD.generateBatch('simple', X_tr)\n",
        "\n",
        "        # Get response to input\n",
        "        r = MOD.response(Im, -1)\n",
        "\n",
        "        # Compute training loss and accuracy\n",
        "        loss, accTrTa, accClCe = MOD.getLossAccuracy('transfer', r[-1], backwardFlag=True, opt=MOD.tranOpt)\n",
        "        # print(f'mean gradient is {torch.mean(MOD.Ws[-1].grad)}')\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        # if torch.any(torch.isnan(MOD.x[-1])):\n",
        "        #     print(f'Quitting at batch {n} of {par.nEpisodesOut}')\n",
        "        #     return -1\n",
        "        #####\n",
        "        #####\n",
        "\n",
        "        # Store loss and accuracy\n",
        "        outL_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        outA_trTa[n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        outA_trClCe[n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "            \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            Im = MOD.generateBatch('simple', X_val)\n",
        "    \n",
        "            # Get response to input\n",
        "            r = MOD.response(Im, -1)    \n",
        "\n",
        "            # Compute validation loss and accuracy\n",
        "            loss,accValTa,accValClCe = MOD.getLossAccuracy('transfer',r[-1])\n",
        "\n",
        "            # Store loss and accuracy\n",
        "            outL_val[n] = np.copy(np.array(loss.to('cpu').detach()))\n",
        "            outA_valTa[n] = np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            outA_valClCe[n] = np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.tranOpt.param_groups[0]['lr'] = par.etaTransfer * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if (n%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(outL_tr[n-par.reportTime:n])\n",
        "            accTrTa_mean=np.mean(outA_trTa[n-par.reportTime:n])\n",
        "            accTrClCe_mean=np.mean(outA_trClCe[n-par.reportTime:n])\n",
        "            mseVal_mean=np.mean(outL_val[n-par.reportTime:n])\n",
        "            accValTa_mean=np.mean(outA_valTa[n-par.reportTime:n])\n",
        "            accValClCe_mean=np.mean(outA_valClCe[n-par.reportTime:n])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n)/np.float32(par.nEpisodesOut)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                                           Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outL_tr, outputDir + '/' + 'out_lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trTa, outputDir + '/' + 'out_accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trClCe, outputDir + '/' + 'out_accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outL_val, outputDir + '/' + 'out_lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valTa, outputDir + '/' + 'out_accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valClCe, outputDir + '/' + 'out_accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(outRESP, outputDir + '/' + 'out_respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD, outputDir + '/' + 'model'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa8aa35e",
      "metadata": {},
      "source": [
        "# Run parameter sweep (CrossEntropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad44485",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-5,-2,10) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        # Init RNG\n",
        "        torch.manual_seed(sd)\n",
        "        np.random.default_rng(sd)\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.eta = swLR[j]\n",
        "        par.fbLayer = 3\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'FB0_swLR_'+str(j)\n",
        "        xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "5e95e7dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 5.572280645370483\n",
            "Progress: 2.5%   Mean Tr Er: 1.268968424797058, Mean Val Er: 1.2040927839279174;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.6008000075817108, Mean Val AccClCe: 0.6016000020503998\n",
            "Time per stage: 5.349277496337891\n",
            "Progress: 5.0%   Mean Tr Er: 0.9441232562065125, Mean Val Er: 0.9404856157302857;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.6816000056266784, Mean Val AccClCe: 0.6959999990463257\n",
            "Time per stage: 5.084853410720825\n",
            "Progress: 7.5%   Mean Tr Er: 0.7247266185283661, Mean Val Er: 0.743972864151001;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.7607999992370605, Mean Val AccClCe: 0.7639999985694885\n",
            "Time per stage: 5.173245906829834\n",
            "Progress: 10.0%   Mean Tr Er: 0.6092447113990783, Mean Val Er: 0.6729060292243958;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.011199999749660492\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.8143999934196472, Mean Val AccClCe: 0.7919999980926513\n",
            "Time per stage: 5.138530015945435\n",
            "Progress: 12.5%   Mean Tr Er: 0.4662493884563446, Mean Val Er: 0.50306844830513;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.8527999973297119, Mean Val AccClCe: 0.8551999974250794\n",
            "Time per stage: 5.501049518585205\n",
            "Progress: 15.0%   Mean Tr Er: 0.4370172989368439, Mean Val Er: 0.451740460395813;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.8703999948501587, Mean Val AccClCe: 0.8767999911308288\n",
            "Time per stage: 6.11838173866272\n",
            "Progress: 17.5%   Mean Tr Er: 0.42212773382663726, Mean Val Er: 0.4420687735080719;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.8727999901771546, Mean Val AccClCe: 0.8631999945640564\n",
            "Time per stage: 6.054050922393799\n",
            "Progress: 20.0%   Mean Tr Er: 0.3687214463949203, Mean Val Er: 0.37437706768512724;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.8895999932289124, Mean Val AccClCe: 0.8959999918937683\n",
            "Time per stage: 8.956880331039429\n",
            "Progress: 22.5%   Mean Tr Er: 0.3677466303110123, Mean Val Er: 0.3603316992521286;   Mean Tr AccTa: 0.008799999803304672, Mean Val AccTa: 0.003999999910593033\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.883199999332428, Mean Val AccClCe: 0.8863999962806701\n",
            "Time per stage: 8.232136487960815\n",
            "Progress: 25.0%   Mean Tr Er: 0.31333271771669385, Mean Val Er: 0.3804712873697281;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.905599992275238, Mean Val AccClCe: 0.8831999921798706\n",
            "Time per stage: 6.970433235168457\n",
            "Progress: 27.5%   Mean Tr Er: 0.31606371521949767, Mean Val Er: 0.3119262486696243;   Mean Tr AccTa: 0.007199999839067459, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9127999925613404, Mean Val AccClCe: 0.9031999945640564\n",
            "Time per stage: 6.070446968078613\n",
            "Progress: 30.0%   Mean Tr Er: 0.2750075951218605, Mean Val Er: 0.33218340039253236;   Mean Tr AccTa: 0.004799999892711639, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9199999952316285, Mean Val AccClCe: 0.908799991607666\n",
            "Time per stage: 7.628775358200073\n",
            "Progress: 32.5%   Mean Tr Er: 0.24833398938179016, Mean Val Er: 0.2888412031531334;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9311999988555908, Mean Val AccClCe: 0.9103999924659729\n",
            "Time per stage: 7.613749980926514\n",
            "Progress: 35.0%   Mean Tr Er: 0.29300249367952347, Mean Val Er: 0.28690704226493835;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9223999977111816, Mean Val AccClCe: 0.9167999982833862\n",
            "Time per stage: 6.713672876358032\n",
            "Progress: 37.5%   Mean Tr Er: 0.2779552072286606, Mean Val Er: 0.26658227801322937;   Mean Tr AccTa: 0.003199999928474426, Mean Val AccTa: 0.003199999928474426\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9135999917984009, Mean Val AccClCe: 0.9224000000953674\n",
            "Time per stage: 5.831307888031006\n",
            "Progress: 40.0%   Mean Tr Er: 0.21124297872185707, Mean Val Er: 0.28139898911118505;   Mean Tr AccTa: 0.0007999999821186065, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.928799991607666, Mean Val AccClCe: 0.9152000045776367\n",
            "Time per stage: 5.3886730670928955\n",
            "Progress: 42.5%   Mean Tr Er: 0.2689974904060364, Mean Val Er: 0.2670952370762825;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9280000019073487, Mean Val AccClCe: 0.9183999967575073\n",
            "Time per stage: 5.703173875808716\n",
            "Progress: 45.0%   Mean Tr Er: 0.22598394751548767, Mean Val Er: 0.25792266458272933;   Mean Tr AccTa: 0.0007999999821186065, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9311999964714051, Mean Val AccClCe: 0.9255999946594238\n",
            "Time per stage: 5.935255289077759\n",
            "Progress: 47.5%   Mean Tr Er: 0.264978893995285, Mean Val Er: 0.2432047575712204;   Mean Tr AccTa: 0.003199999928474426, Mean Val AccTa: 0.004799999892711639\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9279999995231628, Mean Val AccClCe: 0.9319999957084656\n",
            "Time per stage: 5.898892164230347\n",
            "Progress: 50.0%   Mean Tr Er: 0.2409389165043831, Mean Val Er: 0.21487623810768128;   Mean Tr AccTa: 0.004799999892711639, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9336000013351441, Mean Val AccClCe: 0.9399999928474426\n",
            "Time per stage: 6.277933597564697\n",
            "Progress: 52.5%   Mean Tr Er: 0.21455607503652574, Mean Val Er: 0.2139973196387291;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9463999915122986, Mean Val AccClCe: 0.9367999911308289\n",
            "Time per stage: 5.961622953414917\n",
            "Progress: 55.0%   Mean Tr Er: 0.23344893544912337, Mean Val Er: 0.20636435359716415;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.005599999874830246\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9279999947547912, Mean Val AccClCe: 0.9407999968528747\n",
            "Time per stage: 5.779073238372803\n",
            "Progress: 57.5%   Mean Tr Er: 0.20563622817397118, Mean Val Er: 0.19759683057665825;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9392000007629394, Mean Val AccClCe: 0.9367999982833862\n",
            "Time per stage: 5.212857484817505\n",
            "Progress: 60.0%   Mean Tr Er: 0.20453814953565597, Mean Val Er: 0.2179580482840538;   Mean Tr AccTa: 0.003199999928474426, Mean Val AccTa: 0.003199999928474426\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9423999881744385, Mean Val AccClCe: 0.9368000006675721\n",
            "Time per stage: 5.180718183517456\n",
            "Progress: 62.5%   Mean Tr Er: 0.22911580935120582, Mean Val Er: 0.23875663220882415;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9319999957084656, Mean Val AccClCe: 0.9335999989509582\n",
            "Time per stage: 4.782313585281372\n",
            "Progress: 65.0%   Mean Tr Er: 0.24180590540170668, Mean Val Er: 0.2199172043800354;   Mean Tr AccTa: 0.007999999821186066, Mean Val AccTa: 0.003199999928474426\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9351999974250793, Mean Val AccClCe: 0.9335999941825867\n",
            "Time per stage: 4.98659873008728\n",
            "Progress: 67.5%   Mean Tr Er: 0.16734469391405582, Mean Val Er: 0.17808633685112;   Mean Tr AccTa: 0.0007999999821186065, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9495999979972839, Mean Val AccClCe: 0.9495999932289123\n",
            "Time per stage: 4.7329936027526855\n",
            "Progress: 70.0%   Mean Tr Er: 0.2012855304777622, Mean Val Er: 0.2223869466781616;   Mean Tr AccTa: 0.004799999892711639, Mean Val AccTa: 0.003999999910593033\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9391999959945678, Mean Val AccClCe: 0.9264000034332276\n",
            "Time per stage: 4.954587936401367\n",
            "Progress: 72.5%   Mean Tr Er: 0.17626539662480353, Mean Val Er: 0.15852761216461658;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.003199999928474426\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.948800003528595, Mean Val AccClCe: 0.9520000052452088\n",
            "Time per stage: 4.894191265106201\n",
            "Progress: 75.0%   Mean Tr Er: 0.1822816652804613, Mean Val Er: 0.21027062803506852;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9496000003814697, Mean Val AccClCe: 0.9367999982833862\n",
            "Time per stage: 5.834524154663086\n",
            "Progress: 77.5%   Mean Tr Er: 0.16185031332075595, Mean Val Er: 0.20085462003946306;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9527999997138977, Mean Val AccClCe: 0.9359999966621398\n",
            "Time per stage: 5.569788455963135\n",
            "Progress: 80.0%   Mean Tr Er: 0.19910667970776558, Mean Val Er: 0.1970374060422182;   Mean Tr AccTa: 0.0007999999821186065, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9463999986648559, Mean Val AccClCe: 0.9431999969482422\n",
            "Time per stage: 5.887223958969116\n",
            "Progress: 82.5%   Mean Tr Er: 0.145506798774004, Mean Val Er: 0.15402018554508687;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9583999991416932, Mean Val AccClCe: 0.9623999977111817\n",
            "Time per stage: 7.201232433319092\n",
            "Progress: 85.0%   Mean Tr Er: 0.15538307949900626, Mean Val Er: 0.1413029032945633;   Mean Tr AccTa: 0.001599999964237213, Mean Val AccTa: 0.001599999964237213\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9552000045776368, Mean Val AccClCe: 0.9567999935150147\n",
            "Time per stage: 5.734302759170532\n",
            "Progress: 87.5%   Mean Tr Er: 0.18788693681359292, Mean Val Er: 0.16592207327485084;   Mean Tr AccTa: 0.0, Mean Val AccTa: 0.0007999999821186065\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9447999978065491, Mean Val AccClCe: 0.9471999955177307\n",
            "Time per stage: 5.603269100189209\n",
            "Progress: 90.0%   Mean Tr Er: 0.12080884203314782, Mean Val Er: 0.17118214339017868;   Mean Tr AccTa: 0.0007999999821186065, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9624000000953674, Mean Val AccClCe: 0.9503999972343444\n",
            "Time per stage: 5.412177801132202\n",
            "Progress: 92.5%   Mean Tr Er: 0.16909514404833317, Mean Val Er: 0.1749475407600403;   Mean Tr AccTa: 0.0007999999821186065, Mean Val AccTa: 0.0\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9527999997138977, Mean Val AccClCe: 0.9560000014305114\n",
            "Time per stage: 5.53899884223938\n",
            "Progress: 95.0%   Mean Tr Er: 0.154060560464859, Mean Val Er: 0.1867316870391369;   Mean Tr AccTa: 0.0023999999463558195, Mean Val AccTa: 0.0023999999463558195\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9551999974250793, Mean Val AccClCe: 0.9511999988555908\n",
            "Time per stage: 5.417359352111816\n",
            "Progress: 97.5%   Mean Tr Er: 0.16198583126068114, Mean Val Er: 0.2207511606812477;   Mean Tr AccTa: 0.007999999821186066, Mean Val AccTa: 0.003999999910593033\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9551999950408936, Mean Val AccClCe: 0.9335999965667725\n",
            "Saving response on iteration 999 of 1000\n",
            "**********************START TRANSFER TRAINING\n",
            "Length of weights: 3\n",
            "Length of ouput layer weights: 1\n",
            "maxLayer: 3\n",
            "Time per stage: 2.928947687149048\n",
            "Progress: 2.5%   Mean Tr Er: 2.0336875534057617, Mean Val Er: 2.0030592250823975;   Mean Tr AccTa: 0.3047999988496304, Mean Val AccTa: 0.34000000327825547\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.7999999976158142\n",
            "Time per stage: 2.171321153640747\n",
            "Progress: 5.0%   Mean Tr Er: 1.6362832117080688, Mean Val Er: 1.6293368339538574;   Mean Tr AccTa: 0.4871999979019165, Mean Val AccTa: 0.48399999618530276\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.8327999949455261\n",
            "Time per stage: 2.281475305557251\n",
            "Progress: 7.5%   Mean Tr Er: 1.4578100538253784, Mean Val Er: 1.4686700868606568;   Mean Tr AccTa: 0.48399999737739563, Mean Val AccTa: 0.48399999618530276\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.7743999910354614\n",
            "Time per stage: 2.3752329349517822\n",
            "Progress: 10.0%   Mean Tr Er: 1.3837568712234498, Mean Val Er: 1.3922771835327148;   Mean Tr AccTa: 0.4927999949455261, Mean Val AccTa: 0.48959999680519106\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.7319999933242798\n",
            "Time per stage: 2.3929078578948975\n",
            "Progress: 12.5%   Mean Tr Er: 1.3476809692382812, Mean Val Er: 1.3576856327056885;   Mean Tr AccTa: 0.5039999985694885, Mean Val AccTa: 0.4991999959945679\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.7288000011444091\n",
            "Time per stage: 2.149386167526245\n",
            "Progress: 15.0%   Mean Tr Er: 1.3051306200027466, Mean Val Er: 1.3139411306381226;   Mean Tr AccTa: 0.5183999979496002, Mean Val AccTa: 0.5191999959945679\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6944000101089478\n",
            "Time per stage: 2.3022072315216064\n",
            "Progress: 17.5%   Mean Tr Er: 1.3052102613449097, Mean Val Er: 1.3092990922927856;   Mean Tr AccTa: 0.5280000030994415, Mean Val AccTa: 0.5248000037670135\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6952000069618225\n",
            "Time per stage: 2.5439748764038086\n",
            "Progress: 20.0%   Mean Tr Er: 1.2884085512161254, Mean Val Er: 1.3027850675582886;   Mean Tr AccTa: 0.5335999965667725, Mean Val AccTa: 0.5264000022411346\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6920000052452088\n",
            "Time per stage: 2.264221668243408\n",
            "Progress: 22.5%   Mean Tr Er: 1.2887002897262574, Mean Val Er: 1.288726372718811;   Mean Tr AccTa: 0.543199999332428, Mean Val AccTa: 0.5440000009536743\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6600000071525574\n",
            "Time per stage: 2.4089198112487793\n",
            "Progress: 25.0%   Mean Tr Er: 1.280741572380066, Mean Val Er: 1.2837316799163818;   Mean Tr AccTa: 0.5432000017166138, Mean Val AccTa: 0.552799997329712\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6640000081062317\n",
            "Time per stage: 3.0856895446777344\n",
            "Progress: 27.5%   Mean Tr Er: 1.2751810693740844, Mean Val Er: 1.2732582712173461;   Mean Tr AccTa: 0.5455999994277954, Mean Val AccTa: 0.5416000056266784\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6704000043869018\n",
            "Time per stage: 2.7705631256103516\n",
            "Progress: 30.0%   Mean Tr Er: 1.2722565269470214, Mean Val Er: 1.2933721733093262;   Mean Tr AccTa: 0.5479999995231628, Mean Val AccTa: 0.5448000037670135\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6567999982833862\n",
            "Time per stage: 2.513305425643921\n",
            "Progress: 32.5%   Mean Tr Er: 1.256165361404419, Mean Val Er: 1.2743615484237671;   Mean Tr AccTa: 0.5600000071525574, Mean Val AccTa: 0.5496000003814697\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6536000061035157\n",
            "Time per stage: 2.7092559337615967\n",
            "Progress: 35.0%   Mean Tr Er: 1.2626539802551269, Mean Val Er: 1.272530746459961;   Mean Tr AccTa: 0.5464000010490417, Mean Val AccTa: 0.5424000000953675\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6464000082015992\n",
            "Time per stage: 2.3193607330322266\n",
            "Progress: 37.5%   Mean Tr Er: 1.2498943662643434, Mean Val Er: 1.265933027267456;   Mean Tr AccTa: 0.5688000011444092, Mean Val AccTa: 0.5536000037193298\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6472000098228454\n",
            "Time per stage: 2.5110557079315186\n",
            "Progress: 40.0%   Mean Tr Er: 1.2564095735549927, Mean Val Er: 1.273811755180359;   Mean Tr AccTa: 0.5576000022888183, Mean Val AccTa: 0.5463999962806702\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6480000066757202\n",
            "Time per stage: 2.192943811416626\n",
            "Progress: 42.5%   Mean Tr Er: 1.25839252948761, Mean Val Er: 1.293210391998291;   Mean Tr AccTa: 0.553600002527237, Mean Val AccTa: 0.5448000025749207\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6344000053405762\n",
            "Time per stage: 2.1829209327697754\n",
            "Progress: 45.0%   Mean Tr Er: 1.273841037750244, Mean Val Er: 1.2735879278182984;   Mean Tr AccTa: 0.5584000027179719, Mean Val AccTa: 0.5551999926567077\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6320000052452087\n",
            "Time per stage: 2.2823734283447266\n",
            "Progress: 47.5%   Mean Tr Er: 1.263006796836853, Mean Val Er: 1.2613482427597047;   Mean Tr AccTa: 0.5560000014305114, Mean Val AccTa: 0.5560000050067901\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6303999996185303\n",
            "Time per stage: 2.4386327266693115\n",
            "Progress: 50.0%   Mean Tr Er: 1.2560519504547119, Mean Val Er: 1.2604951524734498;   Mean Tr AccTa: 0.5544000005722046, Mean Val AccTa: 0.5535999989509582\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6400000095367432\n",
            "Time per stage: 2.2882919311523438\n",
            "Progress: 52.5%   Mean Tr Er: 1.247791748046875, Mean Val Er: 1.2679199981689453;   Mean Tr AccTa: 0.5672000026702881, Mean Val AccTa: 0.5607999992370606\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6248000025749206\n",
            "Time per stage: 2.417942762374878\n",
            "Progress: 55.0%   Mean Tr Er: 1.276472625732422, Mean Val Er: 1.2632205009460449;   Mean Tr AccTa: 0.5591999959945678, Mean Val AccTa: 0.560799994468689\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.628000009059906\n",
            "Time per stage: 2.0391101837158203\n",
            "Progress: 57.5%   Mean Tr Er: 1.2802954196929932, Mean Val Er: 1.2733752870559691;   Mean Tr AccTa: 0.556000006198883, Mean Val AccTa: 0.5552000021934509\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6328000092506408\n",
            "Time per stage: 2.105409860610962\n",
            "Progress: 60.0%   Mean Tr Er: 1.2673972129821778, Mean Val Er: 1.2499255895614625;   Mean Tr AccTa: 0.5592000043392181, Mean Val AccTa: 0.5664000034332275\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6440000128746033\n",
            "Time per stage: 2.207564115524292\n",
            "Progress: 62.5%   Mean Tr Er: 1.238484387397766, Mean Val Er: 1.2601462984085083;   Mean Tr AccTa: 0.5663999986648559, Mean Val AccTa: 0.5648000049591064\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6264000058174133\n",
            "Time per stage: 2.3065690994262695\n",
            "Progress: 65.0%   Mean Tr Er: 1.2448491191864013, Mean Val Er: 1.2653424835205078;   Mean Tr AccTa: 0.5672000050544739, Mean Val AccTa: 0.5615999960899353\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6320000004768371\n",
            "Time per stage: 2.223324775695801\n",
            "Progress: 67.5%   Mean Tr Er: 1.2392243909835816, Mean Val Er: 1.2811600351333619;   Mean Tr AccTa: 0.5639999985694886, Mean Val AccTa: 0.5512000060081482\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6240000033378601\n",
            "Time per stage: 2.209420680999756\n",
            "Progress: 70.0%   Mean Tr Er: 1.2574002408981324, Mean Val Er: 1.2520610666275025;   Mean Tr AccTa: 0.5623999977111817, Mean Val AccTa: 0.5664000010490418\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6408000016212463\n",
            "Time per stage: 2.0478293895721436\n",
            "Progress: 72.5%   Mean Tr Er: 1.2640821886062623, Mean Val Er: 1.2464773559570312;   Mean Tr AccTa: 0.5632000029087066, Mean Val AccTa: 0.5728000020980835\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6360000014305115\n",
            "Time per stage: 2.113978385925293\n",
            "Progress: 75.0%   Mean Tr Er: 1.2370994520187377, Mean Val Er: 1.2313964653015137;   Mean Tr AccTa: 0.572800006866455, Mean Val AccTa: 0.5719999980926513\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6272000074386597\n",
            "Time per stage: 2.833468437194824\n",
            "Progress: 77.5%   Mean Tr Er: 1.2475085067749023, Mean Val Er: 1.2615685606002807;   Mean Tr AccTa: 0.5695999979972839, Mean Val AccTa: 0.5624000036716461\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.619200005531311\n",
            "Time per stage: 2.940269708633423\n",
            "Progress: 80.0%   Mean Tr Er: 1.2536749935150147, Mean Val Er: 1.2597458934783936;   Mean Tr AccTa: 0.5639999985694886, Mean Val AccTa: 0.5712000036239624\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6288000106811523\n",
            "Time per stage: 3.3136355876922607\n",
            "Progress: 82.5%   Mean Tr Er: 1.2371271228790284, Mean Val Er: 1.2509263467788696;   Mean Tr AccTa: 0.5727999997138977, Mean Val AccTa: 0.5679999995231628\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6288000106811523\n",
            "Time per stage: 2.3019816875457764\n",
            "Progress: 85.0%   Mean Tr Er: 1.2381475162506104, Mean Val Er: 1.2550507164001465;   Mean Tr AccTa: 0.5695999979972839, Mean Val AccTa: 0.5639999985694886\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6208000040054321\n",
            "Time per stage: 2.232539415359497\n",
            "Progress: 87.5%   Mean Tr Er: 1.2289873838424683, Mean Val Er: 1.2513976716995239;   Mean Tr AccTa: 0.5760000014305114, Mean Val AccTa: 0.5696000099182129\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6184000062942505\n",
            "Time per stage: 2.3306565284729004\n",
            "Progress: 90.0%   Mean Tr Er: 1.2394754314422607, Mean Val Er: 1.272518277168274;   Mean Tr AccTa: 0.5719999980926513, Mean Val AccTa: 0.5655999970436096\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.625600004196167\n",
            "Time per stage: 2.2349348068237305\n",
            "Progress: 92.5%   Mean Tr Er: 1.2368588161468506, Mean Val Er: 1.251697106361389;   Mean Tr AccTa: 0.5696000003814697, Mean Val AccTa: 0.5600000023841858\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6320000052452087\n",
            "Time per stage: 2.3649888038635254\n",
            "Progress: 95.0%   Mean Tr Er: 1.2338889503479005, Mean Val Er: 1.251002345085144;   Mean Tr AccTa: 0.5696000003814697, Mean Val AccTa: 0.5672000026702881\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.620800006389618\n",
            "Time per stage: 2.4293880462646484\n",
            "Progress: 97.5%   Mean Tr Er: 1.2395377922058106, Mean Val Er: 1.2638064336776733;   Mean Tr AccTa: 0.5767999982833862, Mean Val AccTa: 0.5672000002861023\n",
            "Progress:                                                                                           Mean Tr AccClCe: 0.9399999976158142, Mean Val AccClCe: 0.6040000009536743\n",
            "Saving response on iteration 999 of 1000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "swLR = 0.001#0.00046 # Sweep over these learning rates\n",
        "seed = 11117 # No. runs per hyperparameter\n",
        "# Update hyperparameter\n",
        "importlib.reload(par)\n",
        "par.etaInitial = swLR\n",
        "par.etaTransfer = 0.001\n",
        "par.fbLayer = 2\n",
        "par.saveFlag_FBWeights = True\n",
        "par.nEpochs = 1000\n",
        "par.nEpisodesOut = 1000\n",
        "par.maxLayer = 2\n",
        "par.lossLayer = 2\n",
        "par.nSaveMaxT = par.nEpochs \n",
        "par.metricLossType = 'prototypicalLoss'\n",
        "expName = 'test'\n",
        "# xent_esn_fb(expName, seed)\n",
        "metric_esn_fb(expName, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c625fc92",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-5,-7/3,7) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.saveFlag_FBWeights = True\n",
        "        par.maxLayer = 3\n",
        "        par.lossLayer = 3\n",
        "        par.nSaveMaxT = par.nEpochs \n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = 2\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'FB2_swLR_'+str(j)\n",
        "        complete[j, k] = xent_esn_fb(expName, sd)\n",
        "        # complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************FB2 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/FB2_swLR_0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB2********************************')\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.saveFlag_FBWeights = True\n",
        "        par.maxLayer = 3\n",
        "        par.lossLayer = 3\n",
        "        par.nSaveMaxT = par.nEpochs \n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = 1\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'FB1_swLR_'+str(j)\n",
        "        complete[j, k] = xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************FB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/FB1_swLR_0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB1********************************')\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.saveFlag_FBWeights = True\n",
        "        par.maxLayer = 3\n",
        "        par.lossLayer = 3\n",
        "        par.nSaveMaxT = par.nEpochs \n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = []\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'FB0_swLR_'+str(j)\n",
        "        complete[j, k] = xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************FB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/FB0_swLR_0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB0********************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "80762c57",
      "metadata": {},
      "source": [
        "# Run parameter sweep (MetricLearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30567a8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-5,-7/3,7) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "\n",
        "# complete = torch.zeros((len(swLR), len(seeds)))\n",
        "# for j, lr in enumerate(swLR):\n",
        "#     # if j!=0:\n",
        "#     #     continue\n",
        "#     for k, sd in enumerate(seeds):\n",
        "#         t = time.time()\n",
        "#         # Init RNG\n",
        "#         torch.manual_seed(sd)\n",
        "#         np.random.default_rng(sd)\n",
        "#         # Update hyperparameter\n",
        "#         importlib.reload(par)\n",
        "#         par.etaInitial = swLR[j]\n",
        "#         par.fbLayer = 2\n",
        "#         par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "#         expName = 'metFB2_swLR_'+str(j)\n",
        "#         complete[j, k] = metric_esn_fb(expName, sd)\n",
        "#         print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "#         print(f'*************************metFB2 Run time: {time.time()-t}')\n",
        "# completeName = directory+'/data/metFB2_swLR_0/complete.pt'\n",
        "# torch.save(complete, completeName)\n",
        "\n",
        "# print('*******************************FINISHED FB2********************************')\n",
        "\n",
        "# complete = torch.zeros((len(swLR), len(seeds)))\n",
        "# for j, lr in enumerate(swLR):\n",
        "#     # if j!=0:\n",
        "#     #     continue\n",
        "#     for k, sd in enumerate(seeds):\n",
        "#         t = time.time()\n",
        "#         # Init RNG\n",
        "#         torch.manual_seed(sd)\n",
        "#         np.random.default_rng(sd)\n",
        "#         # Update hyperparameter\n",
        "#         importlib.reload(par)\n",
        "#         par.etaInitial = swLR[j]\n",
        "#         par.fbLayer = 1\n",
        "#         par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "#         expName = 'metFB1_swLR_'+str(j)\n",
        "#         complete[j, k] = metric_esn_fb(expName, sd)\n",
        "#         print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "#         print(f'*************************metFB1 Run time: {time.time()-t}')\n",
        "# completeName = directory+'/data/metFB1_swLR_0/complete.pt'\n",
        "# torch.save(complete, completeName)\n",
        "\n",
        "# print('*******************************FINISHED FB1********************************')\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    # if j!=0:\n",
        "    #     continue\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Init RNG\n",
        "        torch.manual_seed(sd)\n",
        "        np.random.default_rng(sd)\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = []\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'metFB0_swLR_'+str(j)\n",
        "        complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************metFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/metFB0_swLR_0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB0********************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19b65aae",
      "metadata": {
        "id": "19b65aae"
      },
      "source": [
        "# Plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600e634a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600e634a",
        "outputId": "9cd64612-b412-4422-f3f5-f179e36a7907"
      },
      "outputs": [],
      "source": [
        "# ### Load Triplet data\n",
        "# inputDir = directory+'/data/'+experiment    # Storage directory for input/label data\n",
        "# met = torch.load(inputDir + 'met_save.pt')\n",
        "# l_tri = torch.load(inputDir + 'loss_triplet.pt')\n",
        "# l_out = torch.load(inputDir + 'loss_out.pt')\n",
        "# a_tri = torch.load(inputDir + 'acc_triplet.pt')\n",
        "# a_out = torch.load(inputDir + 'acc_out.pt')\n",
        "# dist = torch.load(inputDir + 'dist_triplet.pt')\n",
        "# nt_dist = dist.shape[0] \n",
        "# nt_out = l_out.shape[0]\n",
        "# nt_met = met.shape[0]\n",
        "# N_triplet=45000\n",
        "# N_out=5000\n",
        "# batch_size=64\n",
        "# eta_t=0.0002\n",
        "# eta_o = 0.001\n",
        "# eta_t_tau = 40000.0\n",
        "# eta_o_tau = 4000.0\n",
        "# N_class=10\n",
        "# margin=2\n",
        "# save_N = 100 #100 # # of saved epochs\n",
        "# save_every = np.floor(N_triplet / save_N) # Save data every <> epohsChoice 3\n",
        "# save_Nsamples = 100 # # of inputs from each class for which to save resonses\n",
        "\n",
        "### Load Classic data\n",
        "experiment = 'met_FB0_swLR_0'\n",
        "expDir = directory+'/data/'+experiment # To read in saved data\n",
        "sd_name='11113'\n",
        "inputDir = expDir#+'/'+sd_name  \n",
        "figDir = directory+'/figs/met/'+experiment # To export figures\n",
        "print(figDir)\n",
        "if not os.path.exists(figDir):\n",
        "    os.mkdir(figDir)\n",
        "\n",
        "RESP = torch.load(inputDir + '/respSave'+sd_name+'.pt')\n",
        "print(f'Num layers = {len(RESP)}')\n",
        "lossTr = torch.load(inputDir + '/lossTr'+sd_name+'.pt')\n",
        "accTr = torch.load(inputDir + '/accTr'+sd_name+'.pt')\n",
        "accVal = torch.load(inputDir + '/accVal'+sd_name+'.pt')\n",
        "weights = torch.load(inputDir + '/weightSave'+sd_name+'.pt')\n",
        "nt = RESP[0].shape[-1]\n",
        "kernel = np.ones(50)/50\n",
        "\n",
        "### Set figure properties\n",
        "import matplotlib\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams.update({'font.size': 6})\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91085166",
      "metadata": {},
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e873e09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# saveflag = True\n",
        "saveflag = False\n",
        "\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# layer = 2\n",
        "# c=0\n",
        "# times=[0, 4]\n",
        "\n",
        "# ### Plot responses\n",
        "# # t=0\n",
        "# # for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "# #             pl.plot(RESP[layer][t,i,::10,:].transpose(), linewidth=0.5)\n",
        "# for c in [0, 1, 9]:\n",
        "#     for j, t in enumerate(times):\n",
        "#         for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "#             pl.plot(RESP[layer][t,i,:,:].transpose(), linewidth=0.5)\n",
        "#         if j==0:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,0.2))\n",
        "#         elif j==1:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,10,20))\n",
        "#         if saveflag:\n",
        "#             pl.savefig(figDir+'/resp_c'+str(c)+'t'+str(j)+'.svg', format=\"svg\")\n",
        "#         ax.cla() \n",
        "\n",
        "# ### Plot accuracy\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.linspace(1,par.nEpochs,par.nEpochs),np.convolve(np.pad(accTr, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# pl.plot(np.linspace(1,par.nEpochs,par.nEpochs),np.convolve(np.pad(accVal, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# # pl.plot(accTr, linewidth=0.5); pl.plot(accVal, linewidth=0.5)\n",
        "# ax.xaxis.set_ticks((0,par.nEpochs)); ax.yaxis.set_ticks((0,1))\n",
        "# if saveflag:\n",
        "#     pl.savefig(figDir+'/accTrVal.svg', format=\"svg\")\n",
        "\n",
        "### Plot Feedback Weight Evolution\n",
        "if par.saveFlag_FBWeights:\n",
        "    fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "    ax.spines[['top','right']].set_visible(False)\n",
        "    for j in range(0,1000,100):\n",
        "        pl.plot(np.linspace(1,par.nEpochs,par.nSave+1),weights[::2,j,:].transpose(), linewidth=0.5)\n",
        "ax.xaxis.set_ticks((0,par.nEpochs)); ax.yaxis.set_ticks((-.05,0,.05))\n",
        "if saveflag:\n",
        "    pl.savefig(figDir+'/fbWeights.svg', format=\"svg\")\n",
        "\n",
        "# ### Correlation between met responses\n",
        "# layer = 1\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# # imdata = ax.imshow(stats.zscore(np.reshape(RESP[1][0,:,:,:],[RESP[1].shape[1], RESP[1].shape[2]*RESP[1].shape[3]]), 1).transpose(),vmin=-1.0, vmax=1.0)\n",
        "# c0 = np.matmul(stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# c1 = np.matmul(stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "82efca1a",
      "metadata": {
        "id": "82efca1a"
      },
      "source": [
        "### Plot triplet distances and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90dcb30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "e90dcb30",
        "outputId": "2637691c-fdaa-4170-c851-b3d7de14cff7"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as pl\n",
        "# saveflag = True\n",
        "saveflag = False\n",
        "tripletflag = True\n",
        "# tripletflag = False\n",
        "\n",
        "kernel = np.ones(50)/50\n",
        "prefix = experiment#'classic_original_'\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams['axes.labelsize'] = 6\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "###Plot distances\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,0]); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,1], linestyle='dashed')\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,0], linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,1], linewidth=0.5)\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=20)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,15,30))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,20))\n",
        "  # ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'distances.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplett loss\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_distX_tr.shape)),np.convolve(l_tri,kernel,mode='same')); \n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),l_tri, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(l_tri, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  # ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=0.5*1.05*np.max(l_tri))\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=1.0)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,1))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'loss_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplet accuracy\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),a_tri*100, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(a_tri*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,100))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'acc_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot output loss\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),np.convolve(l_tri,kernel,mode='same')); \n",
        "pl.plot(np.linspace(1,nt_out,nt_out),l_out, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(l_out, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=0.8)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0.0,0.8))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "ax.set_xticklabels((0, nt_out), fontdict={'family': 'sans-serif', 'size':5})\n",
        "ax.set_yticklabels((0.5,0.8), fontdict={'family': 'sans-serif', 'size':5})\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'loss_out.svg', format=\"svg\")\n",
        "\n",
        "###Plot output accuracy\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),a_out*100, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(a_out*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0,100))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'acc_out.svg', format=\"svg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a8b3d98",
      "metadata": {
        "id": "3a8b3d98"
      },
      "source": [
        "### Plot met responses over training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c442906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "1c442906",
        "outputId": "97f4589d-d70e-4cb2-e668-7042ecb759a8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as pl\n",
        "from scipy import stats\n",
        "pl.rcParams['savefig.dpi'] = 400\n",
        "\n",
        "saveflag = True\n",
        "# saveflag = False\n",
        "\n",
        "### Raw met reponses\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,5.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "plmet = np.squeeze(met[0,:,199])\n",
        "pl.plot(np.linspace(1,nt_met,nt_met)*save_every,plmet)\n",
        "ax.set_xlim(xmin=0, xmax=nt_met*save_every); ax.set_ylim(ymin=0.0, ymax=1.05*np.max(plmet))\n",
        "ax.xaxis.set_ticks((0,nt_met*save_every)); ax.yaxis.set_ticks((0,))\n",
        "pl.show()\n",
        "\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[0,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[-1,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1.svg', format=\"svg\")\n",
        "\n",
        "### Correlation between met responses\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = np.matmul(stats.zscore(np.squeeze(met[0,:,:]),0).transpose(), stats.zscore(np.squeeze(met[0,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c1 = np.matmul(stats.zscore(np.squeeze(met[-1,:,:]),0).transpose(), stats.zscore(np.squeeze(met[-1,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9f4c2a58",
      "metadata": {
        "id": "9f4c2a58"
      },
      "source": [
        "# Plot weights"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
