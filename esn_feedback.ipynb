{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aW_6ADldkbuB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW_6ADldkbuB",
        "outputId": "fe8306c7-3a8e-4165-e62e-41a424a840b9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "id": "_QJywZpKBcrP",
      "metadata": {
        "id": "_QJywZpKBcrP"
      },
      "outputs": [],
      "source": [
        "directory='.'\n",
        "# directory='/content/drive/MyDrive/esn2sparse'; \n",
        "# !cp \"/content/drive/MyDrive/esn2sparse/params.py\" \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "id": "0caa84e4",
      "metadata": {
        "id": "0caa84e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from scipy import stats\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as pl\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import time\n",
        "import torch.jit as jit\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from os.path import exists\n",
        "import gc\n",
        "import importlib\n",
        "import params_feedback as par\n",
        "import time\n",
        "import os\n",
        "# device = 'cpu'\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "id": "e75f2936",
      "metadata": {
        "id": "e75f2936"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root=directory+'/data', train=True, download=True, transform=None)\n",
        "mnist_testset = datasets.MNIST(root=directory+'/data', train=False, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "id": "d9c42c75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9c42c75",
        "outputId": "0928ebb6-40ac-4b31-b171-06853d2d74fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/its/home/jb739/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "/its/home/jb739/anaconda3/envs/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_tr shape = torch.Size([50000, 28, 28]),    Y_tr shape = torch.Size([50000, 10])\n"
          ]
        }
      ],
      "source": [
        "X_te=mnist_testset.data               ## Test set images\n",
        "y_te=mnist_testset.test_labels        ## Test set labels\n",
        "\n",
        "N_o=10                                ## Number of output nodes/classes\n",
        "N_te=y_te.size()[0]                   ## Number of test samples\n",
        "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
        "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
        "\n",
        "X_tr=mnist_trainset.data              ## Train set images\n",
        "y_tr=mnist_trainset.train_labels      ## Train labels \n",
        "N_tr=y_tr.size()[0]                   ## Number of training samples\n",
        "N_i = X_tr.size()[1]                  ## Number of inputs to ESN\n",
        "\n",
        "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
        "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
        "\n",
        "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
        "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
        "\n",
        "X_val=X_tr[i_val,:,:]\n",
        "Y_val=Y_tr[i_val,:]\n",
        "\n",
        "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
        "N_tr=N_tr-N_val\n",
        "\n",
        "X_tr=X_tr[i_tr,:,:]\n",
        "Y_tr=Y_tr[i_tr,:]\n",
        "\n",
        "T=X_tr.size()[2]\n",
        "N_in=X_tr.size()[1]\n",
        "\n",
        "## Normalisation and conversion to float\n",
        "X_M=255\n",
        "# X_tr=torch.reshape( (X_tr.float()/X_M),[-1,784]) \n",
        "# X_val=torch.reshape((X_val.float()/X_M),[-1,784])\n",
        "# X_te=torch.reshape((X_te.float()/X_M),[-1,784])\n",
        "X_tr=torch.reshape( (X_tr.float()),[-1,784]).to(device)\n",
        "X_val=torch.reshape((X_val.float()),[-1,784]).to(device)\n",
        "X_te=torch.reshape((X_te.float()),[-1,784]).to(device)\n",
        "\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])\n",
        "\n",
        "X_tr=torch.reshape( (X_tr),[-1,28,28]) \n",
        "X_val=torch.reshape((X_val),[-1,28,28])\n",
        "X_te=torch.reshape((X_te),[-1,28,28])\n",
        "\n",
        "Y_tr=Y_tr.float().to(device)\n",
        "Y_val=Y_val.float().to(device)\n",
        "Y_te=Y_te.float().to(device)\n",
        "\n",
        "print(f'X_tr shape = {X_tr.shape},    Y_tr shape = {Y_tr.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2148197e",
      "metadata": {
        "id": "2148197e"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data (OLD VERSION - results in outlines)\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,X_tr[j,:]==X_tr[j,:].min()] = X_tr[j,X_tr[j,:]==X_tr[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_tr[j,:].max() - X_tr[j,:].min())\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,X_val[j,:]==X_val[j,:].min()] = X_val[j,X_val[j,:]==X_val[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_val[j,:].max() - X_val[j,:].min())\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,X_te[j,:]==X_te[j,:].min()] = X_te[j,X_te[j,:]==X_te[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_te[j,:].max() - X_te[j,:].min())\n",
        "\n",
        "# Z-score inputs\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a64e52",
      "metadata": {
        "id": "d8a64e52"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data\n",
        "contrast_mean = torch.tensor(0.5).to(device)\n",
        "contrast_range = torch.tensor(0.99).to(device)\n",
        "# Training data\n",
        "for j in range(X_tr.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_tr[j,:] -= torch.min(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.max(X_tr[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_tr[j,:] = torch.tensor(1.0).to(device) - X_tr[j,:]\n",
        "    # Z-score\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= (torch.std(X_tr[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Validation data\n",
        "for j in range(X_val.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_val[j,:] -= torch.min(X_val[j,:])\n",
        "    X_val[j,:] /= torch.max(X_val[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_val[j,:] = torch.tensor(1.0).to(device) - X_val[j,:]\n",
        "    # Z-score\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= (torch.std(X_val[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Test data\n",
        "for j in range(X_te.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_te[j,:] -= torch.min(X_te[j,:])\n",
        "    X_te[j,:] /= torch.max(X_te[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_te[j,:] = torch.tensor(1.0).to(device) - X_te[j,:]\n",
        "    # Z-score\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= (torch.std(X_te[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a05756",
      "metadata": {
        "id": "04a05756"
      },
      "outputs": [],
      "source": [
        "# IF NOT USING ESN\n",
        "# Make inputs 2800-dim and z-score\n",
        "from scipy import stats\n",
        "with torch.no_grad():\n",
        "    w = torch.randn([784, par.N_esn*28]).to(device)/torch.sqrt(torch.tensor(par.N_esn*28+784)).to(device)\n",
        "    batchsize = torch.tensor(100).to(device)\n",
        "    # Operate on batches of data to save GPU-memory\n",
        "    # Training data\n",
        "    nbatch = torch.ceil(X_tr.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_tr.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_tr[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]),:,:].reshape(batchsize, X_tr.shape[1]*X_tr.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_tr = torch.clone(perm).cpu()\n",
        "    # Validation data\n",
        "    nbatch = torch.ceil(X_val.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_val.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_val[j*batchsize:min((j+1)*batchsize,X_val.shape[0]),:,:].reshape(batchsize, X_val.shape[1]*X_val.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_val.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_val = torch.clone(perm).cpu()\n",
        "    # Test data\n",
        "    nbatch = torch.ceil(X_te.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_te.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_te[j*batchsize:min((j+1)*batchsize,X_te.shape[0]),:,:].reshape(batchsize, X_te.shape[1]*X_te.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_te.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_te = torch.clone(perm).cpu()\n",
        "    \n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "\n",
        "    # X_tr = torch.tensor(stats.zscore(torch.matmul(X_tr.reshape(X_tr.shape[0], X_tr.shape[1]*X_tr.shape[2]), w).numpy(), axis=1))\n",
        "    # X_val = torch.tensor(stats.zscore(torch.matmul(X_val.reshape(X_val.shape[0], X_val.shape[1]*X_val.shape[2]), w).numpy(), axis=1))\n",
        "    # X_te = torch.tensor(stats.zscore(torch.matmul(X_te.reshape(X_te.shape[0], X_te.shape[1]*X_te.shape[2]), w).numpy(), axis=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "id": "3f55bc9b",
      "metadata": {
        "id": "3f55bc9b"
      },
      "outputs": [],
      "source": [
        "def Data2Classes(X,Y):\n",
        "    \n",
        "    ind=torch.where(Y==1)[1]\n",
        "\n",
        "    N_class=torch.max(ind)+1\n",
        "    \n",
        "    X1=[]\n",
        "    Y1=[]\n",
        "    \n",
        "    for n in range(N_class):\n",
        "    \n",
        "        ind1=torch.where(ind==n)[0].type(torch.long)\n",
        "\n",
        "        X1.append(X[ind1,:].to(device))\n",
        "        Y1.append(Y[ind1,:].to(device))\n",
        "        \n",
        "    return X1, Y1\n",
        "        \n",
        "# X_tr/X_val/X_te are lists of length 10 (1 entry per class)\n",
        "X_tr, Y_tr=Data2Classes(X_tr,Y_tr)\n",
        "\n",
        "X_val, Y_val=Data2Classes(X_val,Y_val)\n",
        "\n",
        "X_te, Y_te=Data2Classes(X_te,Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhLHzeGYPsE1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "zhLHzeGYPsE1",
        "outputId": "c15a1dbb-6850-4f41-febc-a380d4d35bae"
      },
      "outputs": [],
      "source": [
        "# Plot activity and correlation between classes\n",
        "N_esn = par.N_esn\n",
        "a = np.zeros((1000,N_esn*28))\n",
        "for j in range(len(X_tr)):\n",
        "    a[j*100:(j+1)*100,:] = X_tr[j][0:100,:].numpy()\n",
        "print(np.max(a), np.min(a))\n",
        "c = np.matmul(stats.zscore(a,axis=1), np.transpose(stats.zscore(a,axis=1))) / a.shape[1]\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[0][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[2][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(c,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[0, 1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85bd1a87",
      "metadata": {},
      "source": [
        "xent: \n",
        " - train from layer 0 to layer -1\n",
        " - train forward pass to layer -1\n",
        " - train loss from layer -1\n",
        " - transfer from layer tranFrom to layer -1\n",
        " - transfer forward pass to layer -1\n",
        " - transfer loss from layer -1\n",
        "\n",
        "metric:\n",
        " - train from layer 0 to layer tranFrom-1\n",
        " - train forward pass to layer tranFrom-1\n",
        " - train loss from layer tranForm-1\n",
        " - transfer frmo layer tranFrom to layer -1\n",
        " - transfer forward pass to layer -1\n",
        " - transfer loss from layer -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "fc6708d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPclassic(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        self.lossfn = nn.BCEWithLogitsLoss()\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)\n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.fbLayer = par.fbLayer\n",
        "        self.lossLayer = len(par.Ns)-1\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        \n",
        "        self.Ns = par.Ns\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/torch.sqrt(torch.tensor(self.Ns[self.fbLayer] + self.N))).to(device))\n",
        "\n",
        "    def initialOptimiser(self):\n",
        "        if self.fbLayer:\n",
        "            self.iniOpt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':self.etaInitial }])\n",
        "        else:\n",
        "            self.iniOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaInitial }])\n",
        "        \n",
        "    def Forward(self, input):\n",
        "        ### ESN\n",
        "        if self.fbLayer:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.xFB, self.W_fb))\n",
        "        else:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W))\n",
        "        \n",
        "        ### Hidden layers\n",
        "        for n in range(1,len(self.Ns)-1): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n] = torch.relu( torch.add(torch.matmul(self.x[n-1],self.Ws[n-1]),self.bs[n-1]) )\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "        \n",
        "        ### Output\n",
        "        self.x[-1] = torch.add(torch.matmul(self.x[-2],self.Ws[-1]),self.bs[-1])\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True).to(device))\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "    \n",
        "    def lossCrossEntropy(self, nSamples):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(self.x[-1]), torch.sum(torch.exp(self.x[-1]), 1, keepdim=True).tile((1,self.x[-1].shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "\n",
        "    def accuracyTarget(self):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(self.x[-1],1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self):\n",
        "        nSamples = self.x[-1].shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device)\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = self.x[-1][j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(self.x[-1][j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "    \n",
        "    def getLossAccuracy(self, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        L = self.lossCrossEntropy(self.x[-1].shape[0])\n",
        "        accTa = self.accuracyTarget()\n",
        "        accClCe = self.accuracyClassCentroid()\n",
        "\n",
        "        if backwardFlag:\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            \n",
        "        return L,accTa,accClCe\n",
        "    \n",
        "    def response(self, Input):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        \n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t])\n",
        "\n",
        "    # def generateBatch(self, X):\n",
        "       \n",
        "    #     batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
        "    #     self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
        "    #     # for j in range(self.batch_size):\n",
        "    #     #     ind1 = torch.randint(0,self.nClass,[1])\n",
        "    #     #     ind2 = torch.randint(0,X[ind1].shape[0],[1])\n",
        "    #     #     batch[j,:,:] = torch.clone(X[ind1][ind2,:,:])\n",
        "    #     #     self.target[j,ind1] = 1.\n",
        "\n",
        "    #     for k in range(self.nClass):\n",
        "    #         rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "    #         batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
        "    #         self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
        "\n",
        "    #     return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t])\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "id": "GWXIPFSI_Ynd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWXIPFSI_Ynd",
        "outputId": "00ee5616-9405-48e4-b18f-5efaa192ed57"
      },
      "outputs": [],
      "source": [
        "###\n",
        "### WITHOUT metric learning\n",
        "###\n",
        "def xent_esn_fb(expName,rngSeed):\n",
        "\n",
        "    ### Initiliase RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save weights every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2]\n",
        "\n",
        "    ###############################\n",
        "    #### First phase of training\n",
        "    ###############################\n",
        "\n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nWeightSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    savDW = [] # List of arrays to store DW histograms over time\n",
        "    for k in range(len(par.Ns)-1):\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPclassic(par)\n",
        "\n",
        "    MOD.initialOptimiser()\n",
        "    L_tr = []; L_tr.append(np.zeros([par.nEpisodes]))\n",
        "    A_trTa = []; A_trTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_trClCe = []; A_trClCe.append(np.zeros([par.nEpisodes]))\n",
        "    L_val = []; L_val.append(np.zeros([par.nEpisodes]))\n",
        "    A_valTa = []; A_valTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_valClCe = []; A_valClCe.append(np.zeros([par.nEpisodes]))\n",
        "\n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "    for n in range(par.nEpisodes):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodes-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpisodes}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li, l in enumerate(par.saveLayers):\n",
        "                    RESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights and (n%save_every)<1:\n",
        "            savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "        \n",
        "        # Get response to input\n",
        "        MOD.response(Im)\n",
        "        # Compute loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy(backwardFlag=True, opt=MOD.iniOpt)\n",
        "\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpisodes}')\n",
        "            return -1\n",
        "        #####\n",
        "        #####\n",
        "        \n",
        "        # Store loss and accuracy\n",
        "        L_tr[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_trTa[0][n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        A_trClCe[0][n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "\n",
        "        # Store weight changes for effective learning rate\n",
        "        if par.saveFlag_DW and (n%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "        if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "            with torch.no_grad():\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    savDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "        \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Get response to input\n",
        "            MOD.response(Im)\n",
        "            # Compute loss and accuracy\n",
        "            loss, accValTa, accValClCe = MOD.getLossAccuracy()\n",
        "            \n",
        "            # Store loss and accuracy\n",
        "            L_val[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_valTa[0][n]=np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            A_valClCe[0][n]=np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "        \n",
        "        # Update learning rate\n",
        "        MOD.iniOpt.param_groups[0]['lr'] = par.etaInitial * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if ((n+1)%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrTa_mean=np.mean(A_trTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrClCe_mean=np.mean(A_trClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            mseVal_mean=np.mean(L_val[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValTa_mean=np.mean(A_valTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValClCe_mean=np.mean(A_valClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n+1)/np.float32(par.nEpisodes)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                                           Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ### Save hidden layer representations for transfer learning\n",
        "    H_tr = []\n",
        "    H_val = []\n",
        "    with torch.no_grad():\n",
        "        for j in range(par.nClass):\n",
        "            MOD.response(X_tr[j])\n",
        "            H_tr.append(MOD.x[par.saveRepLayer])\n",
        "            MOD.response(X_val[j])\n",
        "            H_val.append(MOD.x[par.saveRepLayer])\n",
        "    \n",
        "\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD, outputDir + '/' + 'model'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    torch.save([H_tr, H_val], outputDir + '/' + 'hResp'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "id": "dfa4cf3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPmetric(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)    \n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.nInputs = par.nInputs\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.tMax = par.tMax\n",
        "        self.fbLayer = par.fbLayer\n",
        "        self.metricLossType = par.metricLossType\n",
        "        self.margin = par.margin\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        self.batch_size = par.batch_size\n",
        "        if par.metricLossType=='prototypicalLoss':\n",
        "            self.nP = par.nSampProto\n",
        "            self.nQ = int(par.batch_size / par.nClass)\n",
        "        self.tri = 1 if par.metricLossType=='tripletLoss' else 0\n",
        "        self.wPerf = torch.exp(-torch.arange(self.tMax).flip(0)/par.tauPerf)\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ns=par.Ns\n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/10**4).to(device))\n",
        "\n",
        "    def metricOptimiser(self):\n",
        "\n",
        "        if self.fbLayer:\n",
        "            self.metOpt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':self.etaInitial }])\n",
        "        else:\n",
        "            self.metOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaInitial }])\n",
        "\n",
        "    def Forward(self, input):\n",
        "\n",
        "        ### ESN\n",
        "        if self.fbLayer:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.xFB, self.W_fb))\n",
        "        else:\n",
        "            self.x[0] = (1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                torch.tanh(torch.matmul(input, self.W_in)+torch.matmul(self.x[0], self.W))\n",
        "        \n",
        "        ### Hidden layers\n",
        "        for n in range(1,len(self.Ns)): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n] = torch.relu( torch.add(torch.matmul(self.x[n-1],self.Ws[n-1]),self.bs[n-1]) )\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True).to(device))\n",
        "            if n==self.fbLayer:\n",
        "                self.xFB = torch.clone(self.x[n])\n",
        "\n",
        "    def lossCrossEntropy(self, nSamples, r):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(r), torch.sum(torch.exp(r), 1, keepdim=True).tile((1,r.shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "    \n",
        "    def tripletLoss(self, resp):\n",
        "        # Implement hard negative mining\n",
        "\n",
        "        dAP = (resp[0] - resp[1]).pow(2).sum(1).sqrt() # anchor-positive\n",
        "        dAN = (resp[0] - resp[2]).pow(2).sum(1).sqrt() # anchor-negative\n",
        "        \n",
        "        dPN = (resp[1] - resp[2]).pow(2).sum(1).sqrt() # positive-negative\n",
        "        ind = torch.le(dPN, dAN)\n",
        "        ind1 = torch.nonzero(ind)\n",
        "        ind2 = torch.nonzero(torch.logical_not(ind))\n",
        "        \n",
        "        L = torch.concat((torch.maximum(torch.zeros(ind2.shape).to(device), dAP[ind2] - dAN[ind2] + self.margin), \n",
        "                         torch.maximum(torch.zeros(ind1.shape).to(device), dAP[ind1] - dPN[ind1] + self.margin)), dim=0).mean()\n",
        "        # if t%50==0:\n",
        "        #     print(f'DP = {dAP.mean()}                            DN = {dAN.mean()}                           maxRESP = {resp[0].max()}')\n",
        "\n",
        "        return L.mean()\n",
        "    \n",
        "    def prototypicalLoss(self, r):\n",
        "        proto = torch.zeros(self.N_class, self.Ns[-1]).to(device) # init mem for prototypes\n",
        "        dist = torch.zeros(self.N_class*self.nQ, self.N_class).to(device) # init mem for distances\n",
        "        prob = torch.zeros(self.N_class*self.nQ).to(device) # Init mem for probabilities\n",
        "        # Compute prototypes\n",
        "        for j in range(self.N_class):\n",
        "            proto[j,:] = torch.clone(r[j*self.nP:(j+1)*self.nP,:]).mean(0)\n",
        "        for j in range(self.N_class): # for each class\n",
        "            for k in range(self.nQ): # for each query in class j\n",
        "                # Compute distances between queries and prototypes\n",
        "                fx = torch.clone(r[self.nP*self.N_class + j*self.nQ + k,:]).unsqueeze(0).tile(self.N_class,1)\n",
        "                dist[j*self.nQ+k,:] = (fx - proto).pow(2).sum(1).sqrt()\n",
        "            # Compute probabilities using softmax\n",
        "            prob[j*self.nQ:(j+1)*self.nQ] = torch.divide(torch.exp(-dist[j*self.nQ:(j+1)*self.nQ,j]), \n",
        "                                               torch.sum(torch.exp(-dist[j*self.nQ:(j+1)*self.nQ,:]), 1))\n",
        "        \n",
        "        L = - torch.log(prob)\n",
        "\n",
        "        return L.mean(), proto\n",
        "    \n",
        "    def accuracyTarget(self, r):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(r,1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self, r, prototypes=[]):\n",
        "        nSamples = r.shape[0]\n",
        "        # Compute class centroids\n",
        "        if len(prototypes)>0:\n",
        "            centroids = prototypes\n",
        "        else:\n",
        "            centroids = torch.zeros(self.N_class, r.shape[1]).to(device)\n",
        "            for j in range(self.N_class):\n",
        "                centroids[j,:] = r[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(r[j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "\n",
        "    def getLossAccuracy(self, learning, r, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        if learning=='metric':\n",
        "            if self.metricLossType=='tripletLoss':\n",
        "                L = self.tripletLoss(r)\n",
        "                accTa = self.accuracyTarget(r[0])\n",
        "                accClCe = self.accuracyClassCentroid(r[0])\n",
        "            elif self.metricLossType=='prototypicalLoss':\n",
        "                L, proto = self.prototypicalLoss(r[0])\n",
        "                accTa = self.accuracyTarget(r[0][self.N_class*self.nP:,:])\n",
        "                accClCe = self.accuracyClassCentroid(r[0][self.N_class*self.nP:,:], proto)\n",
        "                \n",
        "        elif learning=='transfer':\n",
        "            L = self.lossCrossEntropy(r.shape[0], r)\n",
        "            accTa = self.accuracyTarget(r)\n",
        "            accClCe = self.accuracyClassCentroid(r)\n",
        "\n",
        "        if backwardFlag:          \n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        return L,accTa,accClCe\n",
        "      \n",
        "    def response(self, Input, tripletFlag=False):\n",
        "\n",
        "        if tripletFlag:\n",
        "            N_samples = int(Input.shape[0] / 3)\n",
        "        else:\n",
        "            N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "\n",
        "        # Forward pass for anchor, positive, and negative\n",
        "        if tripletFlag:\n",
        "            self.Reset(N_samples * 3)\n",
        "        else:\n",
        "            self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:,:,t])\n",
        "\n",
        "        # Compile list of responses from lossLayer. When using Triplet Loss, \n",
        "        # list is 3x1 for [anchor, positive, negative]\n",
        "        r = [] \n",
        "        for j in range(1+2*int(tripletFlag)):\n",
        "            r.append(torch.clone(self.x[-1][j*N_samples:(j+1)*N_samples,:]))\n",
        "        \n",
        "        return r\n",
        "    \n",
        "    def generateBatch(self, method, X):\n",
        "       \n",
        "        ### For normal batches, e.g. for transfer learning\n",
        "        if method=='simple':\n",
        "            batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
        "            self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
        "            for k in range(self.nClass):\n",
        "                rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
        "                self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
        "        \n",
        "        ### For triplet loss\n",
        "        if method=='tripletLoss':\n",
        "            \n",
        "            # Populate batch\n",
        "            batch = torch.zeros([3*self.batch_size, self.nInputs,self.tMax]).to(device)\n",
        "            for k in range(self.N_class):\n",
        "                # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
        "                ind_ap = np.random.choice(X[k].shape[0],(self.nSampPerClassPerBatch,2), replace=False)\n",
        "                # Populate Anchor and Positive samples\n",
        "                batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,0],:])\n",
        "                batch[self.batch_size+k*self.nSampPerClassPerBatch:self.batch_size+(k+1)*self.nSampPerClassPerBatch,:] = torch.clone(X[k][ind_ap[:,1],:])\n",
        "                # Populate negative samples\n",
        "                randClass = np.random.choice((np.arange(self.N_class)!=k).nonzero()[0], self.nSampPerClassPerBatch)\n",
        "                for m, cl in enumerate(randClass):\n",
        "                    batch[2*self.batch_size + k*self.nSampPerClassPerBatch+m,:] = torch.clone(X[cl][np.random.randint(X[cl].shape[0]),:])\n",
        "            \n",
        "        ### For Protoypical Loss\n",
        "        if method=='prototypicalLoss':\n",
        "            batch = torch.zeros((self.nP+self.nQ)*self.N_class, X[0].shape[1], X[0].shape[2]).to(device)\n",
        "            for j, x in enumerate(X): # For each class (X is a list)\n",
        "                ind = np.random.choice(x.shape[0], self.nP+self.nQ, replace=False)\n",
        "                batch[j*self.nP:(j+1)*self.nP,:] = torch.clone(x[ind[:self.nP],:])\n",
        "                batch[self.nP*self.N_class+j*self.nQ:self.nP*self.N_class+(j+1)*self.nQ] = torch.clone(x[ind[self.nP:],:])\n",
        "\n",
        "        return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:,:,t])\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "id": "f140603c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# swLR = 0.00001#0.00046 # Sweep over these learning rates\n",
        "# rngSeed = 11117 # No. runs per hyperparameter\n",
        "\n",
        "# # Update hyperparameter\n",
        "# importlib.reload(par)\n",
        "# par.eta = swLR\n",
        "# par.fbLayer = 2\n",
        "# par.saveFlag_FBWeights = True\n",
        "# par.nEpisodes = 50\n",
        "# par.nSaveMaxT = par.nEpisodes\n",
        "# expName = 'test'\n",
        "###\n",
        "### WITH metric learning\n",
        "###\n",
        "def metric_esn_fb(expName, rngSeed):\n",
        "\n",
        "    ### Initialise RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save data every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2] # No. time steps per input sequence\n",
        "    \n",
        "    ###############################\n",
        "    #### First phase of training\n",
        "    ###############################\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        print('Saving weights')\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nWeightSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    savDW = [] # List of arrays to store DW histograms over time\n",
        "    for k in range(len(par.Ns)-1):\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        savDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPmetric(par)\n",
        "    MOD.metricOptimiser()\n",
        "\n",
        "    ### Init memory for saving data\n",
        "    L_tr = []; L_tr.append(np.zeros([par.nEpisodes]))\n",
        "    A_trTa = []; A_trTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_trClCe = []; A_trClCe.append(np.zeros([par.nEpisodes]))\n",
        "    L_val = []; L_val.append(np.zeros([par.nEpisodes]))\n",
        "    A_valTa = []; A_valTa.append(np.zeros([par.nEpisodes]))\n",
        "    A_valClCe = []; A_valClCe.append(np.zeros([par.nEpisodes]))\n",
        "    \n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpisodes):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodes-1)):\n",
        "            print(f'Saving response on iteration {n} of {par.nEpisodes}')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li, l in enumerate(par.saveLayers):\n",
        "                    RESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "            resp_saveind += 1\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights and (n%save_every)<1:\n",
        "            savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        Im = MOD.generateBatch(par.metricLossType, X_tr)\n",
        "\n",
        "        # Get response to input\n",
        "        if MOD.metricLossType=='tripletLoss':\n",
        "            r = MOD.response(Im, tripletFlag=True)\n",
        "        else:\n",
        "            r = MOD.response(Im)\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy('metric',r,backwardFlag=True, opt=MOD.metOpt)\n",
        "\n",
        "        ####\n",
        "        #### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpisodes}')\n",
        "            return -1\n",
        "        ####\n",
        "        ####\n",
        "\n",
        "        # Store training loss and accuracy\n",
        "        L_tr[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_trTa[0][n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        A_trClCe[0][n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "\n",
        "        # Store weight changes for effective learning rate\n",
        "        with torch.no_grad():\n",
        "            if par.saveFlag_DW and (n%save_every)<1:\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(len(MOD.Ns)-1):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "            if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(len(MOD.Ns)-1):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    savDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "\n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            Im = MOD.generateBatch(par.metricLossType, X_val)\n",
        "\n",
        "            # Get response to input\n",
        "            if MOD.metricLossType=='tripletLoss':\n",
        "                r = MOD.response(Im, tripletFlag=True)\n",
        "            else:\n",
        "                r = MOD.response(Im)\n",
        "                \n",
        "            # Compute loss and accuracy\n",
        "            loss, accValTa, accValClCe = MOD.getLossAccuracy('metric',r)\n",
        "\n",
        "            # Store validation loss and accuracy\n",
        "            L_val[0][n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_valTa[0][n]=np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            A_valClCe[0][n]=np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.metOpt.param_groups[0]['lr'] = par.etaInitial * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if ((n+1)%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrTa_mean=np.mean(A_trTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accTrClCe_mean=np.mean(A_trClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            mseVal_mean=np.mean(L_val[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValTa_mean=np.mean(A_valTa[0][(n+1)-par.reportTime:(n+1)])\n",
        "            accValClCe_mean=np.mean(A_valClCe[0][(n+1)-par.reportTime:(n+1)])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n+1)/np.float32(par.nEpisodes)*100.:.3}%   Tr-Loss: {mseTr_mean}, Val-Loss: {mseVal_mean};  Tr-AccTa: {accTrTa_mean}, Val-AccTa: {accValTa_mean}')\n",
        "            print(f'          Tr-AccClCe: {accTrClCe_mean}, Val-AccClCe: {accValClCe_mean}')\n",
        "\n",
        "    ### Save hidden layer representations for transfer learning\n",
        "    H_tr = []\n",
        "    H_val = []\n",
        "    with torch.no_grad():\n",
        "        for j in range(par.nClass):\n",
        "            MOD.response(X_tr[j])\n",
        "            H_tr.append(MOD.x[-1])\n",
        "            MOD.response(X_val[j])\n",
        "            H_val.append(MOD.x[-1])\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trTa, outputDir + '/' + 'accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_trClCe, outputDir + '/' + 'accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valTa, outputDir + '/' + 'accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_valClCe, outputDir + '/' + 'accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD, outputDir + '/' + 'model'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    torch.save([H_tr, H_val], outputDir + '/' + 'hResp'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "id": "3b4c3949",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPtransfer(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        self.lossfn = nn.BCEWithLogitsLoss()\n",
        "        # Setup target vector to compute Loss and Accuracy\n",
        "        self.target = torch.zeros(self.batch_size).long().to(device)\n",
        "        for j in range(1,self.N_class):\n",
        "            self.target[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch] = j\n",
        "\n",
        "        self.N_i = par.nInputs\n",
        "        self.lossLayer = len(par.Ns)-1\n",
        "        self.etaInitial = par.etaInitial\n",
        "        self.etaTransfer = par.etaTransfer\n",
        "        \n",
        "        self.Ns = par.Ns\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        \n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "    def transferOptimiser(self):\n",
        "        \n",
        "        self.tranOpt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':self.etaTransfer }])\n",
        "        \n",
        "    def Forward(self, input):\n",
        "        \n",
        "        self.x[0] = torch.clone(input)\n",
        "        ### Hidden layers\n",
        "        for n in range(1,len(self.Ns)-1): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n] = torch.relu( torch.add(torch.matmul(self.x[n-1],self.Ws[n-1]),self.bs[n-1]) )\n",
        "        \n",
        "        ### Output\n",
        "        self.x[-1] = torch.add(torch.matmul(self.x[-2],self.Ws[-1]),self.bs[-1])\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True).to(device))\n",
        "    \n",
        "    def lossCrossEntropy(self, nSamples):\n",
        "        # Compute softmax probabilities for responses\n",
        "        p = torch.div( torch.exp(self.x[-1]), torch.sum(torch.exp(self.x[-1]), 1, keepdim=True).tile((1,self.x[-1].shape[1])) )\n",
        "        L = torch.mean(- torch.log(p[range(nSamples),self.target]))\n",
        "        return L\n",
        "\n",
        "    def accuracyTarget(self):\n",
        "        acc = torch.mean(torch.eq( torch.argmax(self.x[-1],1), self.target ).type(torch.float))\n",
        "        return acc\n",
        "    \n",
        "    def accuracyClassCentroid(self):\n",
        "        nSamples = self.x[-1].shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device)\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = self.x[-1][j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(torch.clone(self.x[-1][j,:]), [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "    \n",
        "    def getLossAccuracy(self, backwardFlag=False, opt=[]):\n",
        "        # Compute Loss and accuracy\n",
        "        L = self.lossCrossEntropy(self.x[-1].shape[0])\n",
        "        accTa = self.accuracyTarget()\n",
        "        accClCe = self.accuracyClassCentroid()\n",
        "\n",
        "        if backwardFlag:\n",
        "            L.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            \n",
        "        return L,accTa,accClCe\n",
        "    \n",
        "    def response(self, Input):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        \n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t])\n",
        "\n",
        "    # def generateBatch(self, X):\n",
        "       \n",
        "    #     batch = torch.zeros(self.batch_size, X[0].shape[1], X[0].shape[2], dtype=torch.float32, requires_grad=False).to(device)\n",
        "    #     self.target = torch.zeros(self.batch_size, self.nClass, dtype=torch.float32).to(device)\n",
        "    #     # for j in range(self.batch_size):\n",
        "    #     #     ind1 = torch.randint(0,self.nClass,[1])\n",
        "    #     #     ind2 = torch.randint(0,X[ind1].shape[0],[1])\n",
        "    #     #     batch[j,:,:] = torch.clone(X[ind1][ind2,:,:])\n",
        "    #     #     self.target[j,ind1] = 1.\n",
        "\n",
        "    #     for k in range(self.nClass):\n",
        "    #         rand_ind=np.random.randint(0,X[k].shape[0],(self.nSampPerClassPerBatch,))\n",
        "    #         batch[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch,:,:] = torch.clone(X[k][rand_ind,:,:])\n",
        "    #         self.target[k*self.nSampPerClassPerBatch:(k+1)*self.nSampPerClassPerBatch, k] = 1\n",
        "\n",
        "    #     return batch\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.Forward(Input[:, :, t], t)\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "id": "2fd0c2ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################\n",
        "#### Transfer Learning\n",
        "###############################\n",
        "def tran_esn_fb(expName, rngSeed):\n",
        "    \n",
        "    ### Initiliase RNGs\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.default_rng(rngSeed)\n",
        "\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir+'Tran'    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Load inputs\n",
        "    inputs = torch.load(expDir + '/' + 'hResp'+str(rngSeed)+'.pt')\n",
        "    X_tr = inputs[0]\n",
        "    X_val = inputs[1]\n",
        "    print(f'Shape of X_tr is {X_tr[0].shape}')\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nWeightSave)) # Save weights every <> epochs, up to epoch nSaveMaxT\n",
        "    if len(X_tr[0].shape) > 2:\n",
        "        tMax = X_tr[0].shape[2]\n",
        "    else:\n",
        "        tMax = 0\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPtransfer(par)\n",
        "\n",
        "    ### Init lists for saving data\n",
        "    outL_tr = np.zeros([par.nEpisodesTran])\n",
        "    outA_trTa = np.zeros([par.nEpisodesTran])\n",
        "    outA_trClCe = np.zeros([par.nEpisodesTran])\n",
        "    outL_val = np.zeros([par.nEpisodesTran])\n",
        "    outA_valTa = np.zeros([par.nEpisodesTran])\n",
        "    outA_valClCe = np.zeros([par.nEpisodesTran])\n",
        "    outsavDW = []\n",
        "\n",
        "    print(f'**********************START TRANSFER TRAINING')\n",
        "    \n",
        "    # Init memory to save responses\n",
        "    resp_saveind = 0\n",
        "    if par.saveFlag_RESP:\n",
        "        outRESP = []\n",
        "        for layer in range(len(par.Ns)):\n",
        "            if len(X_tr[0].shape) > 2:\n",
        "                outRESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "            else:\n",
        "                outRESP.append(np.zeros((len(par.saveRespAtN)+1, par.nClass * par.nSaveSamples, par.Ns[layer]))) \n",
        "\n",
        "    # Init memory to save dw histograms\n",
        "    dw_saveind = 0\n",
        "    dwEdges = torch.logspace(-10,-3,51)\n",
        "    for k in range(len(par.Ns)-1):\n",
        "        outsavDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histograms for feedforward layers\n",
        "    if par.fbLayer:\n",
        "        outsavDW.append(np.zeros((dwEdges.shape[0]-1, par.nWeightSave))) # Histogram for feedback layer\n",
        "\n",
        "    ### Initialise optimiser\n",
        "    MOD.transferOptimiser()\n",
        "    \n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpisodesTran):\n",
        "        # ### Save output responses before updates\n",
        "        # if par.saveFlag_RESP and (np.any(np.isin(n, par.saveRespAtN)) or n==(par.nEpisodesTran-1)):\n",
        "        #     print(f'Saving response on iteration {n} of {par.nEpisodesTran}')\n",
        "        #     with torch.no_grad():\n",
        "        #         s = X_tr[0][0:par.nSaveSamples,:]\n",
        "        #         for k in range(1,par.nClass):\n",
        "        #             s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "        #         resp = MOD.responseSave(s, range(len(par.Ns)))\n",
        "        #         for li, l in enumerate(range(len(par.Ns))):\n",
        "        #             outRESP[li][resp_saveind,:,:,:] = resp[li].numpy()\n",
        "        #     resp_saveind += 1\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(MOD.nSampPerClassPerBatch,))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "\n",
        "        # Get response to input\n",
        "        MOD.Reset(Im.shape[0])\n",
        "        with torch.no_grad():\n",
        "            MOD.x[0] = torch.clone(Im)\n",
        "        try:\n",
        "            MOD.Forward(Im)\n",
        "        except:\n",
        "            print(f'Size of layer 0 activity: {MOD.x[0].shape}')\n",
        "            for k in range(len(par.Ns)-1):\n",
        "                print(f'Size of layer {k} activity: {MOD.x[k+1].shape}')\n",
        "                print(f'Size of layer {k} weights: {MOD.Ws[k].shape}')\n",
        "        \n",
        "        # Compute training loss and accuracy\n",
        "        loss, accTrTa, accTrClCe = MOD.getLossAccuracy(backwardFlag=True, opt=MOD.tranOpt)\n",
        "        \n",
        "        # print(f'mean gradient is {torch.mean(MOD.Ws[-1].grad)}')\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        # if torch.any(torch.isnan(MOD.x[-1])):\n",
        "        #     print(f'Quitting at batch {n} of {par.nEpisodesTran}')\n",
        "        #     return -1\n",
        "        #####\n",
        "        #####\n",
        "\n",
        "        # Store loss and accuracy\n",
        "        outL_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        outA_trTa[n]=np.copy(np.array(accTrTa.to('cpu').detach()))\n",
        "        outA_trClCe[n]=np.copy(np.array(accTrClCe.to('cpu').detach()))\n",
        "        \n",
        "        # Store weight changes for effective learning rate\n",
        "        with torch.no_grad():\n",
        "            if par.saveFlag_DW and (n%save_every)<1:\n",
        "                # Save before weights\n",
        "                w1 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w1.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w1.append(torch.clone(MOD.W_fb.data))\n",
        "            if par.saveFlag_DW and ((n-1)%save_every)<1:\n",
        "                # Save after weights\n",
        "                w2 = []\n",
        "                for k in range(len(par.Ns)-1):\n",
        "                    w2.append(torch.clone(MOD.Ws[k].data))\n",
        "                if par.fbLayer:\n",
        "                    w2.append(torch.clone(MOD.W_fb.data))\n",
        "                # Compute weight change\n",
        "                for k in range(len(w1)):\n",
        "                    dw = torch.abs(torch.subtract(w2[k],w1[k])).cpu()\n",
        "                    # savDW[k][:,dw_saveind] = torch.divide(torch.histogram(dw[dw>0], dwEdges).hist, torch.numel(dw)).cpu().numpy()\n",
        "                    outsavDW[k][:,dw_saveind] = torch.histogram(dw[dw>0], dwEdges).hist.numpy()\n",
        "                dw_saveind += 1\n",
        "            \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Get response to input\n",
        "            MOD.Reset(Im.shape[0])\n",
        "            MOD.x[0] = Im\n",
        "            MOD.Forward(Im)\n",
        "\n",
        "            # Compute validation loss and accuracy\n",
        "            loss,accValTa,accValClCe = MOD.getLossAccuracy()\n",
        "\n",
        "            # Store loss and accuracy\n",
        "            outL_val[n] = np.copy(np.array(loss.to('cpu').detach()))\n",
        "            outA_valTa[n] = np.copy(np.array(accValTa.to('cpu').detach()))\n",
        "            outA_valClCe[n] = np.copy(np.array(accValClCe.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.tranOpt.param_groups[0]['lr'] = par.etaTransfer * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if ((n+1)%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(outL_tr[(n+1)-par.reportTime:(n+1)])\n",
        "            accTrTa_mean=np.mean(outA_trTa[(n+1)-par.reportTime:(n+1)])\n",
        "            accTrClCe_mean=np.mean(outA_trClCe[(n+1)-par.reportTime:(n+1)])\n",
        "            mseVal_mean=np.mean(outL_val[(n+1)-par.reportTime:(n+1)])\n",
        "            accValTa_mean=np.mean(outA_valTa[(n+1)-par.reportTime:(n+1)])\n",
        "            accValClCe_mean=np.mean(outA_valClCe[(n+1)-par.reportTime:(n+1)])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n+1)/np.float32(par.nEpisodesTran)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr AccTa: {accTrTa_mean}, Mean Val AccTa: {accValTa_mean}')\n",
        "            print(f'Progress:                                                                               Mean Tr AccClCe: {accTrClCe_mean}, Mean Val AccClCe: {accValClCe_mean}')\n",
        "    \n",
        "    wTran = [torch.clone(MOD.Ws[-1]).detach().cpu().numpy(), torch.clone(MOD.bs[-1]).detach().cpu().numpy()]\n",
        "\n",
        "    torch.save(outL_tr, outputDir + '/' + 'out_lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trTa, outputDir + '/' + 'out_accTrTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_trClCe, outputDir + '/' + 'out_accTrClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outL_val, outputDir + '/' + 'out_lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valTa, outputDir + '/' + 'out_accValTa'+str(rngSeed)+'.pt')\n",
        "    torch.save(outA_valClCe, outputDir + '/' + 'out_accValClCe'+str(rngSeed)+'.pt')\n",
        "    torch.save(outRESP, outputDir + '/' + 'out_respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(outsavDW, outputDir + '/' + 'out_dw'+str(rngSeed)+'.pt')\n",
        "    torch.save(wTran, outputDir + '/' + 'wTran'+str(rngSeed)+'.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c1af1725",
      "metadata": {},
      "source": [
        "# Test run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "id": "5e95e7dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 15.099274158477783\n",
            "Progress: 5.0%   Tr-Loss: 0.7348926837444305, Val-Loss: 0.6954882881045341;  Tr-AccTa: 0.0085599996894598, Val-AccTa: 0.007999999672174454\n",
            "          Tr-AccClCe: 0.7719999788999558, Val-AccClCe: 0.7781599793434143\n",
            "Time per stage: 15.01044750213623\n",
            "Progress: 10.0%   Tr-Loss: 0.32043907694518564, Val-Loss: 0.30535681284964084;  Tr-AccTa: 0.0007199999839067459, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9080799782276153, Val-AccClCe: 0.911919976234436\n",
            "Time per stage: 14.921667337417603\n",
            "Progress: 15.0%   Tr-Loss: 0.21937990713119507, Val-Loss: 0.2090386956334114;  Tr-AccTa: 0.010799999698996544, Val-AccTa: 0.009919999733567237\n",
            "          Tr-AccClCe: 0.9374399778842926, Val-AccClCe: 0.9386399757862091\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/its/home/jb739/esn_feedback/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m expName \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# xent_esn_fb(expName, seed)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m metric_esn_fb(expName, seed)\n",
            "\u001b[1;32m/its/home/jb739/esn_feedback/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36mmetric_esn_fb\u001b[0;34m(expName, rngSeed)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     r \u001b[39m=\u001b[39m MOD\u001b[39m.\u001b[39mresponse(Im, tripletFlag\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     r \u001b[39m=\u001b[39m MOD\u001b[39m.\u001b[39;49mresponse(Im)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# Compute loss and accuracy\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m loss, accTrTa, accTrClCe \u001b[39m=\u001b[39m MOD\u001b[39m.\u001b[39mgetLossAccuracy(\u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m,r,backwardFlag\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, opt\u001b[39m=\u001b[39mMOD\u001b[39m.\u001b[39mmetOpt)\n",
            "\u001b[1;32m/its/home/jb739/esn_feedback/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36mMLPmetric.response\u001b[0;34m(self, Input, tripletFlag)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReset(N_samples)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=206'>207</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mForward(Input[:,:,t])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=208'>209</a>\u001b[0m \u001b[39m# Compile list of responses from lossLayer. When using Triplet Loss, \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39m# list is 3x1 for [anchor, positive, negative]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=210'>211</a>\u001b[0m r \u001b[39m=\u001b[39m [] \n",
            "\u001b[1;32m/its/home/jb739/esn_feedback/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36mMLPmetric.Forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m### Hidden layers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNs)): \u001b[39m# For each layer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39m# Build up training data so that outputs learn from all previous layers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[n] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu( torch\u001b[39m.\u001b[39;49madd(torch\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx[n\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWs[n\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]),\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbs[n\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mif\u001b[39;00m n\u001b[39m==\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfbLayer:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn_feedback/esn_feedback.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxFB \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[n])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "swLR = 0.001#0.00046 # Sweep over these learning rates\n",
        "seed = 11117 # No. runs per hyperparameter\n",
        "### Initial training\n",
        "# Update hyperparameter\n",
        "importlib.reload(par)\n",
        "par.etaInitial = swLR\n",
        "par.etaTransfer = 0.001\n",
        "par.fbLayer = 2\n",
        "par.saveRepLayer = 2\n",
        "par.saveFlag_FBWeights = False if not par.fbLayer else True # Save feedback weights\n",
        "par.nEpisodes = 5000\n",
        "par.reportTime = 250\n",
        "# par.Ns = [par.N_esn, 100, par.nClass] # No. neurons in each layer - FOR XENT\n",
        "par.Ns = [par.N_esn, 100, 100] # No. neurons in each layer - FOR METRIC\n",
        "par.nSaveMaxT = par.nEpisodes \n",
        "par.metricLossType = 'prototypicalLoss'\n",
        "expName = 'test'\n",
        "# xent_esn_fb(expName, seed)\n",
        "metric_esn_fb(expName, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "id": "51e8063e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_tr is torch.Size([4915, 100])\n",
            "**********************START TRANSFER TRAINING\n",
            "Time per stage: 3.931821584701538\n",
            "Progress: 25.0%   Mean Tr Er: 0.9297582291960717, Mean Val Er: 0.9282805350422859;   Mean Tr AccTa: 0.7736799834668636, Mean Val AccTa: 0.7699199810475111\n",
            "Progress:                                                                               Mean Tr AccClCe: 0.9787999792098999, Mean Val AccClCe: 0.9720799779891968\n",
            "Time per stage: 3.943636417388916\n",
            "Progress: 50.0%   Mean Tr Er: 0.1694500716626644, Mean Val Er: 0.17569849035143853;   Mean Tr AccTa: 0.9796799805164337, Mean Val AccTa: 0.9738399772644043\n",
            "Progress:                                                                               Mean Tr AccClCe: 0.9813599812984467, Mean Val AccClCe: 0.975519978761673\n",
            "Time per stage: 3.942584276199341\n",
            "Progress: 75.0%   Mean Tr Er: 0.10997967040538788, Mean Val Er: 0.11479077318310738;   Mean Tr AccTa: 0.9800799796581269, Mean Val AccTa: 0.976799978017807\n",
            "Progress:                                                                               Mean Tr AccClCe: 0.9812799799442291, Mean Val AccClCe: 0.9780799803733826\n",
            "Time per stage: 3.942202091217041\n",
            "Progress: 1e+02%   Mean Tr Er: 0.09036052016168833, Mean Val Er: 0.09635147108882666;   Mean Tr AccTa: 0.9805599794387817, Mean Val AccTa: 0.976319979429245\n",
            "Progress:                                                                               Mean Tr AccClCe: 0.9801599805355072, Mean Val AccClCe: 0.978319977760315\n"
          ]
        }
      ],
      "source": [
        "### Test Transfer learning\n",
        "par.nEpisodesTran = 1000\n",
        "par.etaTransfer = 0.001\n",
        "par.Ns = [100,par.nClass]\n",
        "par.nInputs = 100\n",
        "par.fbLayer = []\n",
        "tran_esn_fb(expName, seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa8aa35e",
      "metadata": {},
      "source": [
        "# Run parameter sweep (CrossEntropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c625fc92",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-15/3,-7/3,5) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "modName = 'xenFB1'\n",
        "experiment = 'swLR'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "\n",
        "# complete = torch.zeros((len(swLR), len(seeds)))\n",
        "# for j, lr in enumerate(swLR):\n",
        "#     for k, sd in enumerate(seeds):\n",
        "#         t = time.time()\n",
        "#         # Update hyperparameter\n",
        "#         importlib.reload(par)\n",
        "#         par.saveFlag_FBWeights = True\n",
        "#         par.nSaveMaxT = par.nEpisodes \n",
        "#         par.etaInitial = swLR[j]\n",
        "#         par.fbLayer = 1\n",
        "#         par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "#         expName = expNameSuffix+str(j)\n",
        "#         complete[j, k] = xent_esn_fb(expName, sd)\n",
        "#         print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "#         print(f'*************************FB1 Run time: {time.time()-t}')\n",
        "# completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "# torch.save(complete, completeName)\n",
        "\n",
        "# print('*******************************FINISHED FB1********************************')\n",
        "\n",
        "modName = 'xenFB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.saveFlag_FBWeights = True\n",
        "        par.nSaveMaxT = par.nEpisodes \n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = []\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************FB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB0********************************')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "80762c57",
      "metadata": {},
      "source": [
        "# Run parameter sweep (MetricLearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "id": "30567a8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 14.066001653671265\n",
            "Progress: 5.0%   Tr-Loss: 1.8575203137397767, Val-Loss: 1.8580721697807312;  Tr-AccTa: 0.0007199999839067459, Val-AccTa: 0.0008799999803304672\n",
            "          Tr-AccClCe: 0.5582399841547012, Val-AccClCe: 0.5473599843978881\n",
            "Time per stage: 14.072486639022827\n",
            "Progress: 10.0%   Tr-Loss: 1.3018570010662078, Val-Loss: 1.2801669390201569;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.00047999998927116395\n",
            "          Tr-AccClCe: 0.6122399854660034, Val-AccClCe: 0.6185599871873856\n",
            "Time per stage: 14.114056825637817\n",
            "Progress: 15.0%   Tr-Loss: 0.985913754940033, Val-Loss: 0.9834385516643525;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0005599999874830246\n",
            "          Tr-AccClCe: 0.6995199854373932, Val-AccClCe: 0.6993599827289582\n",
            "Time per stage: 14.069642782211304\n",
            "Progress: 20.0%   Tr-Loss: 0.8431906480789184, Val-Loss: 0.8287174384593964;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0011999999731779098\n",
            "          Tr-AccClCe: 0.7346399846076965, Val-AccClCe: 0.7410399835109711\n",
            "Time per stage: 14.273780822753906\n",
            "Progress: 25.0%   Tr-Loss: 0.7528479642868042, Val-Loss: 0.7175315190553665;  Tr-AccTa: 0.001599999964237213, Val-AccTa: 0.003439999923110008\n",
            "          Tr-AccClCe: 0.7685599792003631, Val-AccClCe: 0.7739199800491333\n",
            "Time per stage: 14.397874355316162\n",
            "Progress: 30.0%   Tr-Loss: 0.6797613981962204, Val-Loss: 0.6364938603639603;  Tr-AccTa: 0.0027199999392032623, Val-AccTa: 0.003519999921321869\n",
            "          Tr-AccClCe: 0.7890399796962738, Val-AccClCe: 0.8073599777221679\n",
            "Time per stage: 14.117019653320312\n",
            "Progress: 35.0%   Tr-Loss: 0.6167992318868637, Val-Loss: 0.5998317750692368;  Tr-AccTa: 0.003039999932050705, Val-AccTa: 0.005519999876618385\n",
            "          Tr-AccClCe: 0.810479975938797, Val-AccClCe: 0.8185599765777588\n",
            "Time per stage: 14.035820960998535\n",
            "Progress: 40.0%   Tr-Loss: 0.5908786946535111, Val-Loss: 0.571571094751358;  Tr-AccTa: 0.005119999885559082, Val-AccTa: 0.005599999874830246\n",
            "          Tr-AccClCe: 0.8179199767112731, Val-AccClCe: 0.8264799749851227\n",
            "Time per stage: 13.99632477760315\n",
            "Progress: 45.0%   Tr-Loss: 0.5709126162528991, Val-Loss: 0.530140998005867;  Tr-AccTa: 0.0038399999141693116, Val-AccTa: 0.004799999892711639\n",
            "          Tr-AccClCe: 0.8246399765014648, Val-AccClCe: 0.84519997382164\n",
            "Time per stage: 14.0550217628479\n",
            "Progress: 50.0%   Tr-Loss: 0.5563243694901466, Val-Loss: 0.5151138392686844;  Tr-AccTa: 0.0036799999177455902, Val-AccTa: 0.004239999905228615\n",
            "          Tr-AccClCe: 0.8295199739933014, Val-AccClCe: 0.84319997549057\n",
            "Time per stage: 14.006035089492798\n",
            "Progress: 55.0%   Tr-Loss: 0.5214523590803146, Val-Loss: 0.4966196285486221;  Tr-AccTa: 0.002159999951720238, Val-AccTa: 0.002559999942779541\n",
            "          Tr-AccClCe: 0.844079974412918, Val-AccClCe: 0.8519999759197235\n",
            "Time per stage: 14.013392210006714\n",
            "Progress: 60.0%   Tr-Loss: 0.4999720684289932, Val-Loss: 0.4764095978140831;  Tr-AccTa: 0.002319999948143959, Val-AccTa: 0.0024799999445676802\n",
            "          Tr-AccClCe: 0.8503199756145478, Val-AccClCe: 0.853599975347519\n",
            "Time per stage: 14.020862102508545\n",
            "Progress: 65.0%   Tr-Loss: 0.4778217532038689, Val-Loss: 0.4494912330508232;  Tr-AccTa: 0.0023999999463558195, Val-AccTa: 0.003039999932050705\n",
            "          Tr-AccClCe: 0.8570399744510651, Val-AccClCe: 0.866639976978302\n",
            "Time per stage: 14.009859323501587\n",
            "Progress: 70.0%   Tr-Loss: 0.46106758135557174, Val-Loss: 0.4489447734951973;  Tr-AccTa: 0.0019199999570846558, Val-AccTa: 0.0029599999338388444\n",
            "          Tr-AccClCe: 0.8599999759197235, Val-AccClCe: 0.8681599767208099\n",
            "Time per stage: 14.004451751708984\n",
            "Progress: 75.0%   Tr-Loss: 0.44752427786588667, Val-Loss: 0.42676314264535903;  Tr-AccTa: 0.0022399999499320986, Val-AccTa: 0.0024799999445676802\n",
            "          Tr-AccClCe: 0.8653599758148194, Val-AccClCe: 0.8716799757480621\n",
            "Time per stage: 14.00867247581482\n",
            "Progress: 80.0%   Tr-Loss: 0.43819657933712003, Val-Loss: 0.4247436504364014;  Tr-AccTa: 0.0015199999660253526, Val-AccTa: 0.002079999953508377\n",
            "          Tr-AccClCe: 0.8695999746322631, Val-AccClCe: 0.8731199750900268\n",
            "Time per stage: 14.003793954849243\n",
            "Progress: 85.0%   Tr-Loss: 0.42658609521389007, Val-Loss: 0.40655721712112425;  Tr-AccTa: 0.0024799999445676802, Val-AccTa: 0.0027199999392032623\n",
            "          Tr-AccClCe: 0.8714399762153625, Val-AccClCe: 0.8776799745559692\n",
            "Time per stage: 14.002735376358032\n",
            "Progress: 90.0%   Tr-Loss: 0.4046905504465103, Val-Loss: 0.3966854351758957;  Tr-AccTa: 0.0015199999660253526, Val-AccTa: 0.003119999930262566\n",
            "          Tr-AccClCe: 0.8774399750232696, Val-AccClCe: 0.8812799746990204\n",
            "Time per stage: 14.0014009475708\n",
            "Progress: 95.0%   Tr-Loss: 0.39057144141197203, Val-Loss: 0.37918523675203325;  Tr-AccTa: 0.002559999942779541, Val-AccTa: 0.0024799999445676802\n",
            "          Tr-AccClCe: 0.8820799760818482, Val-AccClCe: 0.8890399770736694\n",
            "Saving response on iteration 4999 of 5000\n",
            "Time per stage: 14.011677503585815\n",
            "Progress: 1e+02%   Tr-Loss: 0.4035710011124611, Val-Loss: 0.3702480648159981;  Tr-AccTa: 0.0022399999499320986, Val-AccTa: 0.0026399999409914016\n",
            "          Tr-AccClCe: 0.8797599768638611, Val-AccClCe: 0.8915199751853943\n",
            "j=0/5;     k=0/2\n",
            "*************************metFB1 Run time: 281.7349064350128\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 14.032748937606812\n",
            "Progress: 5.0%   Tr-Loss: 1.919588318347931, Val-Loss: 1.9117334971427917;  Tr-AccTa: 0.0661599979698658, Val-AccTa: 0.06991999760270119\n",
            "          Tr-AccClCe: 0.5699199831485748, Val-AccClCe: 0.5755199863910675\n",
            "Time per stage: 14.372291803359985\n",
            "Progress: 10.0%   Tr-Loss: 1.3036418504714966, Val-Loss: 1.2806837878227233;  Tr-AccTa: 0.07103999762237072, Val-AccTa: 0.07367999754846095\n",
            "          Tr-AccClCe: 0.6134399837255478, Val-AccClCe: 0.6311199856996537\n",
            "Time per stage: 16.502187490463257\n",
            "Progress: 15.0%   Tr-Loss: 0.9644039244651794, Val-Loss: 0.9515700762271881;  Tr-AccTa: 0.0816799967288971, Val-AccTa: 0.08159999683499336\n",
            "          Tr-AccClCe: 0.7015199863910675, Val-AccClCe: 0.7016799867153167\n",
            "Time per stage: 14.184751033782959\n",
            "Progress: 20.0%   Tr-Loss: 0.7980259971618653, Val-Loss: 0.8106585054397583;  Tr-AccTa: 0.07487999722361564, Val-AccTa: 0.07479999727010726\n",
            "          Tr-AccClCe: 0.75343998336792, Val-AccClCe: 0.7484799811840057\n",
            "Time per stage: 14.142166376113892\n",
            "Progress: 25.0%   Tr-Loss: 0.7216037881374359, Val-Loss: 0.7068878163099289;  Tr-AccTa: 0.03591999915242195, Val-AccTa: 0.03319999921321869\n",
            "          Tr-AccClCe: 0.7757599802017212, Val-AccClCe: 0.7809599797725677\n",
            "Time per stage: 14.123422145843506\n",
            "Progress: 30.0%   Tr-Loss: 0.6343192569017411, Val-Loss: 0.6411543790102004;  Tr-AccTa: 0.013119999706745147, Val-AccTa: 0.01207999972999096\n",
            "          Tr-AccClCe: 0.803679976940155, Val-AccClCe: 0.8050399777889252\n",
            "Time per stage: 14.540175199508667\n",
            "Progress: 35.0%   Tr-Loss: 0.6030802624225616, Val-Loss: 0.5800287257432938;  Tr-AccTa: 0.005279999881982803, Val-AccTa: 0.0045599998980760574\n",
            "          Tr-AccClCe: 0.8138399777412415, Val-AccClCe: 0.8198399746417999\n",
            "Time per stage: 14.140395164489746\n",
            "Progress: 40.0%   Tr-Loss: 0.5516763209104538, Val-Loss: 0.5342850083112717;  Tr-AccTa: 0.0024799999445676802, Val-AccTa: 0.003119999930262566\n",
            "          Tr-AccClCe: 0.8289599757194519, Val-AccClCe: 0.8345599744319916\n",
            "Time per stage: 14.144481897354126\n",
            "Progress: 45.0%   Tr-Loss: 0.5127800642848015, Val-Loss: 0.5061130788326264;  Tr-AccTa: 0.0019999999552965165, Val-AccTa: 0.0016799999624490737\n",
            "          Tr-AccClCe: 0.8402399742603301, Val-AccClCe: 0.8465599739551544\n",
            "Time per stage: 14.1336088180542\n",
            "Progress: 50.0%   Tr-Loss: 0.4939797681570053, Val-Loss: 0.48106024742126463;  Tr-AccTa: 0.0012799999713897705, Val-AccTa: 0.0010399999767541886\n",
            "          Tr-AccClCe: 0.8492799763679504, Val-AccClCe: 0.8524799754619599\n",
            "Time per stage: 14.301567792892456\n",
            "Progress: 55.0%   Tr-Loss: 0.4755375386476517, Val-Loss: 0.46347303318977356;  Tr-AccTa: 0.0007199999839067459, Val-AccTa: 0.00039999999105930326\n",
            "          Tr-AccClCe: 0.8523999755382538, Val-AccClCe: 0.8585599753856659\n",
            "Time per stage: 14.137333869934082\n",
            "Progress: 60.0%   Tr-Loss: 0.4714807618260384, Val-Loss: 0.45459192180633545;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 0.00039999999105930326\n",
            "          Tr-AccClCe: 0.8559199726581573, Val-AccClCe: 0.8650399756431579\n",
            "Time per stage: 14.127120733261108\n",
            "Progress: 65.0%   Tr-Loss: 0.440604538500309, Val-Loss: 0.44543452537059786;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 7.999999821186065e-05\n",
            "          Tr-AccClCe: 0.8681599745750427, Val-AccClCe: 0.866799973487854\n",
            "Time per stage: 14.145722150802612\n",
            "Progress: 70.0%   Tr-Loss: 0.41542104053497314, Val-Loss: 0.4206523374915123;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.872639976978302, Val-AccClCe: 0.8738399746418\n",
            "Time per stage: 14.36967158317566\n",
            "Progress: 75.0%   Tr-Loss: 0.41342686438560483, Val-Loss: 0.4008246878385544;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.8763999760150909, Val-AccClCe: 0.879119975566864\n",
            "Time per stage: 14.123149156570435\n",
            "Progress: 80.0%   Tr-Loss: 0.4022298393845558, Val-Loss: 0.3793619313836098;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.8795999765396119, Val-AccClCe: 0.8849599776268006\n",
            "Time per stage: 14.134464025497437\n",
            "Progress: 85.0%   Tr-Loss: 0.38986573535203933, Val-Loss: 0.35689942407608033;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.8823999752998352, Val-AccClCe: 0.8966399760246276\n",
            "Time per stage: 14.122133493423462\n",
            "Progress: 90.0%   Tr-Loss: 0.3700140063166618, Val-Loss: 0.36271695321798325;  Tr-AccTa: 0.0, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.8880799777507782, Val-AccClCe: 0.8943199758529663\n",
            "Time per stage: 14.130810737609863\n",
            "Progress: 95.0%   Tr-Loss: 0.3784050885140896, Val-Loss: 0.3561934460401535;  Tr-AccTa: 0.0, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.8835199739933014, Val-AccClCe: 0.8927999758720397\n",
            "Saving response on iteration 4999 of 5000\n",
            "Time per stage: 14.16190791130066\n",
            "Progress: 1e+02%   Tr-Loss: 0.3798448737859726, Val-Loss: 0.3412583655118942;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.8873599753379822, Val-AccClCe: 0.8968799760341645\n",
            "j=0/5;     k=1/2\n",
            "*************************metFB1 Run time: 286.5165286064148\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 14.476731061935425\n",
            "Progress: 5.0%   Tr-Loss: 1.2908555612564088, Val-Loss: 1.2767713041305542;  Tr-AccTa: 0.012959999710321427, Val-AccTa: 0.013599999696016312\n",
            "          Tr-AccClCe: 0.6150399852991104, Val-AccClCe: 0.6233599855899811\n",
            "Time per stage: 14.933385372161865\n",
            "Progress: 10.0%   Tr-Loss: 0.624535207092762, Val-Loss: 0.6070388927459717;  Tr-AccTa: 0.008079999819397927, Val-AccTa: 0.00607999986410141\n",
            "          Tr-AccClCe: 0.8011199753284455, Val-AccClCe: 0.8100799779891967\n",
            "Time per stage: 14.463674306869507\n",
            "Progress: 15.0%   Tr-Loss: 0.45111235171556474, Val-Loss: 0.4384765984416008;  Tr-AccTa: 0.001599999964237213, Val-AccTa: 0.0015199999660253526\n",
            "          Tr-AccClCe: 0.8687999753952026, Val-AccClCe: 0.8681599767208099\n",
            "Time per stage: 14.279870986938477\n",
            "Progress: 20.0%   Tr-Loss: 0.3767139525413513, Val-Loss: 0.36831688332557677;  Tr-AccTa: 0.0012799999713897705, Val-AccTa: 0.0007199999839067459\n",
            "          Tr-AccClCe: 0.887359976530075, Val-AccClCe: 0.8913599767684937\n",
            "Time per stage: 14.374200105667114\n",
            "Progress: 25.0%   Tr-Loss: 0.3320724812448025, Val-Loss: 0.3054614259302616;  Tr-AccTa: 0.0012799999713897705, Val-AccTa: 0.00039999999105930326\n",
            "          Tr-AccClCe: 0.9057599768638611, Val-AccClCe: 0.9095999784469605\n",
            "Time per stage: 14.702558279037476\n",
            "Progress: 30.0%   Tr-Loss: 0.2946210685968399, Val-Loss: 0.2796516514867544;  Tr-AccTa: 0.0005599999874830246, Val-AccTa: 0.00023999999463558198\n",
            "          Tr-AccClCe: 0.9115199768543243, Val-AccClCe: 0.9143999760150909\n",
            "Time per stage: 14.272205591201782\n",
            "Progress: 35.0%   Tr-Loss: 0.26772688356041907, Val-Loss: 0.2581038093566895;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9209599771499634, Val-AccClCe: 0.9216799759864807\n",
            "Time per stage: 14.999744176864624\n",
            "Progress: 40.0%   Tr-Loss: 0.2564210366010666, Val-Loss: 0.23876127967238425;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9249599776268005, Val-AccClCe: 0.9300799779891967\n",
            "Time per stage: 14.87003755569458\n",
            "Progress: 45.0%   Tr-Loss: 0.2176158117800951, Val-Loss: 0.22883222170174122;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.9371199767589569, Val-AccClCe: 0.9320799777507782\n",
            "Time per stage: 14.37804126739502\n",
            "Progress: 50.0%   Tr-Loss: 0.21727085444331168, Val-Loss: 0.2157442493736744;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9370399785041809, Val-AccClCe: 0.9367199780941009\n",
            "Time per stage: 14.321599960327148\n",
            "Progress: 55.0%   Tr-Loss: 0.1976064156591892, Val-Loss: 0.20043306517601014;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9435199782848358, Val-AccClCe: 0.9442399768829346\n",
            "Time per stage: 14.929420948028564\n",
            "Progress: 60.0%   Tr-Loss: 0.19190233462303877, Val-Loss: 0.2012609605267644;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 7.999999821186065e-05\n",
            "          Tr-AccClCe: 0.9440799767971039, Val-AccClCe: 0.9435199787616729\n",
            "Time per stage: 14.22104525566101\n",
            "Progress: 65.0%   Tr-Loss: 0.18227452193200588, Val-Loss: 0.17916332495212556;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.00023999999463558198\n",
            "          Tr-AccClCe: 0.9480799779891967, Val-AccClCe: 0.9488799772262573\n",
            "Time per stage: 14.186967611312866\n",
            "Progress: 70.0%   Tr-Loss: 0.1651372753828764, Val-Loss: 0.17301555390655995;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.00023999999463558198\n",
            "          Tr-AccClCe: 0.9511999771595001, Val-AccClCe: 0.9497599763870239\n",
            "Time per stage: 14.132009029388428\n",
            "Progress: 75.0%   Tr-Loss: 0.167924194149673, Val-Loss: 0.1746005329489708;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.00039999999105930326\n",
            "          Tr-AccClCe: 0.9524799776077271, Val-AccClCe: 0.949359977722168\n",
            "Time per stage: 14.139554500579834\n",
            "Progress: 80.0%   Tr-Loss: 0.16046891723945736, Val-Loss: 0.17219797295704484;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9532799792289733, Val-AccClCe: 0.9489599769115448\n",
            "Time per stage: 14.16098928451538\n",
            "Progress: 85.0%   Tr-Loss: 0.1615979373306036, Val-Loss: 0.1660568925179541;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9514399774074555, Val-AccClCe: 0.9509599785804749\n",
            "Time per stage: 14.168375730514526\n",
            "Progress: 90.0%   Tr-Loss: 0.15823191526532174, Val-Loss: 0.157376075476408;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9525599792003632, Val-AccClCe: 0.9556799778938293\n",
            "Time per stage: 14.113382577896118\n",
            "Progress: 95.0%   Tr-Loss: 0.1477894245609641, Val-Loss: 0.14893089857697486;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 7.999999821186065e-05\n",
            "          Tr-AccClCe: 0.9559999778270721, Val-AccClCe: 0.957599978685379\n",
            "Saving response on iteration 4999 of 5000\n",
            "Time per stage: 14.123549699783325\n",
            "Progress: 1e+02%   Tr-Loss: 0.13509219733253122, Val-Loss: 0.1364808082357049;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.9602399797439575, Val-AccClCe: 0.9635199782848358\n",
            "j=1/5;     k=0/2\n",
            "*************************metFB1 Run time: 288.6937975883484\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 14.40775728225708\n",
            "Progress: 5.0%   Tr-Loss: 1.1627202249765396, Val-Loss: 1.1537895195484162;  Tr-AccTa: 0.018959999576210976, Val-AccTa: 0.020799999535083772\n",
            "          Tr-AccClCe: 0.6887199841737747, Val-AccClCe: 0.685199984908104\n",
            "Time per stage: 15.689396381378174\n",
            "Progress: 10.0%   Tr-Loss: 0.5951348767280579, Val-Loss: 0.5687446953058243;  Tr-AccTa: 0.009839999780058862, Val-AccTa: 0.008399999812245368\n",
            "          Tr-AccClCe: 0.8174399764537811, Val-AccClCe: 0.8287199745178223\n",
            "Time per stage: 14.249891519546509\n",
            "Progress: 15.0%   Tr-Loss: 0.4939669474363327, Val-Loss: 0.45003289663791657;  Tr-AccTa: 0.0049599998891353605, Val-AccTa: 0.005359999880194664\n",
            "          Tr-AccClCe: 0.8499999735355377, Val-AccClCe: 0.8652799749374389\n",
            "Time per stage: 14.192592859268188\n",
            "Progress: 20.0%   Tr-Loss: 0.41327159231901167, Val-Loss: 0.3959072563648224;  Tr-AccTa: 0.005039999887347222, Val-AccTa: 0.005599999874830246\n",
            "          Tr-AccClCe: 0.8761599760055542, Val-AccClCe: 0.8782399756908417\n",
            "Time per stage: 14.442476987838745\n",
            "Progress: 25.0%   Tr-Loss: 0.37946620732545855, Val-Loss: 0.3406442821323872;  Tr-AccTa: 0.004159999907016754, Val-AccTa: 0.0048799998909235\n",
            "          Tr-AccClCe: 0.8899999747276306, Val-AccClCe: 0.8997599756717682\n",
            "Time per stage: 17.120206832885742\n",
            "Progress: 30.0%   Tr-Loss: 0.32760915207862856, Val-Loss: 0.3085142474174499;  Tr-AccTa: 0.003439999923110008, Val-AccTa: 0.0038399999141693116\n",
            "          Tr-AccClCe: 0.9044799768924713, Val-AccClCe: 0.9115999791622161\n",
            "Time per stage: 16.31927752494812\n",
            "Progress: 35.0%   Tr-Loss: 0.29042421489953996, Val-Loss: 0.2733520629107952;  Tr-AccTa: 0.0026399999409914016, Val-AccTa: 0.0036799999177455902\n",
            "          Tr-AccClCe: 0.915039977312088, Val-AccClCe: 0.9195199756622314\n",
            "Time per stage: 14.18843388557434\n",
            "Progress: 40.0%   Tr-Loss: 0.2762601999938488, Val-Loss: 0.24962894745171071;  Tr-AccTa: 0.004319999903440476, Val-AccTa: 0.0036799999177455902\n",
            "          Tr-AccClCe: 0.9208799772262574, Val-AccClCe: 0.9279999759197235\n",
            "Time per stage: 15.456796884536743\n",
            "Progress: 45.0%   Tr-Loss: 0.262246763497591, Val-Loss: 0.24504573902487756;  Tr-AccTa: 0.004639999896287918, Val-AccTa: 0.0038399999141693116\n",
            "          Tr-AccClCe: 0.9219999761581421, Val-AccClCe: 0.927999977350235\n",
            "Time per stage: 16.05005383491516\n",
            "Progress: 50.0%   Tr-Loss: 0.239420728251338, Val-Loss: 0.23004070131480694;  Tr-AccTa: 0.005439999878406525, Val-AccTa: 0.005519999876618385\n",
            "          Tr-AccClCe: 0.9278399786949157, Val-AccClCe: 0.9333599781990052\n",
            "Time per stage: 16.176201343536377\n",
            "Progress: 55.0%   Tr-Loss: 0.22951286439597607, Val-Loss: 0.22445946553349494;  Tr-AccTa: 0.005199999883770943, Val-AccTa: 0.006319999858736992\n",
            "          Tr-AccClCe: 0.9331999754905701, Val-AccClCe: 0.9345599753856659\n",
            "Time per stage: 14.596003293991089\n",
            "Progress: 60.0%   Tr-Loss: 0.21543597456812857, Val-Loss: 0.21031450454890727;  Tr-AccTa: 0.005359999880194664, Val-AccTa: 0.0061599998623132705\n",
            "          Tr-AccClCe: 0.9361599769592285, Val-AccClCe: 0.9411199789047241\n",
            "Time per stage: 16.39911985397339\n",
            "Progress: 65.0%   Tr-Loss: 0.20061607745289803, Val-Loss: 0.18890215809643268;  Tr-AccTa: 0.004479999899864197, Val-AccTa: 0.004479999899864197\n",
            "          Tr-AccClCe: 0.9411999757289886, Val-AccClCe: 0.9458399765491485\n",
            "Time per stage: 16.029242753982544\n",
            "Progress: 70.0%   Tr-Loss: 0.19818609069287776, Val-Loss: 0.17915321899205447;  Tr-AccTa: 0.005599999874830246, Val-AccTa: 0.0061599998623132705\n",
            "          Tr-AccClCe: 0.9409599771499634, Val-AccClCe: 0.9477599782943725\n",
            "Time per stage: 14.490526676177979\n",
            "Progress: 75.0%   Tr-Loss: 0.18682439424097538, Val-Loss: 0.18367711379379034;  Tr-AccTa: 0.006639999851584435, Val-AccTa: 0.006799999848008156\n",
            "          Tr-AccClCe: 0.9455999782085419, Val-AccClCe: 0.9476799774169922\n",
            "Time per stage: 14.33052945137024\n",
            "Progress: 80.0%   Tr-Loss: 0.16877555456012489, Val-Loss: 0.18436104392260314;  Tr-AccTa: 0.004079999908804893, Val-AccTa: 0.005599999874830246\n",
            "          Tr-AccClCe: 0.9493599784374237, Val-AccClCe: 0.9458399775028229\n",
            "Time per stage: 14.454751253128052\n",
            "Progress: 85.0%   Tr-Loss: 0.17704454458504915, Val-Loss: 0.17190356715768576;  Tr-AccTa: 0.005279999881982803, Val-AccTa: 0.0061599998623132705\n",
            "          Tr-AccClCe: 0.9468799777030945, Val-AccClCe: 0.9491199774742126\n",
            "Time per stage: 14.59141230583191\n",
            "Progress: 90.0%   Tr-Loss: 0.1765684163197875, Val-Loss: 0.1543474208563566;  Tr-AccTa: 0.006879999846220016, Val-AccTa: 0.007039999842643738\n",
            "          Tr-AccClCe: 0.9481599769592285, Val-AccClCe: 0.9540799787044525\n",
            "Time per stage: 14.42823839187622\n",
            "Progress: 95.0%   Tr-Loss: 0.17084645723178982, Val-Loss: 0.16585035564750433;  Tr-AccTa: 0.005599999874830246, Val-AccTa: 0.005279999881982803\n",
            "          Tr-AccClCe: 0.9518399748802185, Val-AccClCe: 0.9521599771976471\n",
            "Saving response on iteration 4999 of 5000\n",
            "Time per stage: 14.439489841461182\n",
            "Progress: 1e+02%   Tr-Loss: 0.15317472165077925, Val-Loss: 0.160201326161623;  Tr-AccTa: 0.0049599998891353605, Val-AccTa: 0.004719999894499779\n",
            "          Tr-AccClCe: 0.95583997797966, Val-AccClCe: 0.9549599788188934\n",
            "j=1/5;     k=1/2\n",
            "*************************metFB1 Run time: 302.49781465530396\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 15.3477201461792\n",
            "Progress: 5.0%   Tr-Loss: 0.7131137353181839, Val-Loss: 0.7003391407728196;  Tr-AccTa: 0.003119999930262566, Val-AccTa: 0.0028799999356269837\n",
            "          Tr-AccClCe: 0.7955999784469604, Val-AccClCe: 0.7975999774932861\n",
            "Time per stage: 14.443535566329956\n",
            "Progress: 10.0%   Tr-Loss: 0.3514043425321579, Val-Loss: 0.34191161063313485;  Tr-AccTa: 0.0008799999803304672, Val-AccTa: 0.0010399999767541886\n",
            "          Tr-AccClCe: 0.8980799753665925, Val-AccClCe: 0.8942399754524231\n",
            "Time per stage: 14.338602304458618\n",
            "Progress: 15.0%   Tr-Loss: 0.2582841991633177, Val-Loss: 0.24861469808220862;  Tr-AccTa: 0.00039999999105930326, Val-AccTa: 0.0006399999856948852\n",
            "          Tr-AccClCe: 0.9241599786281586, Val-AccClCe: 0.926399977684021\n",
            "Time per stage: 14.318327903747559\n",
            "Progress: 20.0%   Tr-Loss: 0.21570246274769306, Val-Loss: 0.2147112527489662;  Tr-AccTa: 0.0005599999874830246, Val-AccTa: 0.0011999999731779098\n",
            "          Tr-AccClCe: 0.9362399785518646, Val-AccClCe: 0.9371199791431427\n",
            "Time per stage: 14.750547409057617\n",
            "Progress: 25.0%   Tr-Loss: 0.1818380782008171, Val-Loss: 0.1777534888833761;  Tr-AccTa: 0.0007199999839067459, Val-AccTa: 0.0007199999839067459\n",
            "          Tr-AccClCe: 0.9448799781799316, Val-AccClCe: 0.9480799779891967\n",
            "Time per stage: 14.769653558731079\n",
            "Progress: 30.0%   Tr-Loss: 0.16381710352748632, Val-Loss: 0.15885537783801557;  Tr-AccTa: 0.0010399999767541886, Val-AccTa: 0.0005599999874830246\n",
            "          Tr-AccClCe: 0.9515999782085419, Val-AccClCe: 0.9536799776554108\n",
            "Time per stage: 15.025335788726807\n",
            "Progress: 35.0%   Tr-Loss: 0.15047337255626916, Val-Loss: 0.1568006314896047;  Tr-AccTa: 0.0007999999821186065, Val-AccTa: 0.0007199999839067459\n",
            "          Tr-AccClCe: 0.9547999782562255, Val-AccClCe: 0.9543199789524078\n",
            "Time per stage: 14.706276893615723\n",
            "Progress: 40.0%   Tr-Loss: 0.13766505844891072, Val-Loss: 0.14543915378302336;  Tr-AccTa: 0.0005599999874830246, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9587999784946442, Val-AccClCe: 0.959039977312088\n",
            "Time per stage: 14.711987257003784\n",
            "Progress: 45.0%   Tr-Loss: 0.12557327073439956, Val-Loss: 0.13270026381313801;  Tr-AccTa: 0.0015199999660253526, Val-AccTa: 0.0013599999696016312\n",
            "          Tr-AccClCe: 0.9637599771022797, Val-AccClCe: 0.9617599771022797\n",
            "Time per stage: 14.673386096954346\n",
            "Progress: 50.0%   Tr-Loss: 0.13451015461981297, Val-Loss: 0.12999882364086807;  Tr-AccTa: 0.0010399999767541886, Val-AccTa: 0.0009599999785423279\n",
            "          Tr-AccClCe: 0.9584799764156342, Val-AccClCe: 0.9641599769592285\n",
            "Time per stage: 14.751173257827759\n",
            "Progress: 55.0%   Tr-Loss: 0.1157143964394927, Val-Loss: 0.1128822718784213;  Tr-AccTa: 0.0009599999785423279, Val-AccTa: 0.00047999998927116395\n",
            "          Tr-AccClCe: 0.9648799772262573, Val-AccClCe: 0.9667999770641327\n",
            "Time per stage: 14.36932897567749\n",
            "Progress: 60.0%   Tr-Loss: 0.11208859925344586, Val-Loss: 0.11287564155086875;  Tr-AccTa: 0.0012799999713897705, Val-AccTa: 0.0009599999785423279\n",
            "          Tr-AccClCe: 0.9663999762535095, Val-AccClCe: 0.9680799787044525\n",
            "Time per stage: 14.312385082244873\n",
            "Progress: 65.0%   Tr-Loss: 0.10451173510402441, Val-Loss: 0.11665857691317796;  Tr-AccTa: 0.0019199999570846558, Val-AccTa: 0.0011199999749660493\n",
            "          Tr-AccClCe: 0.9683999774456025, Val-AccClCe: 0.9667999775409698\n",
            "Time per stage: 14.13414740562439\n",
            "Progress: 70.0%   Tr-Loss: 0.09813031888380647, Val-Loss: 0.11100978135690093;  Tr-AccTa: 0.0015199999660253526, Val-AccTa: 0.0019199999570846558\n",
            "          Tr-AccClCe: 0.9700799787044525, Val-AccClCe: 0.9680799775123596\n",
            "Time per stage: 14.327165126800537\n",
            "Progress: 75.0%   Tr-Loss: 0.09734244258701802, Val-Loss: 0.10980875512212515;  Tr-AccTa: 0.003039999932050705, Val-AccTa: 0.0028799999356269837\n",
            "          Tr-AccClCe: 0.9707999782562255, Val-AccClCe: 0.968559977054596\n",
            "Time per stage: 14.361419200897217\n",
            "Progress: 80.0%   Tr-Loss: 0.08999117185361684, Val-Loss: 0.11308876466844231;  Tr-AccTa: 0.0013599999696016312, Val-AccTa: 0.001599999964237213\n",
            "          Tr-AccClCe: 0.9746399812698364, Val-AccClCe: 0.9686399776935577\n",
            "Time per stage: 14.288590908050537\n",
            "Progress: 85.0%   Tr-Loss: 0.07665770639851689, Val-Loss: 0.09958832210674883;  Tr-AccTa: 0.0006399999856948852, Val-AccTa: 0.00039999999105930326\n",
            "          Tr-AccClCe: 0.975519978761673, Val-AccClCe: 0.9717599794864654\n",
            "Time per stage: 14.329910278320312\n",
            "Progress: 90.0%   Tr-Loss: 0.08310843069758266, Val-Loss: 0.10182552734669298;  Tr-AccTa: 0.0019199999570846558, Val-AccTa: 0.001599999964237213\n",
            "          Tr-AccClCe: 0.9752799799442291, Val-AccClCe: 0.9719199759960174\n",
            "Time per stage: 14.328669786453247\n",
            "Progress: 95.0%   Tr-Loss: 0.07428098743222654, Val-Loss: 0.09739785119425505;  Tr-AccTa: 0.0023999999463558195, Val-AccTa: 0.0024799999445676802\n",
            "          Tr-AccClCe: 0.9774399793148041, Val-AccClCe: 0.9722399761676789\n",
            "Saving response on iteration 4999 of 5000\n",
            "Time per stage: 14.273589134216309\n",
            "Progress: 1e+02%   Tr-Loss: 0.07332822549901903, Val-Loss: 0.10117086242605;  Tr-AccTa: 0.0022399999499320986, Val-AccTa: 0.0029599999338388444\n",
            "          Tr-AccClCe: 0.9772799801826477, Val-AccClCe: 0.9707999784946442\n",
            "j=2/5;     k=0/2\n",
            "*************************metFB1 Run time: 291.015841960907\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 14.205159664154053\n",
            "Progress: 5.0%   Tr-Loss: 0.7240850742459297, Val-Loss: 0.7073185271024705;  Tr-AccTa: 0.0033599999248981475, Val-AccTa: 0.003439999923110008\n",
            "          Tr-AccClCe: 0.7910399779081344, Val-AccClCe: 0.7911999782323837\n",
            "Time per stage: 14.178666830062866\n",
            "Progress: 10.0%   Tr-Loss: 0.34796549087762835, Val-Loss: 0.3218203973770142;  Tr-AccTa: 0.0015199999660253526, Val-AccTa: 0.0012799999713897705\n",
            "          Tr-AccClCe: 0.8969599757194519, Val-AccClCe: 0.903999975681305\n",
            "Time per stage: 14.18131685256958\n",
            "Progress: 15.0%   Tr-Loss: 0.2774902639389038, Val-Loss: 0.25228177186846734;  Tr-AccTa: 0.0019999999552965165, Val-AccTa: 0.0018399999588727951\n",
            "          Tr-AccClCe: 0.9183199772834778, Val-AccClCe: 0.9283199753761292\n",
            "Time per stage: 14.3328857421875\n",
            "Progress: 20.0%   Tr-Loss: 0.21507266749441623, Val-Loss: 0.2062726326882839;  Tr-AccTa: 0.0013599999696016312, Val-AccTa: 0.0019999999552965165\n",
            "          Tr-AccClCe: 0.9378399770259858, Val-AccClCe: 0.941599980354309\n",
            "Time per stage: 14.253515243530273\n",
            "Progress: 25.0%   Tr-Loss: 0.19457957173883916, Val-Loss: 0.16927884484827518;  Tr-AccTa: 0.0007999999821186065, Val-AccTa: 0.0007199999839067459\n",
            "          Tr-AccClCe: 0.9421599774360657, Val-AccClCe: 0.95183997797966\n",
            "Time per stage: 14.240457773208618\n",
            "Progress: 30.0%   Tr-Loss: 0.17829897338151932, Val-Loss: 0.17602918004244567;  Tr-AccTa: 0.0005599999874830246, Val-AccTa: 0.0009599999785423279\n",
            "          Tr-AccClCe: 0.9490399763584138, Val-AccClCe: 0.9479199771881104\n",
            "Time per stage: 14.20449447631836\n",
            "Progress: 35.0%   Tr-Loss: 0.15415826599672436, Val-Loss: 0.1511132667660713;  Tr-AccTa: 0.0006399999856948852, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.9531199777126312, Val-AccClCe: 0.9575199785232544\n",
            "Time per stage: 14.5342378616333\n",
            "Progress: 40.0%   Tr-Loss: 0.13557208281010388, Val-Loss: 0.12558509695529937;  Tr-AccTa: 0.0008799999803304672, Val-AccTa: 0.0003199999928474426\n",
            "          Tr-AccClCe: 0.959999977350235, Val-AccClCe: 0.9645599772930146\n",
            "Time per stage: 14.312803506851196\n",
            "Progress: 45.0%   Tr-Loss: 0.12558948265761136, Val-Loss: 0.11898069897666573;  Tr-AccTa: 0.0006399999856948852, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.9634399771690368, Val-AccClCe: 0.9640799775123596\n",
            "Time per stage: 14.245867252349854\n",
            "Progress: 50.0%   Tr-Loss: 0.11585394874960184, Val-Loss: 0.11855430866032839;  Tr-AccTa: 0.0003199999928474426, Val-AccTa: 7.999999821186065e-05\n",
            "          Tr-AccClCe: 0.9664799768924713, Val-AccClCe: 0.9661599781513214\n",
            "Time per stage: 14.305298089981079\n",
            "Progress: 55.0%   Tr-Loss: 0.10252377565391362, Val-Loss: 0.11711896962299943;  Tr-AccTa: 0.00039999999105930326, Val-AccTa: 0.00023999999463558198\n",
            "          Tr-AccClCe: 0.9695199780464172, Val-AccClCe: 0.9644799792766571\n",
            "Time per stage: 14.342660665512085\n",
            "Progress: 60.0%   Tr-Loss: 0.09658365628682077, Val-Loss: 0.1098538110665977;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 7.999999821186065e-05\n",
            "          Tr-AccClCe: 0.9717599763870239, Val-AccClCe: 0.9681599776744843\n",
            "Time per stage: 14.346740961074829\n",
            "Progress: 65.0%   Tr-Loss: 0.09094850819930435, Val-Loss: 0.11317112673446536;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.9727199792861938, Val-AccClCe: 0.9696799786090851\n",
            "Time per stage: 14.22464895248413\n",
            "Progress: 70.0%   Tr-Loss: 0.09921645620837807, Val-Loss: 0.1134124162402004;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9706399784088134, Val-AccClCe: 0.967199976682663\n",
            "Time per stage: 14.250032663345337\n",
            "Progress: 75.0%   Tr-Loss: 0.09193420907668769, Val-Loss: 0.1034547636397183;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.9725599768161773, Val-AccClCe: 0.9680799775123596\n",
            "Time per stage: 14.31236720085144\n",
            "Progress: 80.0%   Tr-Loss: 0.0864513699542731, Val-Loss: 0.10518559883348644;  Tr-AccTa: 7.999999821186065e-05, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9740799784660339, Val-AccClCe: 0.9684799790382386\n",
            "Time per stage: 14.244990110397339\n",
            "Progress: 85.0%   Tr-Loss: 0.08433322761207819, Val-Loss: 0.09658072841912509;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9752799775600434, Val-AccClCe: 0.97183997797966\n",
            "Time per stage: 14.197803735733032\n",
            "Progress: 90.0%   Tr-Loss: 0.07298344949632883, Val-Loss: 0.09316360400803388;  Tr-AccTa: 0.0, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9779999792575836, Val-AccClCe: 0.9713599767684936\n",
            "Time per stage: 14.266381740570068\n",
            "Progress: 95.0%   Tr-Loss: 0.08073892344348133, Val-Loss: 0.09176531351078301;  Tr-AccTa: 0.0, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9751999790668487, Val-AccClCe: 0.9723199784755707\n",
            "Saving response on iteration 4999 of 5000\n",
            "Time per stage: 14.353277921676636\n",
            "Progress: 1e+02%   Tr-Loss: 0.07650744055770338, Val-Loss: 0.10068560512550175;  Tr-AccTa: 0.0, Val-AccTa: 0.0\n",
            "          Tr-AccClCe: 0.9767199776172638, Val-AccClCe: 0.9711199774742126\n",
            "j=2/5;     k=1/2\n",
            "*************************metFB1 Run time: 285.9801375865936\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Time per stage: 14.640200853347778\n",
            "Progress: 5.0%   Tr-Loss: 0.46394745165109635, Val-Loss: 0.4528523443341255;  Tr-AccTa: 0.00023999999463558198, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.8643199763298035, Val-AccClCe: 0.8636799774169922\n",
            "Time per stage: 14.678142547607422\n",
            "Progress: 10.0%   Tr-Loss: 0.24659344437718392, Val-Loss: 0.24044415986537934;  Tr-AccTa: 0.0, Val-AccTa: 7.999999821186065e-05\n",
            "          Tr-AccClCe: 0.9297599771022796, Val-AccClCe: 0.9310399770736695\n",
            "Time per stage: 14.504630327224731\n",
            "Progress: 15.0%   Tr-Loss: 0.18633313009142877, Val-Loss: 0.18896847668290137;  Tr-AccTa: 0.0001599999964237213, Val-AccTa: 0.0001599999964237213\n",
            "          Tr-AccClCe: 0.9447199778556824, Val-AccClCe: 0.9472799787521362\n"
          ]
        }
      ],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-15/3,-7/3,5) # Sweep over these learning rates\n",
        "nSeeds = 2 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "experiment = 'swLR'\n",
        "losstype = 'prototypicalLoss'\n",
        "if losstype=='tripletLoss':\n",
        "    lossSuffix = 'tri'\n",
        "elif losstype=='prototypicalLoss':\n",
        "    lossSuffix = 'pro'\n",
        "\n",
        "modName = lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [par.N_esn, 100]\n",
        "        par.metricLossType = 'prototypicalLoss'\n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = 1\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************metFB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB1********************************')\n",
        "\n",
        "modName = lossSuffix+'FB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [par.N_esn, 100]\n",
        "        par.metricLossType = 'prototypicalLoss'\n",
        "        par.etaInitial = swLR[j]\n",
        "        par.fbLayer = []\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************metFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "print('*******************************FINISHED FB0********************************')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0550739c",
      "metadata": {},
      "source": [
        "# Run transfer learning, using learned representations as inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7da07e",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Transfer learning\n",
        "nSeeds = 2 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "experiment = 'swLR'\n",
        "losstype = 'xentropy'\n",
        "if losstype=='tripletLoss':\n",
        "    lossSuffix = 'tri'\n",
        "elif losstype=='prototypicalLoss':\n",
        "    lossSuffix = 'pro'\n",
        "elif losstype=='xentropy':\n",
        "    lossSuffix = 'xen'\n",
        "\n",
        "###\n",
        "### Learned representations using CROSS-ENTROPY Loss with FB1\n",
        "###\n",
        "modName = 'tran_'+lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameters\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [100, par.nClass]\n",
        "        par.nInputs = 100\n",
        "        par.fbLayer = []\n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_xenFB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "###\n",
        "### Learned representations using CROSS-ENTROPY Loss with FB0\n",
        "###\n",
        "modName = 'tran_'+lossSuffix+'FB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameters\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [100, par.nClass]\n",
        "        par.nInputs = 100\n",
        "        par.fbLayer = []\n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_xenFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "losstype = 'prototypicalLoss'\n",
        "if losstype=='tripletLoss':\n",
        "    lossSuffix = 'tri'\n",
        "elif losstype=='prototypicalLoss':\n",
        "    lossSuffix = 'pro'\n",
        "elif losstype=='xentropy':\n",
        "    lossSuffix = 'xen'\n",
        "\n",
        "###\n",
        "### Learned representations using PROTOTYPICAL Loss with FB1\n",
        "###\n",
        "modName = 'tran_'+lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameters\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [100, par.nClass]\n",
        "        par.nInputs = 100\n",
        "        par.fbLayer = []\n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_proFB1 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "###\n",
        "### Learned representations using PROTOTYPICAL Loss with FB0\n",
        "###\n",
        "modName = 'tran_'+lossSuffix+'FB0'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Update hyperparameters\n",
        "        importlib.reload(par)\n",
        "        par.Ns = [100, par.nClass]\n",
        "        par.nInputs = 100\n",
        "        par.fbLayer = []\n",
        "        expName = expNameSuffix+str(j)\n",
        "        complete[j, k] = tran_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************tran_proFB0 Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/'+expNameSuffix+'0/complete.pt'\n",
        "torch.save(complete, completeName)\n",
        "\n",
        "\n",
        "modName = lossSuffix+'FB1'\n",
        "expNameSuffix = modName+'_'+experiment+'_'\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "par.nEpisodesTran = 1000\n",
        "par.etaTransfer = 0.001\n",
        "par.Ns = [100,par.nClass]\n",
        "par.nInputs = 100\n",
        "par.fbLayer = []\n",
        "tran_esn_fb(expName, seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "04ddc697",
      "metadata": {},
      "source": [
        "### Run multiple cases of transfer learning per trained network, to investigate whether performance is hindered by getting stuck in local minima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87f370f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-5,-7/3,7) # LRs used for sweep above\n",
        "swLR = [swLR[4]]\n",
        "nSeeds = 1 # No. runs per hyperparameter\n",
        "\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for k, sd in enumerate(seeds):\n",
        "    t = time.time()\n",
        "    # Update hyperparameter\n",
        "    importlib.reload(par)\n",
        "    par.saveFlag_FBWeights = True\n",
        "    par.maxLayer = 3\n",
        "    par.lossLayer = 3\n",
        "    par.nSaveMaxT = par.nEpisodes \n",
        "    par.etaInitial = swLR[0]\n",
        "    par.fbLayer = 2\n",
        "    par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "    par.nTransferRuns = 10\n",
        "    expName = 'FB2_LocMin'\n",
        "    complete = xent_esn_fb(expName, sd)\n",
        "    print(f'k={k}/{len(seeds)}')\n",
        "    print(f'*************************FB2_LocMin Run time: {time.time()-t}')\n",
        "\n",
        "print('*******************************FINISHED FB2********************************')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19b65aae",
      "metadata": {
        "id": "19b65aae"
      },
      "source": [
        "# Plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600e634a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600e634a",
        "outputId": "9cd64612-b412-4422-f3f5-f179e36a7907"
      },
      "outputs": [],
      "source": [
        "# ### Load Triplet data\n",
        "# inputDir = directory+'/data/'+experiment    # Storage directory for input/label data\n",
        "# met = torch.load(inputDir + 'met_save.pt')\n",
        "# l_tri = torch.load(inputDir + 'loss_triplet.pt')\n",
        "# l_out = torch.load(inputDir + 'loss_out.pt')\n",
        "# a_tri = torch.load(inputDir + 'acc_triplet.pt')\n",
        "# a_out = torch.load(inputDir + 'acc_out.pt')\n",
        "# dist = torch.load(inputDir + 'dist_triplet.pt')\n",
        "# nt_dist = dist.shape[0] \n",
        "# nt_out = l_out.shape[0]\n",
        "# nt_met = met.shape[0]\n",
        "# N_triplet=45000\n",
        "# N_out=5000\n",
        "# batch_size=64\n",
        "# eta_t=0.0002\n",
        "# eta_o = 0.001\n",
        "# eta_t_tau = 40000.0\n",
        "# eta_o_tau = 4000.0\n",
        "# N_class=10\n",
        "# margin=2\n",
        "# save_N = 100 #100 # # of saved epochs\n",
        "# save_every = np.floor(N_triplet / save_N) # Save data every <> epohsChoice 3\n",
        "# save_Nsamples = 100 # # of inputs from each class for which to save resonses\n",
        "\n",
        "### Load Classic data\n",
        "experiment = 'met_FB0_swLR_0'\n",
        "expDir = directory+'/data/'+experiment # To read in saved data\n",
        "sd_name='11113'\n",
        "inputDir = expDir#+'/'+sd_name  \n",
        "figDir = directory+'/figs/met/'+experiment # To export figures\n",
        "print(figDir)\n",
        "if not os.path.exists(figDir):\n",
        "    os.mkdir(figDir)\n",
        "\n",
        "RESP = torch.load(inputDir + '/respSave'+sd_name+'.pt')\n",
        "print(f'Num layers = {len(RESP)}')\n",
        "lossTr = torch.load(inputDir + '/lossTr'+sd_name+'.pt')\n",
        "accTr = torch.load(inputDir + '/accTr'+sd_name+'.pt')\n",
        "accVal = torch.load(inputDir + '/accVal'+sd_name+'.pt')\n",
        "weights = torch.load(inputDir + '/weightSave'+sd_name+'.pt')\n",
        "nt = RESP[0].shape[-1]\n",
        "kernel = np.ones(50)/50\n",
        "\n",
        "### Set figure properties\n",
        "import matplotlib\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams.update({'font.size': 6})\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91085166",
      "metadata": {},
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e873e09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# saveflag = True\n",
        "saveflag = False\n",
        "\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# layer = 2\n",
        "# c=0\n",
        "# times=[0, 4]\n",
        "\n",
        "# ### Plot responses\n",
        "# # t=0\n",
        "# # for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "# #             pl.plot(RESP[layer][t,i,::10,:].transpose(), linewidth=0.5)\n",
        "# for c in [0, 1, 9]:\n",
        "#     for j, t in enumerate(times):\n",
        "#         for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "#             pl.plot(RESP[layer][t,i,:,:].transpose(), linewidth=0.5)\n",
        "#         if j==0:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,0.2))\n",
        "#         elif j==1:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,10,20))\n",
        "#         if saveflag:\n",
        "#             pl.savefig(figDir+'/resp_c'+str(c)+'t'+str(j)+'.svg', format=\"svg\")\n",
        "#         ax.cla() \n",
        "\n",
        "# ### Plot accuracy\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.linspace(1,par.nEpisodes,par.nEpisodes),np.convolve(np.pad(accTr, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# pl.plot(np.linspace(1,par.nEpisodes,par.nEpisodes),np.convolve(np.pad(accVal, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# # pl.plot(accTr, linewidth=0.5); pl.plot(accVal, linewidth=0.5)\n",
        "# ax.xaxis.set_ticks((0,par.nEpisodes)); ax.yaxis.set_ticks((0,1))\n",
        "# if saveflag:\n",
        "#     pl.savefig(figDir+'/accTrVal.svg', format=\"svg\")\n",
        "\n",
        "### Plot Feedback Weight Evolution\n",
        "if par.saveFlag_FBWeights:\n",
        "    fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "    ax.spines[['top','right']].set_visible(False)\n",
        "    for j in range(0,1000,100):\n",
        "        pl.plot(np.linspace(1,par.nEpisodes,par.nSave+1),weights[::2,j,:].transpose(), linewidth=0.5)\n",
        "ax.xaxis.set_ticks((0,par.nEpisodes)); ax.yaxis.set_ticks((-.05,0,.05))\n",
        "if saveflag:\n",
        "    pl.savefig(figDir+'/fbWeights.svg', format=\"svg\")\n",
        "\n",
        "# ### Correlation between met responses\n",
        "# layer = 1\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# # imdata = ax.imshow(stats.zscore(np.reshape(RESP[1][0,:,:,:],[RESP[1].shape[1], RESP[1].shape[2]*RESP[1].shape[3]]), 1).transpose(),vmin=-1.0, vmax=1.0)\n",
        "# c0 = np.matmul(stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# c1 = np.matmul(stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "82efca1a",
      "metadata": {
        "id": "82efca1a"
      },
      "source": [
        "### Plot triplet distances and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90dcb30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "e90dcb30",
        "outputId": "2637691c-fdaa-4170-c851-b3d7de14cff7"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as pl\n",
        "# saveflag = True\n",
        "saveflag = False\n",
        "tripletflag = True\n",
        "# tripletflag = False\n",
        "\n",
        "kernel = np.ones(50)/50\n",
        "prefix = experiment#'classic_original_'\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams['axes.labelsize'] = 6\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "###Plot distances\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,0]); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,1], linestyle='dashed')\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,0], linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,1], linewidth=0.5)\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=20)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,15,30))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,20))\n",
        "  # ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'distances.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplett loss\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_distX_tr.shape)),np.convolve(l_tri,kernel,mode='same')); \n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),l_tri, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(l_tri, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  # ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=0.5*1.05*np.max(l_tri))\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=1.0)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,1))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'loss_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplet accuracy\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),a_tri*100, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(a_tri*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,100))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'acc_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot output loss\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),np.convolve(l_tri,kernel,mode='same')); \n",
        "pl.plot(np.linspace(1,nt_out,nt_out),l_out, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(l_out, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=0.8)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0.0,0.8))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "ax.set_xticklabels((0, nt_out), fontdict={'family': 'sans-serif', 'size':5})\n",
        "ax.set_yticklabels((0.5,0.8), fontdict={'family': 'sans-serif', 'size':5})\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'loss_out.svg', format=\"svg\")\n",
        "\n",
        "###Plot output accuracy\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),a_out*100, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(a_out*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0,100))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'acc_out.svg', format=\"svg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a8b3d98",
      "metadata": {
        "id": "3a8b3d98"
      },
      "source": [
        "### Plot met responses over training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c442906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "1c442906",
        "outputId": "97f4589d-d70e-4cb2-e668-7042ecb759a8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as pl\n",
        "from scipy import stats\n",
        "pl.rcParams['savefig.dpi'] = 400\n",
        "\n",
        "saveflag = True\n",
        "# saveflag = False\n",
        "\n",
        "### Raw met reponses\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,5.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "plmet = np.squeeze(met[0,:,199])\n",
        "pl.plot(np.linspace(1,nt_met,nt_met)*save_every,plmet)\n",
        "ax.set_xlim(xmin=0, xmax=nt_met*save_every); ax.set_ylim(ymin=0.0, ymax=1.05*np.max(plmet))\n",
        "ax.xaxis.set_ticks((0,nt_met*save_every)); ax.yaxis.set_ticks((0,))\n",
        "pl.show()\n",
        "\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[0,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[-1,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1.svg', format=\"svg\")\n",
        "\n",
        "### Correlation between met responses\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = np.matmul(stats.zscore(np.squeeze(met[0,:,:]),0).transpose(), stats.zscore(np.squeeze(met[0,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c1 = np.matmul(stats.zscore(np.squeeze(met[-1,:,:]),0).transpose(), stats.zscore(np.squeeze(met[-1,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9f4c2a58",
      "metadata": {
        "id": "9f4c2a58"
      },
      "source": [
        "# Plot weights"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:venv] *",
      "language": "python",
      "name": "conda-env-venv-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
